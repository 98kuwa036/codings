From ee2eb51434466b006ac2c0705dd3ffe8c6041ff7 Mon Sep 17 00:00:00 2001
From: Claude <noreply@anthropic.com>
Date: Fri, 30 Jan 2026 09:11:24 +0000
Subject: [PATCH] Add Shogun System v8.0.1 - Multi-Agent Framework

Migrate complete Shogun system from codings repository:
- Core multi-agent orchestration framework
- CLI interface and main entry point
- Agent definitions and Ashigaru worker system
- Configuration and instruction templates
- Provider integrations and setup utilities
- Test suite

https://claude.ai/code/session_01NiNmr3a1nnCscpHBB7Miki
---
 .env.example                       |  60 +++
 README_ja.md                       | 217 ++++++++
 __init__.py                        |   2 +
 agents/__init__.py                 |   0
 agents/taisho.py                   | 732 ++++++++++++++++++++++++++
 ashigaru/groq_recorder.py          | 564 ++++++++++++++++++++
 ashigaru/ollama_web_search.py      | 610 ++++++++++++++++++++++
 cli.py                             | 483 +++++++++++++++++
 config/mcp_config.json             |  49 ++
 config/settings.yaml               | 708 +++++++++++++++++++++++++
 core/__init__.py                   |   0
 core/activity_memory.py            | 709 +++++++++++++++++++++++++
 core/ashigaru_selection.py         | 496 ++++++++++++++++++
 core/complexity.py                 |  85 +++
 core/controller.py                 | 548 +++++++++++++++++++
 core/dashboard.py                  | 136 +++++
 core/error_handling.py             | 525 +++++++++++++++++++
 core/escalation.py                 | 108 ++++
 core/knowledge_base.py             | 480 +++++++++++++++++
 core/maintenance.py                | 525 +++++++++++++++++++
 core/mcp_manager.py                | 223 ++++++++
 core/rag_integration.py            | 369 +++++++++++++
 core/sandbox_executor.py           | 812 +++++++++++++++++++++++++++++
 core/system_orchestrator.py        | 596 +++++++++++++++++++++
 core/task_queue.py                 | 244 +++++++++
 instructions/ashigaru.md           |  52 ++
 instructions/karo.md               |  41 ++
 instructions/maintenance.md        | 211 ++++++++
 instructions/shogun.md             |  36 ++
 instructions/taisho.md             |  56 ++
 integrations/__init__.py           |   0
 integrations/ha_interface.py       | 115 ++++
 integrations/notion_integration.py | 507 ++++++++++++++++++
 integrations/slack_bot.py          | 243 +++++++++
 main.py                            | 416 +++++++++++++++
 providers/__init__.py              |   0
 providers/anthropic_api.py         | 252 +++++++++
 providers/claude_cli.py            | 192 +++++++
 providers/openvino_client.py       | 107 ++++
 pyproject.toml                     |  55 ++
 requirements.txt                   |  70 +++
 setup/install.sh                   | 200 +++++++
 setup/maintenance.sh               | 170 ++++++
 setup/openvino_setup.sh            | 263 ++++++++++
 setup/proxmox_setup.sh             | 110 ++++
 setup/setup_groq.sh                | 326 ++++++++++++
 setup/setup_r1_japanese.sh         | 656 +++++++++++++++++++++++
 templates/context_template.md      |  23 +
 tests/test_error_handling.py       | 340 ++++++++++++
 49 files changed, 13722 insertions(+)
 create mode 100644 .env.example
 create mode 100644 README_ja.md
 create mode 100644 __init__.py
 create mode 100644 agents/__init__.py
 create mode 100644 agents/taisho.py
 create mode 100644 ashigaru/groq_recorder.py
 create mode 100644 ashigaru/ollama_web_search.py
 create mode 100644 cli.py
 create mode 100644 config/mcp_config.json
 create mode 100644 config/settings.yaml
 create mode 100644 core/__init__.py
 create mode 100644 core/activity_memory.py
 create mode 100644 core/ashigaru_selection.py
 create mode 100644 core/complexity.py
 create mode 100644 core/controller.py
 create mode 100644 core/dashboard.py
 create mode 100644 core/error_handling.py
 create mode 100644 core/escalation.py
 create mode 100644 core/knowledge_base.py
 create mode 100644 core/maintenance.py
 create mode 100644 core/mcp_manager.py
 create mode 100644 core/rag_integration.py
 create mode 100644 core/sandbox_executor.py
 create mode 100644 core/system_orchestrator.py
 create mode 100644 core/task_queue.py
 create mode 100644 instructions/ashigaru.md
 create mode 100644 instructions/karo.md
 create mode 100644 instructions/maintenance.md
 create mode 100644 instructions/shogun.md
 create mode 100644 instructions/taisho.md
 create mode 100644 integrations/__init__.py
 create mode 100644 integrations/ha_interface.py
 create mode 100644 integrations/notion_integration.py
 create mode 100644 integrations/slack_bot.py
 create mode 100644 main.py
 create mode 100644 providers/__init__.py
 create mode 100644 providers/anthropic_api.py
 create mode 100644 providers/claude_cli.py
 create mode 100644 providers/openvino_client.py
 create mode 100644 pyproject.toml
 create mode 100644 requirements.txt
 create mode 100644 setup/install.sh
 create mode 100644 setup/maintenance.sh
 create mode 100644 setup/openvino_setup.sh
 create mode 100644 setup/proxmox_setup.sh
 create mode 100644 setup/setup_groq.sh
 create mode 100644 setup/setup_r1_japanese.sh
 create mode 100644 templates/context_template.md
 create mode 100644 tests/test_error_handling.py

diff --git a/.env.example b/.env.example
new file mode 100644
index 0000000..7e2a2e9
--- /dev/null
+++ b/.env.example
@@ -0,0 +1,60 @@
+# å°†è»ã‚·ã‚¹ãƒ†ãƒ  v8.0.1 - ç’°å¢ƒå¤‰æ•°
+
+# â”€â”€â”€ Claude API (å°†è»/å®¶è€ã®APIãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨) â”€â”€â”€
+# Proç‰ˆclaude-cliã®åˆ¶é™åˆ°é”æ™‚ã«ã®ã¿ä½¿ç”¨
+# æœ€æ–°ç‰ˆ: Opus 4.5, Sonnet 4.5
+# ANTHROPIC_API_KEY=sk-ant-api03-xxxxx
+
+# â”€â”€â”€ Groq API (9ç•ªè¶³è»½ãƒ»è¨˜éŒ²ä¿‚) â”€â”€â”€
+# ç„¡æ–™æ : 14,400 requests/day
+# GROQ_API_KEY=gsk_xxxxx
+
+# â”€â”€â”€ Ollama Web Search (10ç•ªè¶³è»½) â”€â”€â”€ [v8.0 æ–°è¦]
+# ç„¡æ–™æ : 1000 requests/month
+# OLLAMA_API_KEY=xxxxx
+
+# â”€â”€â”€ Notion API (ãƒŠãƒ¬ãƒƒã‚¸DB) â”€â”€â”€
+# NOTION_TOKEN=secret_xxxxx
+# NOTION_DATABASE_ID=xxxxx
+
+# â”€â”€â”€ GitHub â”€â”€â”€
+# GITHUB_TOKEN=ghp_xxxxx
+
+# â”€â”€â”€ Slack Bots (12ã‚¢ã‚«ã‚¦ãƒ³ãƒˆ: å°†è»ãƒ»å®¶è€ãƒ»ä¾å¤§å°† + è¶³è»½Ã—10 + ä¸­éšŠ) â”€â”€â”€
+# SLACK_APP_TOKEN=xapp-xxxxx
+
+# å°†è»ãƒ»å®¶è€ãƒ»ä¾å¤§å°†
+# SLACK_TOKEN_SHOGUN=xoxb-xxxxx
+# SLACK_TOKEN_KARO=xoxb-xxxxx
+# SLACK_TOKEN_TAISHO=xoxb-xxxxx
+
+# è¶³è»½ Ã— 10 (v8.0: 10ç•ªè¿½åŠ )
+# SLACK_TOKEN_ASHIGARU_1=xoxb-xxxxx  # filesystem
+# SLACK_TOKEN_ASHIGARU_2=xoxb-xxxxx  # github
+# SLACK_TOKEN_ASHIGARU_3=xoxb-xxxxx  # fetch
+# SLACK_TOKEN_ASHIGARU_4=xoxb-xxxxx  # memory
+# SLACK_TOKEN_ASHIGARU_5=xoxb-xxxxx  # postgres
+# SLACK_TOKEN_ASHIGARU_6=xoxb-xxxxx  # puppeteer
+# SLACK_TOKEN_ASHIGARU_7=xoxb-xxxxx  # brave-search
+# SLACK_TOKEN_ASHIGARU_8=xoxb-xxxxx  # slack
+# SLACK_TOKEN_ASHIGARU_9=xoxb-xxxxx  # groq-recorder
+# SLACK_TOKEN_ASHIGARU_10=xoxb-xxxxx # ollama-web-search [v8.0 æ–°è¦]
+
+# ä¸­éšŠãƒ¢ãƒ¼ãƒ‰å°‚ç”¨
+# SLACK_TOKEN_LIGHT=xoxb-xxxxx
+
+# SLACK_TEAM_ID=T_xxxxx
+
+# â”€â”€â”€ Brave Search â”€â”€â”€
+# BRAVE_API_KEY=BSAxxxxx
+
+# â”€â”€â”€ Qdrant (çŸ¥è­˜åŸºç›¤ãƒ»RAG) â”€â”€â”€ [v8.0 æ–°è¦]
+# QDRANT_HOST=192.168.1.10
+# QDRANT_PORT=6333
+
+# â”€â”€â”€ Database â”€â”€â”€
+# DATABASE_URL=postgresql://user:pass@localhost:5432/shogun
+
+# â”€â”€â”€ Home Assistant â”€â”€â”€
+# HA_URL=http://192.168.1.20:8123
+# HA_TOKEN=xxxxx
diff --git a/README_ja.md b/README_ja.md
new file mode 100644
index 0000000..1800e6f
--- /dev/null
+++ b/README_ja.md
@@ -0,0 +1,217 @@
+# å°†è»ã‚·ã‚¹ãƒ†ãƒ  v8.0.1 - å®Œå…¨è‡ªå¾‹å‹AIé–‹ç™ºç’°å¢ƒ
+
+[![Claude Code](https://img.shields.io/badge/Claude%20Code-Powered-blue)](https://claude.ai/code)
+[![Version](https://img.shields.io/badge/Version-8.0.1-green)](https://github.com/98kuwa036/codings)
+[![Claude](https://img.shields.io/badge/Claude-Opus%204.5%20%2F%20Sonnet%204.5-purple)](https://www.anthropic.com/claude)
+
+## æ¦‚è¦
+
+`codings`ã¯ã€æ—¥æœ¬ã®æ­¦å®¶ç¤¾ä¼šã®éšå±¤æ§‹é€ ã‚’ãƒ¢ãƒ‡ãƒ«ã«ã—ãŸé©æ–°çš„ãªAIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚·ã‚¹ãƒ†ãƒ ã§ã™ã€‚å°†è»ï¼ˆShogunï¼‰ã‚’é ‚ç‚¹ã¨ã™ã‚‹çµ„ç¹”æ§‹é€ ã§ã€è¤‡æ•°ã®AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒå”èª¿ã—ã¦å‹•ä½œã—ã€è¤‡é›‘ãªã‚¿ã‚¹ã‚¯ã‚’åŠ¹ç‡çš„ã«å‡¦ç†ã—ã¾ã™ã€‚
+
+## ã‚·ã‚¹ãƒ†ãƒ æ§‹æˆ
+
+### éšå±¤æ§‹é€ 
+
+```
+å°†è»ï¼ˆShogunï¼‰- æœ€é«˜å¸ä»¤å®˜
+â”œâ”€â”€ å®¶è€ï¼ˆKaroï¼‰- é‡è¦äº‹é …ã®åŸ·è¡Œå½¹
+â”œâ”€â”€ å¤§å°†ï¼ˆTaishoï¼‰- æˆ¦ç•¥ç«‹æ¡ˆãƒ»æŒ‡æ®å®˜
+â””â”€â”€ è¶³è»½ï¼ˆAshigaruï¼‰- å®Ÿè¡Œéƒ¨éšŠï¼ˆæœ€å¤§4åã¾ã§å‹•çš„é¸æŠœï¼‰
+```
+
+### ä¸»è¦ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ
+
+- **å°†è»ã‚·ã‚¹ãƒ†ãƒ ** (`shogun/`) - ãƒ¡ã‚¤ãƒ³ã®AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆç®¡ç†ã‚·ã‚¹ãƒ†ãƒ 
+- **å®¶è¨“ç®¡ç†** - NotionãŠã‚ˆã³RAGã‚·ã‚¹ãƒ†ãƒ ã§ã®äºŒé‡ç®¡ç†
+- **GroqRecorder** - APIå‘¼ã³å‡ºã—ç®¡ç†ã¨ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾å¿œ
+- **åœ¨åº«ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ ** (`Inventry-Management/`) - VBAç‰ˆãŠã‚ˆã³Webç‰ˆ
+- **PWAã‚¢ãƒ—ãƒª** (`pwa-warehouse-app/`) - ãƒ¢ãƒã‚¤ãƒ«å¯¾å¿œå€‰åº«ç®¡ç†
+- **Home Assistantçµ±åˆ** (`custom_components/nature_remo/`) - IoTãƒ‡ãƒã‚¤ã‚¹é€£æº
+
+## ä¸»è¦æ©Ÿèƒ½
+
+### ğŸ¯ å°†è»AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚·ã‚¹ãƒ†ãƒ 
+
+- **å‹•çš„ã‚¿ã‚¹ã‚¯åˆ†æ•£**: è¤‡é›‘æ€§ã«å¿œã˜ã¦é©åˆ‡ãªã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã«è‡ªå‹•åˆ†æ•£
+- **éšå±¤çš„æ„æ€æ±ºå®š**: æ­¦å®¶ç¤¾ä¼šã®æŒ‡æ®ç³»çµ±ã‚’æ¨¡ã—ãŸåŠ¹ç‡çš„ãªå‡¦ç†ãƒ•ãƒ­ãƒ¼
+- **ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°**: Exponential Backoffã«ã‚ˆã‚‹å …ç‰¢ãªã‚¨ãƒ©ãƒ¼å›å¾©
+- **ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾å¿œ**: Groqã®RPM/TPMåˆ¶é™ã‚’è€ƒæ…®ã—ãŸåˆ†æ•£å‡¦ç†
+
+### ğŸ“ å®¶è¨“ï¼ˆçµ„ç¹”è¦ç´„ï¼‰ç®¡ç†
+
+- **Notionçµ±åˆ**: çµ„ç¹”ã®è¦ç´„ã‚„ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹ã‚’Notionä¸Šã§ç®¡ç†
+- **RAGçµ±åˆ**: æ¤œç´¢æ‹¡å¼µç”Ÿæˆã«ã‚ˆã‚‹å®¶è¨“ã®åŠ¹ç‡çš„ãªå‚ç…§ãƒ»é©ç”¨
+- **å‹•çš„æ›´æ–°**: ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§ã®å®¶è¨“ã®æ›´æ–°ã¨å…¨ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¸ã®åæ˜ 
+
+### âš¡ GroqRecorder - APIãƒ¬ãƒ¼ãƒˆåˆ¶é™ç®¡ç†
+
+```python
+# æ—¥æ¬¡ã‚«ã‚¦ãƒ³ãƒˆç®¡ç†
+daily_count = GroqRecorder.get_daily_usage()
+
+# åˆ†é–“åˆ¶é™å¯¾å¿œï¼ˆRPM/TPMï¼‰
+rate_limiter = ExponentialBackoffHandler(
+    rpm_limit=30,  # ãƒªã‚¯ã‚¨ã‚¹ãƒˆ/åˆ†
+    tpm_limit=6000,  # ãƒˆãƒ¼ã‚¯ãƒ³/åˆ†
+    base_delay=1.0,
+    max_delay=60.0
+)
+```
+
+### ğŸ›¡ï¸ ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
+
+- **Exponential Backoff**: æ®µéšçš„ãªå†è©¦è¡Œé–“éš”èª¿æ•´
+- **ãƒ¬ãƒ¼ãƒˆåˆ¶é™æ¤œçŸ¥**: APIãƒ¬ã‚¹ãƒãƒ³ã‚¹ç›£è¦–ã«ã‚ˆã‚‹åˆ¶é™æ¤œçŸ¥
+- **ãƒ•ã‚§ã‚¤ãƒ«ã‚»ãƒ¼ãƒ•**: ä»£æ›¿å‡¦ç†ãƒ‘ã‚¹ã«ã‚ˆã‚‹ç¶™ç¶šçš„ã‚µãƒ¼ãƒ“ã‚¹æä¾›
+
+## ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
+
+### å¿…è¦ç’°å¢ƒ
+
+- Python 3.8+
+- Node.js 16+ (PWAã‚¢ãƒ—ãƒªç”¨)
+- Claude API Key
+- Groq API Key
+- Notion API Tokenï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
+
+### ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
+
+```bash
+# ãƒªãƒã‚¸ãƒˆãƒªã®ã‚¯ãƒ­ãƒ¼ãƒ³
+git clone https://github.com/98kuwa036/codings.git
+cd codings
+
+# å°†è»ã‚·ã‚¹ãƒ†ãƒ ã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
+cd shogun
+pip install -r requirements.txt
+
+# è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®æº–å‚™
+cp config/settings.yaml.example config/settings.yaml
+# API ã‚­ãƒ¼ã‚’è¨­å®šã—ã¦ãã ã•ã„
+
+# ã‚·ã‚¹ãƒ†ãƒ èµ·å‹•
+python main.py
+```
+
+## ä½¿ç”¨æ–¹æ³•
+
+### åŸºæœ¬çš„ãªä½¿ç”¨æ–¹æ³•
+
+```bash
+# å°†è»ã‚·ã‚¹ãƒ†ãƒ èµ·å‹•
+cd shogun
+python cli.py --mode interactive
+
+# ã‚¿ã‚¹ã‚¯ã®å®Ÿè¡Œ
+python cli.py --task "åœ¨åº«ãƒ‡ãƒ¼ã‚¿ã®åˆ†æã¨å ±å‘Šæ›¸ä½œæˆ"
+```
+
+### å®¶è¨“ã®ç®¡ç†
+
+```yaml
+# config/settings.yaml
+family_rules:
+  notion:
+    enabled: true
+    database_id: "your-notion-database-id"
+  rag:
+    enabled: true
+    vector_store: "chroma"
+    embedding_model: "text-embedding-3-small"
+```
+
+### è¶³è»½ï¼ˆå®Ÿè¡Œéƒ¨éšŠï¼‰ã®ç®¡ç†
+
+```python
+# æœ€å¤§4åã®è¶³è»½ã‚’å‹•çš„é¸æŠœ
+ashigaru_manager = AshigaruManager(max_members=4)
+selected_members = ashigaru_manager.select_optimal_team(task_complexity)
+```
+
+## ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ§‹é€ 
+
+```
+codings/
+â”œâ”€â”€ shogun/                    # ãƒ¡ã‚¤ãƒ³AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚·ã‚¹ãƒ†ãƒ 
+â”‚   â”œâ”€â”€ agents/               # ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå®Ÿè£…
+â”‚   â”œâ”€â”€ ashigaru/            # è¶³è»½ï¼ˆå®Ÿè¡Œéƒ¨éšŠï¼‰
+â”‚   â”œâ”€â”€ core/                # ã‚³ã‚¢ã‚·ã‚¹ãƒ†ãƒ 
+â”‚   â”œâ”€â”€ integrations/        # å¤–éƒ¨ã‚µãƒ¼ãƒ“ã‚¹çµ±åˆ
+â”‚   â””â”€â”€ providers/           # AIãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼
+â”œâ”€â”€ Inventry-Management/      # åœ¨åº«ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ ï¼ˆVBAç‰ˆï¼‰
+â”œâ”€â”€ pwa-warehouse-app/       # PWAå€‰åº«ç®¡ç†ã‚¢ãƒ—ãƒª
+â”œâ”€â”€ custom_components/       # Home Assistantçµ±åˆ
+â”œâ”€â”€ Admin-Page/             # ç®¡ç†è€…ãƒšãƒ¼ã‚¸
+â””â”€â”€ loginform/              # èªè¨¼ã‚·ã‚¹ãƒ†ãƒ 
+```
+
+## è¨­å®š
+
+### APIåˆ¶é™è¨­å®š
+
+```yaml
+# config/settings.yaml
+groq:
+  daily_limit: 1000
+  rpm_limit: 30        # ãƒªã‚¯ã‚¨ã‚¹ãƒˆ/åˆ†
+  tpm_limit: 6000      # ãƒˆãƒ¼ã‚¯ãƒ³/åˆ†
+  backoff:
+    base_delay: 1.0
+    max_delay: 60.0
+    exponential_base: 2
+```
+
+### è¶³è»½é¸æŠœè¨­å®š
+
+```yaml
+ashigaru:
+  max_members: 4
+  selection_criteria:
+    - task_complexity
+    - current_workload  
+    - specialization
+  dynamic_rebalancing: true
+```
+
+## é–‹ç™º
+
+### è²¢çŒ®
+
+1. ã“ã®ãƒªãƒã‚¸ãƒˆãƒªã‚’ãƒ•ã‚©ãƒ¼ã‚¯
+2. ãƒ•ã‚£ãƒ¼ãƒãƒ£ãƒ¼ãƒ–ãƒ©ãƒ³ãƒã‚’ä½œæˆ (`git checkout -b feature/amazing-feature`)
+3. å¤‰æ›´ã‚’ã‚³ãƒŸãƒƒãƒˆ (`git commit -m 'Add amazing feature'`)
+4. ãƒ–ãƒ©ãƒ³ãƒã«ãƒ—ãƒƒã‚·ãƒ¥ (`git push origin feature/amazing-feature`)
+5. ãƒ—ãƒ«ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’ä½œæˆ
+
+### ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
+
+```bash
+# å˜ä½“ãƒ†ã‚¹ãƒˆ
+cd shogun
+python -m pytest tests/
+
+# çµ±åˆãƒ†ã‚¹ãƒˆ
+python -m pytest tests/test_integration_v7.py
+```
+
+## ãƒ©ã‚¤ã‚»ãƒ³ã‚¹
+
+ã“ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¯MITãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã®ä¸‹ã§å…¬é–‹ã•ã‚Œã¦ã„ã¾ã™ã€‚è©³ç´°ã¯[LICENSE](LICENSE)ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚
+
+## ã‚µãƒãƒ¼ãƒˆ
+
+- **Issues**: [GitHub Issues](https://github.com/98kuwa036/codings/issues)
+- **Wiki**: [ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆWiki](https://github.com/98kuwa036/codings/wiki)
+- **Discussions**: [GitHub Discussions](https://github.com/98kuwa036/codings/discussions)
+
+## æ›´æ–°å±¥æ­´
+
+| ãƒãƒ¼ã‚¸ãƒ§ãƒ³ | æ—¥ä»˜ | å¤‰æ›´å†…å®¹ |
+|-----------|------|----------|
+| v1.0.0 | 2025-01-29 | æ—¥æœ¬èªREADMEä½œæˆã€ã‚·ã‚¹ãƒ†ãƒ æ¦‚è¦æ•´å‚™ |
+
+---
+
+**ä½¿ç”¨æŠ€è¡“**: Claude Opus (20251101), Python, JavaScript, VBA, Google Apps Script, Home Assistant
+
+**Generated with [Claude Code](https://claude.ai/code)**
\ No newline at end of file
diff --git a/__init__.py b/__init__.py
new file mode 100644
index 0000000..242cb7c
--- /dev/null
+++ b/__init__.py
@@ -0,0 +1,2 @@
+"""å°†è»ã‚·ã‚¹ãƒ†ãƒ  v8.0.1ã€Œã‚¹ãƒ”ãƒ¼ãƒ‰ã‚ˆã‚Šè³ª + å®Ÿè¨¼ä¸»ç¾© + çŸ¥è­˜è“„ç©æ´»ç”¨ã€"""
+__version__ = "8.0.1"
diff --git a/agents/__init__.py b/agents/__init__.py
new file mode 100644
index 0000000..e69de29
diff --git a/agents/taisho.py b/agents/taisho.py
new file mode 100644
index 0000000..682cf34
--- /dev/null
+++ b/agents/taisho.py
@@ -0,0 +1,732 @@
+"""
+å°†è»ã‚·ã‚¹ãƒ†ãƒ  v8.0 - Enhanced Taisho (ä¾å¤§å°†) Agent
+ç¾å ´æŒ‡æ®å®˜: DeepSeek R1 Japanese + å…¨v8.0æ©Ÿèƒ½çµ±åˆ
+
+Enhanced Features:
+- Knowledge Base (RAG) integration
+- Activity Memory (é™£ä¸­æ—¥è¨˜) for decision consistency
+- Sandbox execution validation (å®Ÿè¨¼ä¸»ç¾©)
+- Dynamic parameter optimization
+- Thought process monitoring
+- Ashigaru (è¶³è»½) coordination
+"""
+
+import asyncio
+import json
+import logging
+import time
+from datetime import datetime
+from typing import Dict, Any, List, Optional, Tuple
+from dataclasses import dataclass
+from enum import Enum
+import hashlib
+import httpx
+
+# Local imports
+import sys
+from pathlib import Path
+sys.path.append(str(Path(__file__).parent.parent))
+
+from core.knowledge_base import KnowledgeBase, KnowledgeEntry, DataSource
+from core.activity_memory import ActivityMemory, TaskRecord, TaskComplexity, DecisionConfidence, SimilarTaskResult
+from core.sandbox_executor import SandboxExecutor, CodeExecution, Language as SandboxLanguage
+from ashigaru.ollama_web_search import OllamaWebSearch, SearchQuery
+
+# Configure logging
+logging.basicConfig(level=logging.INFO)
+logger = logging.getLogger(__name__)
+
+
+class TaskPriority(Enum):
+    """Task priority levels"""
+    LOW = "low"
+    MEDIUM = "medium"
+    HIGH = "high"
+    CRITICAL = "critical"
+
+
+@dataclass
+class TaishoTask:
+    """Task structure for Taisho processing"""
+    id: str
+    description: str
+    complexity: TaskComplexity
+    priority: TaskPriority = TaskPriority.MEDIUM
+    context: Optional[str] = None
+    requires_execution: bool = False  # Whether code execution is needed
+    requires_latest_info: bool = False  # Whether web search is needed
+    max_processing_time: int = 300  # Max time in seconds
+    requested_by: str = "unknown"
+    created_at: datetime = None
+    
+    def __post_init__(self):
+        if self.created_at is None:
+            self.created_at = datetime.utcnow()
+        if not self.id:
+            task_hash = hashlib.md5(f"{self.description}{self.created_at}".encode()).hexdigest()[:8]
+            self.id = f"task_{task_hash}"
+
+
+@dataclass
+class TaishoResponse:
+    """Response from Taisho processing"""
+    task_id: str
+    success: bool
+    response: str
+    reasoning: str
+    confidence: DecisionConfidence
+    processing_time_seconds: float
+    tools_used: List[str]
+    similar_tasks_found: int = 0
+    knowledge_retrieved: int = 0
+    code_executed: bool = False
+    execution_successful: bool = False
+    created_at: datetime = None
+    
+    def __post_init__(self):
+        if self.created_at is None:
+            self.created_at = datetime.utcnow()
+
+
+class TaishoAgent:
+    """
+    å°†è»ã‚·ã‚¹ãƒ†ãƒ  v8.0 Enhanced Taisho Agent
+    
+    DeepSeek R1 Japanese-based field commander with full v8.0 integration:
+    - Knowledge Base consultation for latest information
+    - Activity Memory for consistent decision-making
+    - Sandbox execution for code validation
+    - Dynamic parameter adjustment based on task complexity
+    - Intelligent tool selection and coordination
+    """
+    
+    def __init__(
+        self,
+        r1_endpoint: str = "http://192.168.1.11:11434",
+        model_name: str = "cyberagent/DeepSeek-R1-Distill-Qwen-14B-Japanese",
+        knowledge_base: Optional[KnowledgeBase] = None,
+        activity_memory: Optional[ActivityMemory] = None,
+        sandbox_executor: Optional[SandboxExecutor] = None,
+        web_search: Optional[OllamaWebSearch] = None
+    ):
+        self.r1_endpoint = r1_endpoint.rstrip('/')
+        self.model_name = model_name
+        
+        # v8.0 Core systems
+        self.knowledge_base = knowledge_base
+        self.activity_memory = activity_memory
+        self.sandbox_executor = sandbox_executor
+        self.web_search = web_search
+        
+        # Dynamic parameters for different task types
+        self.parameter_configs = {
+            TaskComplexity.SIMPLE: {
+                "temperature": 0.3,
+                "max_tokens": 1000,
+                "timeout_seconds": 60
+            },
+            TaskComplexity.MEDIUM: {
+                "temperature": 0.7,
+                "max_tokens": 2000,
+                "timeout_seconds": 120
+            },
+            TaskComplexity.COMPLEX: {
+                "temperature": 0.9,
+                "max_tokens": 4000,
+                "timeout_seconds": 300
+            },
+            TaskComplexity.STRATEGIC: {
+                "temperature": 0.5,  # More focused for strategic thinking
+                "max_tokens": 3000,
+                "timeout_seconds": 240
+            }
+        }
+        
+        # Audit mode parameters (for quality monitoring)
+        self.audit_parameters = {
+            "temperature": 0.1,  # Very focused
+            "max_tokens": 1500,
+            "timeout_seconds": 90
+        }
+        
+        logger.info(f"TaishoAgent v8.0 initialized - Endpoint: {r1_endpoint}")
+    
+    async def process_task(self, task: TaishoTask, audit_mode: bool = False) -> TaishoResponse:
+        """
+        Process a task using the full v8.0 enhancement stack
+        
+        Args:
+            task: Task to process
+            audit_mode: Use strict parameters for audit/review tasks
+            
+        Returns:
+            TaishoResponse with results and metadata
+        """
+        start_time = time.time()
+        tools_used = []
+        
+        try:
+            logger.info(f"Processing task {task.id}: {task.description[:50]}...")
+            
+            # Step 1: Check Activity Memory for similar tasks
+            similar_tasks = []
+            if self.activity_memory:
+                logger.info("Checking activity memory for similar tasks...")
+                similar_tasks = await self.activity_memory.find_similar_tasks(
+                    task_summary=task.description,
+                    task_type=task.complexity,
+                    limit=3,
+                    min_confidence=DecisionConfidence.MEDIUM
+                )
+                tools_used.append("activity_memory")
+                
+                if similar_tasks:
+                    logger.info(f"Found {len(similar_tasks)} similar tasks")
+            
+            # Step 2: Retrieve relevant knowledge if needed
+            knowledge_context = ""
+            knowledge_retrieved = 0
+            if self.knowledge_base:
+                logger.info("Searching knowledge base...")
+                knowledge_entries = await self.knowledge_base.search(
+                    query=task.description,
+                    max_results=3,
+                    score_threshold=0.6
+                )
+                
+                if knowledge_entries:
+                    knowledge_retrieved = len(knowledge_entries)
+                    knowledge_snippets = []
+                    for entry in knowledge_entries:
+                        snippet = f"ã€{entry.title}ã€‘: {entry.content[:200]}..."
+                        knowledge_snippets.append(snippet)
+                    knowledge_context = "\n".join(knowledge_snippets)
+                    tools_used.append("knowledge_base")
+                    logger.info(f"Retrieved {knowledge_retrieved} knowledge entries")
+            
+            # Step 3: Get latest information if required
+            latest_info = ""
+            if task.requires_latest_info and self.web_search:
+                logger.info("Searching for latest information...")
+                search_query = SearchQuery(
+                    id=f"search_{task.id}",
+                    query=task.description,
+                    language="ja",
+                    max_results=3,
+                    requested_by="taisho",
+                    context=task.context
+                )
+                
+                search_response = await self.web_search.search(search_query)
+                if search_response.success:
+                    search_snippets = [result.snippet for result in search_response.results]
+                    latest_info = "\n".join(search_snippets[:2])  # Top 2 results
+                    tools_used.append("web_search")
+                    logger.info("Latest information retrieved")
+            
+            # Step 4: Prepare enhanced context for R1
+            enhanced_context = self._build_enhanced_context(
+                task=task,
+                similar_tasks=similar_tasks,
+                knowledge_context=knowledge_context,
+                latest_info=latest_info
+            )
+            
+            # Step 5: Get R1 response with dynamic parameters
+            parameters = self.audit_parameters if audit_mode else self.parameter_configs[task.complexity]
+            r1_response = await self._query_r1(
+                prompt=enhanced_context,
+                parameters=parameters
+            )
+            
+            # Step 6: Execute code validation if needed
+            code_executed = False
+            execution_successful = False
+            if task.requires_execution and self.sandbox_executor:
+                logger.info("Executing code validation...")
+                code_executed, execution_successful = await self._validate_code_execution(task, r1_response)
+                tools_used.append("sandbox_executor")
+            
+            # Step 7: Determine confidence and final response
+            confidence = self._determine_confidence(
+                similar_tasks_found=len(similar_tasks),
+                knowledge_retrieved=knowledge_retrieved,
+                execution_successful=execution_successful if code_executed else True,
+                task_complexity=task.complexity
+            )
+            
+            # Extract reasoning and response from R1 output
+            reasoning, final_response = self._parse_r1_response(r1_response)
+            
+            processing_time = time.time() - start_time
+            
+            # Step 8: Record to Activity Memory
+            if self.activity_memory:
+                await self._record_activity(
+                    task=task,
+                    response=final_response,
+                    reasoning=reasoning,
+                    confidence=confidence,
+                    processing_time=processing_time,
+                    tools_used=tools_used,
+                    similar_tasks=[t.task_id for t in similar_tasks],
+                    success=True
+                )
+            
+            # Create response object
+            response = TaishoResponse(
+                task_id=task.id,
+                success=True,
+                response=final_response,
+                reasoning=reasoning,
+                confidence=confidence,
+                processing_time_seconds=processing_time,
+                tools_used=tools_used,
+                similar_tasks_found=len(similar_tasks),
+                knowledge_retrieved=knowledge_retrieved,
+                code_executed=code_executed,
+                execution_successful=execution_successful
+            )
+            
+            logger.info(f"Task {task.id} completed successfully in {processing_time:.2f}s")
+            return response
+            
+        except Exception as e:
+            logger.error(f"Task {task.id} processing failed: {e}")
+            
+            processing_time = time.time() - start_time
+            
+            # Record failure to Activity Memory
+            if self.activity_memory:
+                await self._record_activity(
+                    task=task,
+                    response=f"å‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(e)}",
+                    reasoning="ã‚·ã‚¹ãƒ†ãƒ ã‚¨ãƒ©ãƒ¼ã«ã‚ˆã‚Šå‡¦ç†ãŒä¸­æ–­ã•ã‚Œã¾ã—ãŸ",
+                    confidence=DecisionConfidence.LOW,
+                    processing_time=processing_time,
+                    tools_used=tools_used,
+                    similar_tasks=[],
+                    success=False,
+                    error_message=str(e)
+                )
+            
+            return TaishoResponse(
+                task_id=task.id,
+                success=False,
+                response=f"ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ãŒã€å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}",
+                reasoning="ã‚·ã‚¹ãƒ†ãƒ ã‚¨ãƒ©ãƒ¼",
+                confidence=DecisionConfidence.LOW,
+                processing_time_seconds=processing_time,
+                tools_used=tools_used
+            )
+    
+    def _build_enhanced_context(
+        self,
+        task: TaishoTask,
+        similar_tasks: List[SimilarTaskResult],
+        knowledge_context: str,
+        latest_info: str
+    ) -> str:
+        """Build enhanced context prompt for R1"""
+        
+        context_parts = []
+        
+        # Base instruction
+        context_parts.append("""ã‚ãªãŸã¯å°†è»ã‚·ã‚¹ãƒ†ãƒ ã®ä¾å¤§å°†ï¼ˆç¾å ´æŒ‡æ®å®˜ï¼‰ã§ã™ã€‚DeepSeek R1 Japaneseã¨ã—ã¦ã€æ·±ã„<think>ã‚¿ã‚°ã§ã®æ€è€ƒãƒ—ãƒ­ã‚»ã‚¹ã‚’æ´»ç”¨ã—ã€é«˜å“è³ªãªåˆ†æã¨åˆ¤æ–­ã‚’è¡Œã£ã¦ãã ã•ã„ã€‚
+
+ä»¥ä¸‹ã®ã‚¿ã‚¹ã‚¯ã‚’å‡¦ç†ã—ã¦ãã ã•ã„ï¼š""")
+        
+        # Task description
+        context_parts.append(f"""
+ã€ã‚¿ã‚¹ã‚¯ã€‘
+{task.description}
+
+è¤‡é›‘åº¦: {task.complexity.value}
+å„ªå…ˆåº¦: {task.priority.value}
+""")
+        
+        # Context if provided
+        if task.context:
+            context_parts.append(f"""
+ã€è¿½åŠ ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã€‘
+{task.context}
+""")
+        
+        # Similar tasks from memory
+        if similar_tasks:
+            context_parts.append("\nã€éå»ã®é¡ä¼¼ã‚¿ã‚¹ã‚¯ã€‘")
+            for i, similar in enumerate(similar_tasks[:2], 1):  # Top 2
+                context_parts.append(f"""
+{i}. {similar.task_summary}
+   æ±ºå®š: {similar.final_decision}
+   ç†ç”±: {similar.decision_reasoning}
+   ä¿¡é ¼åº¦: {similar.confidence_level.value}
+   å‡¦ç†æ™‚é–“: {similar.processing_time_seconds:.1f}ç§’
+   é¡ä¼¼åº¦: {similar.similarity_score:.2f}
+""")
+        
+        # Knowledge base context
+        if knowledge_context:
+            context_parts.append(f"""
+ã€é–¢é€£çŸ¥è­˜ã€‘
+{knowledge_context}
+""")
+        
+        # Latest information
+        if latest_info:
+            context_parts.append(f"""
+ã€æœ€æ–°æƒ…å ±ã€‘
+{latest_info}
+""")
+        
+        # Instructions
+        context_parts.append("""
+ã€å›ç­”å½¢å¼ã€‘
+<think>
+ã“ã“ã§æ·±ãåˆ†æãƒ»æ¤œè¨ã—ã¦ãã ã•ã„ï¼š
+1. ã‚¿ã‚¹ã‚¯ã®æœ¬è³ªçš„ãªè¦æ±‚ã‚’ç†è§£
+2. éå»ã®é¡ä¼¼ã‚¿ã‚¹ã‚¯ã¨ã®æ¯”è¼ƒæ¤œè¨
+3. é–¢é€£çŸ¥è­˜ãƒ»æœ€æ–°æƒ…å ±ã®æ´»ç”¨æ–¹æ³•
+4. æœ€é©ãªã‚¢ãƒ—ãƒ­ãƒ¼ãƒã®é¸æŠ
+5. æœŸå¾…ã•ã‚Œã‚‹çµæœã¨ãƒªã‚¹ã‚¯è©•ä¾¡
+</think>
+
+ã€åˆ¤æ–­ã€‘
+æ˜ç¢ºã§å®Ÿç”¨çš„ãªåˆ¤æ–­ãƒ»æ¨å¥¨äº‹é …
+
+ã€æ ¹æ‹ ã€‘
+åˆ¤æ–­ã«è‡³ã£ãŸç†ç”±ã¨è€ƒæ…®ã—ãŸè¦ç´ 
+""")
+        
+        return "\n".join(context_parts)
+    
+    async def _query_r1(self, prompt: str, parameters: Dict[str, Any]) -> str:
+        """Query DeepSeek R1 Japanese model"""
+        try:
+            request_data = {
+                "model": self.model_name,
+                "prompt": prompt,
+                "temperature": parameters["temperature"],
+                "max_tokens": parameters["max_tokens"],
+                "stream": False
+            }
+            
+            async with httpx.AsyncClient(timeout=parameters["timeout_seconds"]) as client:
+                response = await client.post(
+                    f"{self.r1_endpoint}/api/generate",
+                    json=request_data,
+                    headers={"Content-Type": "application/json"}
+                )
+                
+                if response.status_code == 200:
+                    result = response.json()
+                    return result.get("response", "å¿œç­”ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
+                else:
+                    logger.error(f"R1 API error: {response.status_code} - {response.text}")
+                    return "R1 APIã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚"
+                    
+        except httpx.TimeoutException:
+            logger.error("R1 query timed out")
+            return "å‡¦ç†ãŒã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã—ã¾ã—ãŸã€‚"
+        except Exception as e:
+            logger.error(f"R1 query failed: {e}")
+            return f"R1ã‚¯ã‚¨ãƒªã‚¨ãƒ©ãƒ¼: {str(e)}"
+    
+    def _parse_r1_response(self, r1_output: str) -> Tuple[str, str]:
+        """Parse R1 response to extract reasoning and final answer"""
+        try:
+            # Extract <think> content
+            think_start = r1_output.find("<think>")
+            think_end = r1_output.find("</think>")
+            
+            if think_start != -1 and think_end != -1:
+                thinking_process = r1_output[think_start + 7:think_end].strip()
+                remaining_text = r1_output[think_end + 8:].strip()
+            else:
+                thinking_process = "æ€è€ƒãƒ—ãƒ­ã‚»ã‚¹ä¸æ˜"
+                remaining_text = r1_output
+            
+            # Extract judgment and reasoning
+            judgment_start = remaining_text.find("ã€åˆ¤æ–­ã€‘")
+            reasoning_start = remaining_text.find("ã€æ ¹æ‹ ã€‘")
+            
+            if judgment_start != -1:
+                if reasoning_start != -1:
+                    judgment = remaining_text[judgment_start + 4:reasoning_start].strip()
+                    reasoning = remaining_text[reasoning_start + 4:].strip()
+                else:
+                    judgment = remaining_text[judgment_start + 4:].strip()
+                    reasoning = thinking_process
+            else:
+                # Fallback parsing
+                lines = remaining_text.split('\n')
+                judgment = remaining_text[:200] + "..." if len(remaining_text) > 200 else remaining_text
+                reasoning = thinking_process
+            
+            return reasoning, judgment
+            
+        except Exception as e:
+            logger.warning(f"Failed to parse R1 response: {e}")
+            return "è§£æã‚¨ãƒ©ãƒ¼", r1_output[:500] + "..." if len(r1_output) > 500 else r1_output
+    
+    async def _validate_code_execution(self, task: TaishoTask, r1_response: str) -> Tuple[bool, bool]:
+        """Validate code execution in sandbox"""
+        try:
+            # Extract code from R1 response (simple heuristic)
+            code_blocks = []
+            lines = r1_response.split('\n')
+            in_code_block = False
+            current_code = []
+            current_language = None
+            
+            for line in lines:
+                if line.strip().startswith('```'):
+                    if not in_code_block:
+                        # Starting code block
+                        in_code_block = True
+                        language_hint = line.strip()[3:].lower()
+                        if 'python' in language_hint:
+                            current_language = SandboxLanguage.PYTHON
+                        elif 'javascript' in language_hint or 'js' in language_hint:
+                            current_language = SandboxLanguage.NODEJS
+                        elif 'rust' in language_hint:
+                            current_language = SandboxLanguage.RUST
+                        else:
+                            current_language = SandboxLanguage.PYTHON  # Default
+                    else:
+                        # Ending code block
+                        in_code_block = False
+                        if current_code and current_language:
+                            code_blocks.append((current_language, '\n'.join(current_code)))
+                        current_code = []
+                        current_language = None
+                elif in_code_block:
+                    current_code.append(line)
+            
+            if not code_blocks:
+                logger.info("No code blocks found in R1 response")
+                return False, False
+            
+            # Execute first code block
+            language, code = code_blocks[0]
+            execution = CodeExecution(
+                id=f"validation_{task.id}",
+                language=language,
+                code=code,
+                description=f"Validation for task: {task.description}",
+                timeout_seconds=30
+            )
+            
+            result = await self.sandbox_executor.execute_code(execution)
+            return True, result.success
+            
+        except Exception as e:
+            logger.error(f"Code validation failed: {e}")
+            return False, False
+    
+    def _determine_confidence(
+        self,
+        similar_tasks_found: int,
+        knowledge_retrieved: int,
+        execution_successful: bool,
+        task_complexity: TaskComplexity
+    ) -> DecisionConfidence:
+        """Determine confidence level based on available information"""
+        
+        confidence_score = 0
+        
+        # Base confidence by complexity
+        if task_complexity == TaskComplexity.SIMPLE:
+            confidence_score += 2
+        elif task_complexity == TaskComplexity.MEDIUM:
+            confidence_score += 1
+        # Complex and Strategic start at 0
+        
+        # Boost from similar tasks
+        if similar_tasks_found >= 2:
+            confidence_score += 2
+        elif similar_tasks_found == 1:
+            confidence_score += 1
+        
+        # Boost from knowledge
+        if knowledge_retrieved >= 2:
+            confidence_score += 1
+        
+        # Boost from successful execution
+        if execution_successful:
+            confidence_score += 2
+        
+        # Map to confidence levels
+        if confidence_score >= 5:
+            return DecisionConfidence.VERY_HIGH
+        elif confidence_score >= 3:
+            return DecisionConfidence.HIGH
+        elif confidence_score >= 1:
+            return DecisionConfidence.MEDIUM
+        else:
+            return DecisionConfidence.LOW
+    
+    async def _record_activity(
+        self,
+        task: TaishoTask,
+        response: str,
+        reasoning: str,
+        confidence: DecisionConfidence,
+        processing_time: float,
+        tools_used: List[str],
+        similar_tasks: List[str],
+        success: bool,
+        error_message: Optional[str] = None
+    ) -> bool:
+        """Record activity to memory"""
+        try:
+            record = TaskRecord(
+                id=f"activity_{task.id}",
+                task_hash="",  # Will be auto-generated
+                task_summary=task.description,
+                task_type=task.complexity,
+                think_summary=reasoning[:500],  # Condensed reasoning
+                final_decision=response[:500],   # Condensed response
+                decision_reasoning=reasoning,
+                confidence_level=confidence,
+                processing_time_seconds=processing_time,
+                tools_used=tools_used,
+                similar_tasks=similar_tasks,
+                success=success,
+                error_message=error_message,
+                metadata={
+                    "priority": task.priority.value,
+                    "context": task.context,
+                    "requested_by": task.requested_by
+                }
+            )
+            
+            return await self.activity_memory.record_activity(record)
+            
+        except Exception as e:
+            logger.error(f"Failed to record activity: {e}")
+            return False
+    
+    async def get_performance_metrics(self) -> Dict[str, Any]:
+        """Get performance metrics from activity memory"""
+        try:
+            if not self.activity_memory:
+                return {"error": "Activity memory not available"}
+            
+            stats = await self.activity_memory.get_statistics()
+            return stats
+            
+        except Exception as e:
+            logger.error(f"Failed to get performance metrics: {e}")
+            return {"error": str(e)}
+    
+    async def health_check(self) -> Dict[str, Any]:
+        """Comprehensive health check"""
+        try:
+            health_status = {
+                "status": "unknown",
+                "r1_accessible": False,
+                "knowledge_base_status": "unknown",
+                "activity_memory_status": "unknown",
+                "sandbox_status": "unknown",
+                "web_search_status": "unknown",
+                "checked_at": datetime.utcnow().isoformat()
+            }
+            
+            # Check R1 endpoint
+            try:
+                async with httpx.AsyncClient(timeout=10.0) as client:
+                    response = await client.get(f"{self.r1_endpoint}/api/version")
+                    health_status["r1_accessible"] = response.status_code == 200
+            except:
+                health_status["r1_accessible"] = False
+            
+            # Check subsystems
+            if self.knowledge_base:
+                kb_health = await self.knowledge_base.health_check()
+                health_status["knowledge_base_status"] = kb_health.get("status", "unknown")
+            
+            if self.activity_memory:
+                stats = await self.activity_memory.get_statistics()
+                health_status["activity_memory_status"] = "healthy" if stats else "unhealthy"
+            
+            if self.sandbox_executor:
+                sandbox_health = await self.sandbox_executor.health_check()
+                health_status["sandbox_status"] = sandbox_health.get("status", "unknown")
+            
+            if self.web_search:
+                search_health = await self.web_search.health_check()
+                health_status["web_search_status"] = search_health.get("status", "unknown")
+            
+            # Overall status
+            critical_systems = [health_status["r1_accessible"]]
+            if all(critical_systems):
+                health_status["status"] = "healthy"
+            else:
+                health_status["status"] = "unhealthy"
+            
+            return health_status
+            
+        except Exception as e:
+            logger.error(f"Health check failed: {e}")
+            return {
+                "status": "error",
+                "error": str(e),
+                "checked_at": datetime.utcnow().isoformat()
+            }
+
+
+# Factory function
+def create_taisho_agent(
+    config: Dict[str, Any],
+    knowledge_base: Optional[KnowledgeBase] = None,
+    activity_memory: Optional[ActivityMemory] = None,
+    sandbox_executor: Optional[SandboxExecutor] = None,
+    web_search: Optional[OllamaWebSearch] = None
+) -> TaishoAgent:
+    """Create TaishoAgent instance from configuration"""
+    return TaishoAgent(
+        r1_endpoint=config.get("url", "http://192.168.1.11:11434"),
+        model_name=config.get("model", "cyberagent/DeepSeek-R1-Distill-Qwen-14B-Japanese"),
+        knowledge_base=knowledge_base,
+        activity_memory=activity_memory,
+        sandbox_executor=sandbox_executor,
+        web_search=web_search
+    )
+
+
+# Example usage and testing
+if __name__ == "__main__":
+    async def test_taisho_agent():
+        """Test the enhanced Taisho agent"""
+        # Create agent (without subsystems for testing)
+        agent = TaishoAgent()
+        
+        # Test task
+        task = TaishoTask(
+            id="test_task",
+            description="I2Sè¨­å®šã®æœ€é©åŒ–æ–¹æ³•ã‚’æ•™ãˆã¦ãã ã•ã„",
+            complexity=TaskComplexity.MEDIUM,
+            priority=TaskPriority.HIGH,
+            context="ESP32ã§ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªå‡¦ç†ã‚’è¡Œã†éš›ã®è¨­å®šã«ã¤ã„ã¦",
+            requires_execution=False,
+            requires_latest_info=True,
+            requested_by="test_user"
+        )
+        
+        # Process task
+        response = await agent.process_task(task)
+        
+        print(f"Task processed: {response.success}")
+        print(f"Response: {response.response}")
+        print(f"Reasoning: {response.reasoning}")
+        print(f"Confidence: {response.confidence.value}")
+        print(f"Processing time: {response.processing_time_seconds:.2f}s")
+        print(f"Tools used: {response.tools_used}")
+        
+        # Health check
+        health = await agent.health_check()
+        print(f"Health: {health}")
+    
+    # Run test
+    asyncio.run(test_taisho_agent())
\ No newline at end of file
diff --git a/ashigaru/groq_recorder.py b/ashigaru/groq_recorder.py
new file mode 100644
index 0000000..4988b2b
--- /dev/null
+++ b/ashigaru/groq_recorder.py
@@ -0,0 +1,564 @@
+"""Groq Recorder - 9th Ashigaru (è¶³è»½)
+
+Real-time recording and 60-day summary automation using Groq Llama 3.3 70B.
+
+Key Features:
+  - Real-time interaction recording
+  - Ultra-fast 60-day summaries (5,000 lines â†’ 3 minutes)
+  - Automatic Notion integration
+  - Knowledge extraction and family precepts (å®¶è¨“)
+  - Free tier utilization (14,400 requests/day)
+
+This is the 9th ashigaru that handles all knowledge management for the Shogun system.
+"""
+
+import asyncio
+import json
+import logging
+from datetime import datetime, timedelta
+from pathlib import Path
+from typing import Dict, List, Optional, Any
+import os
+
+try:
+    import groq
+except ImportError:
+    groq = None
+
+logger = logging.getLogger("shogun.ashigaru.groq")
+
+
+class GroqRecorder:
+    """9th Ashigaru - Groq-powered knowledge recorder and summarizer."""
+
+    def __init__(self, api_key: str, notion_integration: Dict[str, Any]):
+        self.api_key = api_key
+        self.notion_config = notion_integration
+        self.client = None
+        
+        # Session tracking
+        self.current_session = None
+        self.session_data = []
+        
+        # Statistics
+        self.stats = {
+            "sessions_started": 0,
+            "interactions_recorded": 0,
+            "summaries_generated": 0,
+            "notion_uploads": 0,
+            "family_precepts_extracted": 0,
+            "groq_requests": 0,
+            "total_tokens": 0,
+        }
+        
+        # Storage paths
+        self.storage_dir = Path("/tmp/shogun_recordings")
+        self.storage_dir.mkdir(exist_ok=True)
+        
+        # Daily request tracking for free tier
+        self.daily_requests = 0
+        self.last_request_date = datetime.now().date()
+        
+        # RPM/TPM tracking for short-term rate limiting
+        self.rpm_requests = []  # List of request timestamps for RPM tracking
+        self.tpm_tokens = []    # List of (timestamp, token_count) for TPM tracking
+        self.rpm_limit = 30     # Groq free tier RPM limit
+        self.tpm_limit = 6000   # Groq free tier TPM limit
+        
+    async def initialize(self) -> None:
+        """Initialize Groq client."""
+        if not self.api_key:
+            logger.warning("[9ç•ªè¶³è»½] Groqã‚­ãƒ¼æœªè¨­å®š - è¨˜éŒ²æ©Ÿèƒ½ç„¡åŠ¹")
+            return
+            
+        if groq is None:
+            logger.error("[9ç•ªè¶³è»½] groqãƒ©ã‚¤ãƒ–ãƒ©ãƒªæœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ« - pip install groq")
+            return
+        
+        self.client = groq.Groq(api_key=self.api_key)
+        logger.info("[9ç•ªè¶³è»½] Groqè¨˜éŒ²ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
+        
+        # Check daily quota
+        self._check_daily_quota()
+        
+    def _check_daily_quota(self) -> None:
+        """Check and reset daily request quota."""
+        today = datetime.now().date()
+        if today != self.last_request_date:
+            self.daily_requests = 0
+            self.last_request_date = today
+            logger.info("[9ç•ªè¶³è»½] æ—¥åˆ¥ã‚¯ã‚©ãƒ¼ã‚¿ãƒªã‚»ãƒƒãƒˆ (14,400/day)")
+    
+    async def start_session(self, task_id: str, prompt: str) -> None:
+        """Start a new recording session."""
+        if not self.client:
+            return
+            
+        self.current_session = {
+            "id": task_id,
+            "start_time": datetime.now().isoformat(),
+            "initial_prompt": prompt,
+            "interactions": [],
+        }
+        
+        self.stats["sessions_started"] += 1
+        logger.info("[9ç•ªè¶³è»½] ã‚»ãƒƒã‚·ãƒ§ãƒ³é–‹å§‹: %s", task_id)
+    
+    async def record_interaction(
+        self, 
+        task_id: str, 
+        agent_type: str, 
+        prompt: str, 
+        response: str
+    ) -> None:
+        """Record an agent interaction."""
+        if not self.client or not self.current_session:
+            return
+            
+        interaction = {
+            "timestamp": datetime.now().isoformat(),
+            "agent": agent_type,
+            "prompt": prompt,
+            "response": response,
+            "token_count": len(prompt.split()) + len(response.split()),  # Rough estimate
+        }
+        
+        self.current_session["interactions"].append(interaction)
+        self.stats["interactions_recorded"] += 1
+        
+        logger.debug("[9ç•ªè¶³è»½] ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³è¨˜éŒ²: %s", agent_type)
+    
+    async def record_completion(
+        self, 
+        task_id: str, 
+        original_prompt: str, 
+        final_result: str, 
+        cost_yen: float
+    ) -> None:
+        """Record task completion."""
+        if not self.client or not self.current_session:
+            return
+            
+        self.current_session.update({
+            "end_time": datetime.now().isoformat(),
+            "final_result": final_result,
+            "cost_yen": cost_yen,
+            "status": "completed"
+        })
+        
+        # Save to disk
+        await self._save_session_to_disk()
+        
+        # Extract knowledge if significant interaction
+        if len(self.current_session["interactions"]) > 2:
+            await self._extract_knowledge_async()
+        
+        self.current_session = None
+        logger.info("[9ç•ªè¶³è»½] ã‚»ãƒƒã‚·ãƒ§ãƒ³å®Œäº†è¨˜éŒ²: %s (Â¥%.0f)", task_id, cost_yen)
+    
+    async def record_failure(
+        self, 
+        task_id: str, 
+        original_prompt: str, 
+        error: str
+    ) -> None:
+        """Record task failure."""
+        if not self.client or not self.current_session:
+            return
+            
+        self.current_session.update({
+            "end_time": datetime.now().isoformat(),
+            "error": error,
+            "status": "failed"
+        })
+        
+        await self._save_session_to_disk()
+        self.current_session = None
+        
+        logger.info("[9ç•ªè¶³è»½] ã‚»ãƒƒã‚·ãƒ§ãƒ³å¤±æ•—è¨˜éŒ²: %s", task_id)
+    
+    async def _save_session_to_disk(self) -> None:
+        """Save session data to disk."""
+        if not self.current_session:
+            return
+            
+        filename = f"session_{self.current_session['id']}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
+        filepath = self.storage_dir / filename
+        
+        try:
+            with open(filepath, 'w', encoding='utf-8') as f:
+                json.dump(self.current_session, f, ensure_ascii=False, indent=2)
+            logger.debug("[9ç•ªè¶³è»½] ã‚»ãƒƒã‚·ãƒ§ãƒ³ä¿å­˜: %s", filename)
+        except Exception as e:
+            logger.error("[9ç•ªè¶³è»½] ã‚»ãƒƒã‚·ãƒ§ãƒ³ä¿å­˜å¤±æ•—: %s", e)
+    
+    async def _extract_knowledge_async(self) -> None:
+        """Extract knowledge and family precepts from session."""
+        # Estimate tokens for the extraction request
+        session_text = self._format_session_for_analysis()
+        estimated_tokens = len(session_text.split()) * 1.5  # Rough estimate
+        
+        if not self._can_make_request(int(estimated_tokens)):
+            logger.info("[9ç•ªè¶³è»½] ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã®ãŸã‚çŸ¥è­˜æŠ½å‡ºã‚’ã‚¹ã‚­ãƒƒãƒ—")
+            return
+            
+        try:
+            # Extract family precepts (å®¶è¨“)
+            precepts = await self._extract_family_precepts(session_text)
+            if precepts:
+                self.stats["family_precepts_extracted"] += 1
+                logger.info("[9ç•ªè¶³è»½] å®¶è¨“æŠ½å‡º: %då€‹", len(precepts))
+                
+        except Exception as e:
+            logger.warning("[9ç•ªè¶³è»½] çŸ¥è­˜æŠ½å‡ºå¤±æ•—: %s", e)
+    
+    def _format_session_for_analysis(self) -> str:
+        """Format session data for Groq analysis."""
+        if not self.current_session:
+            return ""
+            
+        lines = [
+            f"# ã‚»ãƒƒã‚·ãƒ§ãƒ³: {self.current_session['id']}",
+            f"é–‹å§‹æ™‚é–“: {self.current_session['start_time']}",
+            f"åˆæœŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: {self.current_session['initial_prompt']}",
+            "",
+            "## ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³å±¥æ­´",
+        ]
+        
+        for i, interaction in enumerate(self.current_session["interactions"], 1):
+            lines.extend([
+                f"### {i}. {interaction['agent']} ({interaction['timestamp']})",
+                f"**ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ:** {interaction['prompt'][:200]}{'...' if len(interaction['prompt']) > 200 else ''}",
+                f"**å¿œç­”:** {interaction['response'][:500]}{'...' if len(interaction['response']) > 500 else ''}",
+                "",
+            ])
+        
+        return "\n".join(lines)
+    
+    async def _extract_family_precepts(self, session_text: str) -> List[str]:
+        """Extract family precepts (å®¶è¨“) from session using Groq."""
+        if not self.client or not session_text:
+            return []
+            
+        prompt = f"""
+ä»¥ä¸‹ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‹ã‚‰ã€å°†æ¥ã®é–‹ç™ºã§å‚è€ƒã«ãªã‚‹ã€Œå®¶è¨“ã€ï¼ˆæ±ºå®šäº‹é …ãƒ»å­¦ã³ãƒ»åŸå‰‡ï¼‰ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚
+
+{session_text}
+
+æŠ½å‡ºæ¡ä»¶:
+1. å…·ä½“çš„ã§å®Ÿç”¨çš„ãªæ•™è¨“
+2. å†åˆ©ç”¨å¯èƒ½ãªçŸ¥è­˜
+3. é‡è¦ãªæ±ºå®šäº‹é …
+4. ã‚¨ãƒ©ãƒ¼å¯¾å‡¦æ³•
+5. æœ€é©åŒ–ã®ãƒã‚¤ãƒ³ãƒˆ
+
+å®¶è¨“å½¢å¼ã§å‡ºåŠ›ï¼ˆä¾‹ï¼šã€ŒESP32ã®SPIè¨­å®šã§ã¯å¿…ãšDMAè¨­å®šã‚’ç¢ºèªã›ã‚ˆã€ï¼‰:
+"""
+        
+        try:
+            response = await self._call_groq(
+                prompt=prompt,
+                system="ã‚ãªãŸã¯çŸ¥è­˜æŠ½å‡ºã®å°‚é–€å®¶ã§ã™ã€‚ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‹ã‚‰ä¾¡å€¤ã‚ã‚‹æ•™è¨“ã‚’æŠ½å‡ºã—ã€å°†æ¥ã®å‚è€ƒã¨ãªã‚‹å®¶è¨“ã¨ã—ã¦æ•´ç†ã—ã¦ãã ã•ã„ã€‚",
+                max_tokens=1000
+            )
+            
+            if response:
+                # Parse precepts from response
+                precepts = []
+                for line in response.split('\n'):
+                    line = line.strip()
+                    if line and ('å®¶è¨“' in line or 'æ•™è¨“' in line or line.endswith('ã¹ã—') or line.endswith('ã›ã‚ˆ')):
+                        precepts.append(line)
+                
+                return precepts
+                
+        except Exception as e:
+            logger.error("[9ç•ªè¶³è»½] å®¶è¨“æŠ½å‡ºã‚¨ãƒ©ãƒ¼: %s", e)
+        
+        return []
+    
+    async def generate_60day_summary(self) -> str:
+        """Generate 60-day summary of all recorded sessions."""
+        if not self.client:
+            return "Groqã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆæœªåˆæœŸåŒ–"
+            
+        logger.info("[9ç•ªè¶³è»½] 60æ—¥è¦ç´„ç”Ÿæˆé–‹å§‹")
+        
+        # Collect sessions from last 60 days
+        cutoff_date = datetime.now() - timedelta(days=60)
+        sessions = self._load_recent_sessions(cutoff_date)
+        
+        if not sessions:
+            return "å¯¾è±¡æœŸé–“ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"
+        
+        # Generate summary using Groq's ultra-fast processing
+        summary = await self._generate_summary_with_groq(sessions)
+        
+        if summary:
+            self.stats["summaries_generated"] += 1
+            logger.info("[9ç•ªè¶³è»½] 60æ—¥è¦ç´„å®Œäº† (%d ã‚»ãƒƒã‚·ãƒ§ãƒ³)", len(sessions))
+        
+        return summary or "è¦ç´„ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ"
+    
+    def _load_recent_sessions(self, cutoff_date: datetime) -> List[Dict]:
+        """Load sessions from the last N days."""
+        sessions = []
+        
+        if not self.storage_dir.exists():
+            return sessions
+        
+        for json_file in self.storage_dir.glob("session_*.json"):
+            try:
+                with open(json_file, 'r', encoding='utf-8') as f:
+                    session = json.load(f)
+                    
+                # Check if session is within date range
+                start_time = datetime.fromisoformat(session.get('start_time', ''))
+                if start_time >= cutoff_date:
+                    sessions.append(session)
+                    
+            except Exception as e:
+                logger.warning("[9ç•ªè¶³è»½] ã‚»ãƒƒã‚·ãƒ§ãƒ³èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: %s", e)
+        
+        return sorted(sessions, key=lambda x: x.get('start_time', ''))
+    
+    async def _generate_summary_with_groq(self, sessions: List[Dict]) -> str:
+        """Generate comprehensive summary using Groq's speed."""
+        if not sessions:
+            return ""
+            
+        # Prepare sessions text (optimized for Groq processing)
+        sessions_text = self._format_sessions_for_summary(sessions)
+        
+        prompt = f"""
+ä»¥ä¸‹ã®{len(sessions)}ä»¶ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’è¦ç´„ã—ã€åŒ…æ‹¬çš„ãª60æ—¥ãƒ¬ãƒãƒ¼ãƒˆã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚
+
+{sessions_text}
+
+è¦ç´„è¦ä»¶:
+1. ä¸»è¦ãªæˆæœã¨å®Ÿè£…å†…å®¹
+2. é »ç¹ã«ç™ºç”Ÿã—ãŸå•é¡Œã¨ãã®è§£æ±ºç­–
+3. ã‚³ã‚¹ãƒˆåˆ†æï¼ˆÂ¥è¨˜è¼‰ãŒã‚ã‚‹ã‚‚ã®ï¼‰
+4. æŠ€è¡“çš„ãªå­¦ã³ãƒ»å®¶è¨“
+5. æ”¹å–„ææ¡ˆ
+
+Markdownå½¢å¼ã§å‡ºåŠ›:
+"""
+        
+        try:
+            return await self._call_groq(
+                prompt=prompt,
+                system="ã‚ãªãŸã¯é–‹ç™ºãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®åˆ†æå°‚é–€å®¶ã§ã™ã€‚60æ—¥é–“ã®æ´»å‹•ã‚’è¦ç´„ã—ã€ä¾¡å€¤ã‚ã‚‹æ´å¯Ÿã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚",
+                max_tokens=4000
+            )
+        except Exception as e:
+            logger.error("[9ç•ªè¶³è»½] Groqè¦ç´„ç”Ÿæˆã‚¨ãƒ©ãƒ¼: %s", e)
+            return f"è¦ç´„ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}"
+    
+    def _format_sessions_for_summary(self, sessions: List[Dict]) -> str:
+        """Format sessions for Groq summary processing."""
+        lines = []
+        
+        for i, session in enumerate(sessions, 1):
+            lines.extend([
+                f"## ã‚»ãƒƒã‚·ãƒ§ãƒ³ {i}: {session.get('id', 'Unknown')}",
+                f"æ™‚é–“: {session.get('start_time', '')} - {session.get('end_time', '')}",
+                f"åˆæœŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: {session.get('initial_prompt', '')[:200]}",
+                f"ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹: {session.get('status', 'unknown')}",
+            ])
+            
+            if session.get('cost_yen'):
+                lines.append(f"ã‚³ã‚¹ãƒˆ: Â¥{session['cost_yen']}")
+            
+            if session.get('error'):
+                lines.append(f"ã‚¨ãƒ©ãƒ¼: {session['error']}")
+            elif session.get('final_result'):
+                result = session['final_result'][:300]
+                lines.append(f"çµæœ: {result}{'...' if len(session['final_result']) > 300 else ''}")
+            
+            # Add significant interactions
+            interactions = session.get('interactions', [])
+            if interactions:
+                lines.append(f"ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³æ•°: {len(interactions)}")
+                for interaction in interactions[:2]:  # Limit to first 2
+                    lines.append(f"  - {interaction.get('agent', '')}: {interaction.get('response', '')[:100][:100]}")
+            
+            lines.append("")  # Blank line
+        
+        return "\n".join(lines)
+    
+    async def _call_groq(
+        self, 
+        prompt: str, 
+        system: str = "", 
+        max_tokens: int = 2000
+    ) -> Optional[str]:
+        """Call Groq API with rate limiting and exponential backoff."""
+        if not self._can_make_request():
+            logger.warning("[9ç•ªè¶³è»½] Groqæ—¥åˆ¥ä¸Šé™åˆ°é” (14,400/day)")
+            return None
+        
+        # Exponential backoff parameters
+        max_retries = 5
+        base_delay = 1.0  # Start with 1 second
+        max_delay = 60.0  # Max 60 seconds
+        
+        for attempt in range(max_retries):
+            try:
+                messages = []
+                if system:
+                    messages.append({"role": "system", "content": system})
+                messages.append({"role": "user", "content": prompt})
+                
+                response = self.client.chat.completions.create(
+                    model="llama-3.3-70b-versatile",
+                    messages=messages,
+                    max_tokens=max_tokens,
+                    temperature=0.3,
+                )
+                
+                self._track_request(response.usage.total_tokens if response.usage else max_tokens)
+                
+                return response.choices[0].message.content
+                
+            except Exception as e:
+                error_msg = str(e).lower()
+                
+                # Check for rate limit errors
+                if "rate_limit" in error_msg or "429" in error_msg:
+                    if attempt < max_retries - 1:
+                        # Calculate delay with exponential backoff and jitter
+                        delay = min(base_delay * (2 ** attempt), max_delay)
+                        jitter = delay * 0.1 * (0.5 - abs(hash(prompt) % 100 - 50) / 100)
+                        total_delay = delay + jitter
+                        
+                        logger.warning("[9ç•ªè¶³è»½] ãƒ¬ãƒ¼ãƒˆåˆ¶é™ - %då›ç›®å†è©¦è¡Œ (%.1fç§’å¾Œ)", 
+                                     attempt + 1, total_delay)
+                        await asyncio.sleep(total_delay)
+                        continue
+                    else:
+                        logger.error("[9ç•ªè¶³è»½] ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã§æœ€å¤§å†è©¦è¡Œå›æ•°åˆ°é”")
+                        return None
+                
+                # Check for temporary server errors (502, 503, 504)
+                elif any(code in error_msg for code in ["502", "503", "504", "timeout"]):
+                    if attempt < max_retries - 1:
+                        delay = min(base_delay * (1.5 ** attempt), max_delay / 2)
+                        logger.warning("[9ç•ªè¶³è»½] ä¸€æ™‚çš„ã‚¨ãƒ©ãƒ¼ - %då›ç›®å†è©¦è¡Œ (%.1fç§’å¾Œ)", 
+                                     attempt + 1, delay)
+                        await asyncio.sleep(delay)
+                        continue
+                    else:
+                        logger.error("[9ç•ªè¶³è»½] ä¸€æ™‚çš„ã‚¨ãƒ©ãƒ¼ã§æœ€å¤§å†è©¦è¡Œå›æ•°åˆ°é”")
+                        return None
+                
+                # For other errors, don't retry
+                else:
+                    logger.error("[9ç•ªè¶³è»½] Groq APIå‘¼ã³å‡ºã—ã‚¨ãƒ©ãƒ¼: %s", e)
+                    return None
+        
+        return None
+    
+    def _can_make_request(self, estimated_tokens: int = 500) -> bool:
+        """Check if we can make another Groq request (daily, RPM, TPM limits)."""
+        self._check_daily_quota()
+        
+        # Check daily limit
+        if self.daily_requests >= 14400:
+            return False
+        
+        # Check RPM limit
+        now = datetime.now()
+        one_minute_ago = now - timedelta(minutes=1)
+        
+        # Clean old requests (older than 1 minute)
+        self.rpm_requests = [ts for ts in self.rpm_requests if ts > one_minute_ago]
+        
+        if len(self.rpm_requests) >= self.rpm_limit:
+            logger.debug("[9ç•ªè¶³è»½] RPMä¸Šé™ã«è¿‘ã¥ã„ã¦ã„ã¾ã™ (%d/%d)", len(self.rpm_requests), self.rpm_limit)
+            return False
+        
+        # Check TPM limit
+        self.tpm_tokens = [(ts, tokens) for ts, tokens in self.tpm_tokens if ts > one_minute_ago]
+        current_tokens = sum(tokens for _, tokens in self.tpm_tokens)
+        
+        if current_tokens + estimated_tokens > self.tpm_limit:
+            logger.debug("[9ç•ªè¶³è»½] TPMä¸Šé™ã«è¿‘ã¥ã„ã¦ã„ã¾ã™ (%d+%d > %d)", 
+                        current_tokens, estimated_tokens, self.tpm_limit)
+            return False
+        
+        return True
+    
+    def _track_request(self, tokens: int) -> None:
+        """Track request for daily, RPM, and TPM quotas."""
+        now = datetime.now()
+        
+        # Daily tracking
+        self.daily_requests += 1
+        self.stats["groq_requests"] += 1
+        self.stats["total_tokens"] += tokens
+        
+        # RPM tracking
+        self.rpm_requests.append(now)
+        
+        # TPM tracking
+        self.tpm_tokens.append((now, tokens))
+        
+        if self.daily_requests % 1000 == 0:
+            logger.info("[9ç•ªè¶³è»½] Groqä½¿ç”¨çŠ¶æ³: %d/14,400 requests", self.daily_requests)
+    
+    async def finalize_session(self) -> None:
+        """Finalize current session if any."""
+        if self.current_session:
+            self.current_session.update({
+                "end_time": datetime.now().isoformat(),
+                "status": "interrupted"
+            })
+            await self._save_session_to_disk()
+            self.current_session = None
+    
+    def get_status(self) -> Dict[str, Any]:
+        """Get recorder status."""
+        return {
+            "initialized": self.client is not None,
+            "current_session": self.current_session['id'] if self.current_session else None,
+            "daily_requests": self.daily_requests,
+            "daily_quota_remaining": 14400 - self.daily_requests,
+            "stats": dict(self.stats),
+        }
+    
+    def show_stats(self) -> str:
+        """Format stats for display."""
+        s = self.stats
+        remaining = 14400 - self.daily_requests
+        
+        # Calculate current RPM/TPM usage
+        now = datetime.now()
+        one_minute_ago = now - timedelta(minutes=1)
+        current_rpm = len([ts for ts in self.rpm_requests if ts > one_minute_ago])
+        current_tpm = sum(tokens for ts, tokens in self.tpm_tokens if ts > one_minute_ago)
+        
+        lines = [
+            "=" * 50,
+            "ğŸ¯ 9ç•ªè¶³è»½ (Groqè¨˜éŒ²ä¿‚) çµ±è¨ˆ",
+            "=" * 50,
+            f"ã‚»ãƒƒã‚·ãƒ§ãƒ³é–‹å§‹: {s['sessions_started']}å›",
+            f"ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³è¨˜éŒ²: {s['interactions_recorded']}å›",
+            f"è¦ç´„ç”Ÿæˆ: {s['summaries_generated']}å›",
+            f"å®¶è¨“æŠ½å‡º: {s['family_precepts_extracted']}å›",
+            f"NotionæŠ•ç¨¿: {s['notion_uploads']}å›",
+            "",
+            "Groqä½¿ç”¨çŠ¶æ³:",
+            f"  æœ¬æ—¥ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆ: {self.daily_requests}/14,400",
+            f"  æ®‹ã‚Š: {remaining}å›",
+            f"  ç´¯è¨ˆãƒˆãƒ¼ã‚¯ãƒ³: {s['total_tokens']:,}",
+            "",
+            "çŸ­æœŸåˆ¶é™çŠ¶æ³:",
+            f"  RPM (åˆ†é–“ãƒªã‚¯ã‚¨ã‚¹ãƒˆ): {current_rpm}/{self.rpm_limit}",
+            f"  TPM (åˆ†é–“ãƒˆãƒ¼ã‚¯ãƒ³): {current_tpm:,}/{self.tpm_limit:,}",
+            "",
+            "ğŸ’° ã‚³ã‚¹ãƒˆ: Â¥0 (Free Tier) â­",
+            "ğŸ›¡ï¸  ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾å¿œ: Exponential Backoff âœ…",
+            "=" * 50,
+        ]
+        return "\n".join(lines)
diff --git a/ashigaru/ollama_web_search.py b/ashigaru/ollama_web_search.py
new file mode 100644
index 0000000..dbd61b1
--- /dev/null
+++ b/ashigaru/ollama_web_search.py
@@ -0,0 +1,610 @@
+"""
+å°†è»ã‚·ã‚¹ãƒ†ãƒ  v8.0 - Ollama Web Search Integration (10ç•ªè¶³è»½)
+æœ€æ–°æƒ…å ±å°‚é–€è¶³è»½: Ollama Web Search APIã‚’ä½¿ç”¨ã—ãŸæœ€æ–°æƒ…å ±å–å¾—ãƒ»RAGè‡ªå‹•ä¿å­˜
+
+Features:
+- Ollama Web Search API integration
+- Real-time web search capabilities
+- Automatic RAG knowledge base integration
+- Multi-language search (Japanese/English)
+- Cost-effective (Â¥0 with free tier)
+- Knowledge gap filling for R1 and Claude agents
+"""
+
+import asyncio
+import json
+import logging
+from datetime import datetime
+from typing import List, Dict, Any, Optional
+from dataclasses import dataclass
+import hashlib
+import httpx
+from urllib.parse import quote_plus
+
+# Local imports
+import sys
+from pathlib import Path
+sys.path.append(str(Path(__file__).parent.parent))
+
+try:
+    from core.knowledge_base import KnowledgeBase, KnowledgeEntry, DataSource
+except ImportError:
+    # Fallback for standalone testing
+    KnowledgeBase = None
+    KnowledgeEntry = None
+    DataSource = None
+
+# Configure logging
+logging.basicConfig(level=logging.INFO)
+logger = logging.getLogger(__name__)
+
+
+@dataclass
+class SearchQuery:
+    """Web search query structure"""
+    id: str
+    query: str
+    language: str = "ja"
+    max_results: int = 5
+    requested_by: str = "unknown"  # Which agent requested this
+    context: Optional[str] = None  # Additional context for the search
+    
+    def __post_init__(self):
+        if not self.id:
+            # Generate ID from query and timestamp
+            query_hash = hashlib.md5(f"{self.query}{datetime.utcnow()}".encode()).hexdigest()[:8]
+            self.id = f"search_{query_hash}"
+
+
+@dataclass 
+class SearchResult:
+    """Individual search result"""
+    title: str
+    url: str
+    snippet: str
+    published_date: Optional[str] = None
+    source_domain: Optional[str] = None
+    relevance_score: Optional[float] = None
+
+
+@dataclass
+class SearchResponse:
+    """Complete search response"""
+    query_id: str
+    query: str
+    results: List[SearchResult]
+    total_results: int
+    search_time_ms: int
+    language: str
+    requested_by: str
+    executed_at: datetime = None
+    knowledge_entries_created: int = 0
+    
+    def __post_init__(self):
+        if self.executed_at is None:
+            self.executed_at = datetime.utcnow()
+    
+    @property
+    def success(self) -> bool:
+        return len(self.results) > 0
+
+
+class OllamaWebSearch:
+    """
+    å°†è»ã‚·ã‚¹ãƒ†ãƒ  Ollama Web Search Implementation (10ç•ªè¶³è»½)
+    
+    Provides real-time web search capabilities using Ollama's Web Search API:
+    - Fill knowledge gaps for R1 (DeepSeek cutoff: 2024-11)
+    - Provide latest information to Claude (Sonnet/Opus cutoff: 2025-01)  
+    - Automatically save results to RAG knowledge base
+    - Support multi-language searches
+    - Cost-effective with free tier usage
+    """
+    
+    def __init__(
+        self,
+        api_key: Optional[str] = None,
+        base_url: str = "https://api.ollama.com",
+        knowledge_base: Optional[KnowledgeBase] = None,
+        auto_rag_integration: bool = True,
+        free_tier_limit: int = 1000
+    ):
+        self.api_key = api_key
+        self.base_url = base_url.rstrip('/')
+        self.knowledge_base = knowledge_base
+        self.auto_rag_integration = auto_rag_integration
+        self.free_tier_limit = free_tier_limit
+        
+        # Search history for analysis
+        self.search_history: List[SearchResponse] = []
+        
+        # Request statistics
+        self.monthly_requests = 0
+        self.last_reset_date = datetime.utcnow().replace(day=1, hour=0, minute=0, second=0, microsecond=0)
+        
+        logger.info(f"OllamaWebSearch initialized - Auto RAG: {auto_rag_integration}")
+    
+    async def search(self, query: SearchQuery) -> SearchResponse:
+        """
+        Perform web search using Ollama Web Search API
+        
+        Args:
+            query: Search query object
+            
+        Returns:
+            SearchResponse with results and metadata
+        """
+        start_time = datetime.utcnow()
+        
+        try:
+            # Check free tier limits
+            if not self._check_rate_limits():
+                logger.warning(f"Monthly rate limit exceeded: {self.monthly_requests}/{self.free_tier_limit}")
+                return SearchResponse(
+                    query_id=query.id,
+                    query=query.query,
+                    results=[],
+                    total_results=0,
+                    search_time_ms=0,
+                    language=query.language,
+                    requested_by=query.requested_by
+                )
+            
+            # Perform the search
+            search_results = await self._execute_search(query)
+            
+            # Process results
+            processed_results = self._process_results(search_results, query)
+            
+            # Calculate search time
+            search_time_ms = int((datetime.utcnow() - start_time).total_seconds() * 1000)
+            
+            # Create response object
+            response = SearchResponse(
+                query_id=query.id,
+                query=query.query,
+                results=processed_results,
+                total_results=len(processed_results),
+                search_time_ms=search_time_ms,
+                language=query.language,
+                requested_by=query.requested_by
+            )
+            
+            # Auto-save to RAG if enabled
+            if self.auto_rag_integration and self.knowledge_base and processed_results:
+                knowledge_entries_created = await self._save_to_knowledge_base(query, processed_results)
+                response.knowledge_entries_created = knowledge_entries_created
+            
+            # Update statistics
+            self.monthly_requests += 1
+            self.search_history.append(response)
+            
+            # Keep only last 50 searches in memory
+            if len(self.search_history) > 50:
+                self.search_history = self.search_history[-50:]
+            
+            logger.info(f"Search completed: '{query.query}' - {len(processed_results)} results in {search_time_ms}ms")
+            return response
+            
+        except Exception as e:
+            logger.error(f"Search failed for query '{query.query}': {e}")
+            return SearchResponse(
+                query_id=query.id,
+                query=query.query,
+                results=[],
+                total_results=0,
+                search_time_ms=int((datetime.utcnow() - start_time).total_seconds() * 1000),
+                language=query.language,
+                requested_by=query.requested_by
+            )
+    
+    async def _execute_search(self, query: SearchQuery) -> Dict[str, Any]:
+        """Execute the actual search API call"""
+        try:
+            # Prepare search parameters
+            search_params = {
+                "query": query.query,
+                "language": query.language,
+                "num_results": query.max_results
+            }
+            
+            # Add context if provided
+            if query.context:
+                search_params["context"] = query.context
+            
+            # Prepare headers
+            headers = {
+                "Content-Type": "application/json",
+                "User-Agent": "ShogunSystem-v8.0-Ashigaru10"
+            }
+            
+            if self.api_key:
+                headers["Authorization"] = f"Bearer {self.api_key}"
+            
+            # Make the API request
+            async with httpx.AsyncClient(timeout=30.0) as client:
+                response = await client.post(
+                    f"{self.base_url}/api/web-search",
+                    json=search_params,
+                    headers=headers
+                )
+                
+                if response.status_code == 200:
+                    return response.json()
+                elif response.status_code == 429:
+                    # Rate limit exceeded
+                    logger.warning("Rate limit exceeded")
+                    return {"error": "rate_limit_exceeded", "results": []}
+                elif response.status_code == 401:
+                    # Authentication error
+                    logger.error("Authentication failed - check API key")
+                    return {"error": "authentication_failed", "results": []}
+                else:
+                    logger.error(f"API request failed: {response.status_code} - {response.text}")
+                    return {"error": f"api_error_{response.status_code}", "results": []}
+                    
+        except httpx.TimeoutException:
+            logger.error("Search request timed out")
+            return {"error": "timeout", "results": []}
+        except Exception as e:
+            logger.error(f"Search request exception: {e}")
+            return {"error": str(e), "results": []}
+    
+    def _process_results(self, api_response: Dict[str, Any], query: SearchQuery) -> List[SearchResult]:
+        """Process raw API response into SearchResult objects"""
+        try:
+            if "error" in api_response:
+                logger.error(f"API returned error: {api_response['error']}")
+                return []
+            
+            raw_results = api_response.get("results", [])
+            processed_results = []
+            
+            for idx, raw_result in enumerate(raw_results):
+                try:
+                    # Extract fields with fallbacks
+                    title = raw_result.get("title", f"Search Result {idx + 1}")
+                    url = raw_result.get("url", "")
+                    snippet = raw_result.get("snippet", raw_result.get("description", ""))
+                    
+                    # Optional fields
+                    published_date = raw_result.get("published_date")
+                    source_domain = raw_result.get("source", raw_result.get("domain"))
+                    relevance_score = raw_result.get("score", raw_result.get("relevance"))
+                    
+                    # Create SearchResult object
+                    result = SearchResult(
+                        title=title.strip(),
+                        url=url.strip(),
+                        snippet=snippet.strip(),
+                        published_date=published_date,
+                        source_domain=source_domain,
+                        relevance_score=relevance_score
+                    )
+                    
+                    processed_results.append(result)
+                    
+                except Exception as e:
+                    logger.warning(f"Failed to process search result {idx}: {e}")
+                    continue
+            
+            return processed_results
+            
+        except Exception as e:
+            logger.error(f"Failed to process search results: {e}")
+            return []
+    
+    async def _save_to_knowledge_base(self, query: SearchQuery, results: List[SearchResult]) -> int:
+        """Save search results to RAG knowledge base"""
+        try:
+            if not self.knowledge_base:
+                return 0
+            
+            entries_created = 0
+            
+            for idx, result in enumerate(results):
+                try:
+                    # Create knowledge entry
+                    entry_id = f"web_search_{query.id}_{idx}"
+                    
+                    # Combine title and snippet for content
+                    content = f"{result.snippet}"
+                    if result.published_date:
+                        content += f"\n\nç™ºè¡Œæ—¥: {result.published_date}"
+                    if result.source_domain:
+                        content += f"\nå‡ºå…¸: {result.source_domain}"
+                    
+                    # Metadata
+                    metadata = {
+                        "search_query": query.query,
+                        "requested_by": query.requested_by,
+                        "search_context": query.context,
+                        "relevance_score": result.relevance_score,
+                        "source_domain": result.source_domain,
+                        "published_date": result.published_date,
+                        "search_timestamp": datetime.utcnow().isoformat()
+                    }
+                    
+                    # Create knowledge entry
+                    knowledge_entry = KnowledgeEntry(
+                        id=entry_id,
+                        title=result.title,
+                        content=content,
+                        source=DataSource.OLLAMA_WEB_SEARCH,
+                        url=result.url,
+                        metadata=metadata,
+                        language=query.language
+                    )
+                    
+                    # Add to knowledge base
+                    success = await self.knowledge_base.add_entry(knowledge_entry)
+                    if success:
+                        entries_created += 1
+                        
+                except Exception as e:
+                    logger.warning(f"Failed to save search result {idx} to knowledge base: {e}")
+                    continue
+            
+            logger.info(f"Saved {entries_created} search results to knowledge base")
+            return entries_created
+            
+        except Exception as e:
+            logger.error(f"Failed to save results to knowledge base: {e}")
+            return 0
+    
+    def _check_rate_limits(self) -> bool:
+        """Check if we're within rate limits"""
+        current_date = datetime.utcnow().replace(day=1, hour=0, minute=0, second=0, microsecond=0)
+        
+        # Reset counter if new month
+        if current_date > self.last_reset_date:
+            self.monthly_requests = 0
+            self.last_reset_date = current_date
+        
+        return self.monthly_requests < self.free_tier_limit
+    
+    async def quick_search(
+        self,
+        query_text: str,
+        language: str = "ja",
+        requested_by: str = "unknown",
+        max_results: int = 3
+    ) -> List[str]:
+        """
+        Quick search that returns just the snippet texts
+        Useful for simple information retrieval
+        """
+        query = SearchQuery(
+            id="",  # Will be auto-generated
+            query=query_text,
+            language=language,
+            max_results=max_results,
+            requested_by=requested_by
+        )
+        
+        response = await self.search(query)
+        
+        # Return just the snippets
+        return [result.snippet for result in response.results]
+    
+    async def search_for_context(
+        self,
+        topic: str,
+        context_query: str,
+        language: str = "ja",
+        requested_by: str = "taisho"
+    ) -> Optional[str]:
+        """
+        Search for specific context about a topic
+        Returns a condensed summary of relevant information
+        """
+        # Construct enhanced query
+        enhanced_query = f"{topic} {context_query}"
+        
+        query = SearchQuery(
+            id="",
+            query=enhanced_query,
+            language=language,
+            max_results=5,
+            requested_by=requested_by,
+            context=f"Looking for information about {topic} specifically related to {context_query}"
+        )
+        
+        response = await self.search(query)
+        
+        if not response.success:
+            return None
+        
+        # Combine snippets into a coherent summary
+        snippets = [result.snippet for result in response.results[:3]]  # Top 3 results
+        combined_info = " ".join(snippets)
+        
+        # Simple deduplication
+        sentences = combined_info.split('ã€‚')
+        unique_sentences = []
+        seen = set()
+        
+        for sentence in sentences:
+            sentence = sentence.strip()
+            if sentence and sentence not in seen and len(sentence) > 10:
+                unique_sentences.append(sentence)
+                seen.add(sentence)
+        
+        if unique_sentences:
+            return 'ã€‚'.join(unique_sentences[:5]) + 'ã€‚'  # Max 5 sentences
+        else:
+            return combined_info[:500] + "..." if len(combined_info) > 500 else combined_info
+    
+    async def get_latest_info(
+        self,
+        technology: str,
+        info_type: str = "æœ€æ–°æƒ…å ±",
+        language: str = "ja"
+    ) -> Optional[str]:
+        """
+        Get latest information about a specific technology
+        Useful for filling knowledge gaps in R1 and Claude
+        """
+        query_text = f"{technology} {info_type} 2025"  # Include current year
+        
+        return await self.search_for_context(
+            topic=technology,
+            context_query=info_type,
+            language=language,
+            requested_by="knowledge_gap_filling"
+        )
+    
+    def get_usage_statistics(self) -> Dict[str, Any]:
+        """Get usage statistics"""
+        try:
+            if not self.search_history:
+                return {"message": "No search history available"}
+            
+            total_searches = len(self.search_history)
+            successful_searches = sum(1 for s in self.search_history if s.success)
+            
+            # Requester distribution
+            requester_stats = {}
+            language_stats = {}
+            
+            for search in self.search_history:
+                # Requester stats
+                requester = search.requested_by
+                if requester not in requester_stats:
+                    requester_stats[requester] = {"total": 0, "successful": 0}
+                requester_stats[requester]["total"] += 1
+                if search.success:
+                    requester_stats[requester]["successful"] += 1
+                
+                # Language stats
+                lang = search.language
+                if lang not in language_stats:
+                    language_stats[lang] = 0
+                language_stats[lang] += 1
+            
+            # Average results and search time
+            avg_results = sum(s.total_results for s in self.search_history) / total_searches if total_searches > 0 else 0
+            avg_search_time = sum(s.search_time_ms for s in self.search_history) / total_searches if total_searches > 0 else 0
+            
+            # Knowledge base integration stats
+            total_kb_entries = sum(s.knowledge_entries_created for s in self.search_history)
+            
+            stats = {
+                "total_searches": total_searches,
+                "successful_searches": successful_searches,
+                "success_rate_percent": round(successful_searches / total_searches * 100, 2) if total_searches > 0 else 0,
+                "monthly_requests_used": self.monthly_requests,
+                "monthly_limit": self.free_tier_limit,
+                "usage_percent": round(self.monthly_requests / self.free_tier_limit * 100, 2),
+                "average_results_per_search": round(avg_results, 1),
+                "average_search_time_ms": round(avg_search_time, 1),
+                "requester_distribution": requester_stats,
+                "language_distribution": language_stats,
+                "knowledge_base_entries_created": total_kb_entries,
+                "auto_rag_enabled": self.auto_rag_integration
+            }
+            
+            return stats
+            
+        except Exception as e:
+            logger.error(f"Failed to get usage statistics: {e}")
+            return {"error": str(e)}
+    
+    async def health_check(self) -> Dict[str, Any]:
+        """Perform health check of the web search service"""
+        try:
+            # Test search
+            test_query = SearchQuery(
+                id="health_check",
+                query="test search",
+                language="en",
+                max_results=1,
+                requested_by="health_check"
+            )
+            
+            start_time = datetime.utcnow()
+            response = await self.search(test_query)
+            response_time_ms = int((datetime.utcnow() - start_time).total_seconds() * 1000)
+            
+            # Knowledge base connectivity
+            kb_status = "unknown"
+            if self.knowledge_base:
+                kb_health = await self.knowledge_base.health_check()
+                kb_status = kb_health.get("status", "unknown")
+            
+            health_status = {
+                "status": "healthy" if response.success else "unhealthy",
+                "search_api_accessible": response.success or response.search_time_ms > 0,
+                "response_time_ms": response_time_ms,
+                "rate_limit_ok": self._check_rate_limits(),
+                "monthly_usage": f"{self.monthly_requests}/{self.free_tier_limit}",
+                "knowledge_base_status": kb_status,
+                "auto_rag_integration": self.auto_rag_integration,
+                "checked_at": datetime.utcnow().isoformat()
+            }
+            
+            return health_status
+            
+        except Exception as e:
+            logger.error(f"Health check failed: {e}")
+            return {
+                "status": "error",
+                "error": str(e),
+                "checked_at": datetime.utcnow().isoformat()
+            }
+
+
+# Factory function
+def create_ollama_web_search(config: Dict[str, Any], knowledge_base: Optional[KnowledgeBase] = None) -> OllamaWebSearch:
+    """Create OllamaWebSearch instance from configuration"""
+    return OllamaWebSearch(
+        api_key=config.get("api_key"),
+        base_url=config.get("base_url", "https://api.ollama.com"),
+        knowledge_base=knowledge_base,
+        auto_rag_integration=config.get("auto_rag_integration", True),
+        free_tier_limit=config.get("free_tier_limit", 1000)
+    )
+
+
+# Example usage and testing
+if __name__ == "__main__":
+    async def test_ollama_web_search():
+        """Test the Ollama web search functionality"""
+        # Initialize without knowledge base for testing
+        search_service = OllamaWebSearch(auto_rag_integration=False)
+        
+        # Test search
+        query = SearchQuery(
+            id="test_search",
+            query="React 19 æ–°æ©Ÿèƒ½",
+            language="ja",
+            max_results=3,
+            requested_by="test",
+            context="React.jsã®æœ€æ–°ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã®æ©Ÿèƒ½ã«ã¤ã„ã¦èª¿ã¹ã¦ã„ã¾ã™"
+        )
+        
+        print(f"Performing search: '{query.query}'")
+        response = await search_service.search(query)
+        
+        print(f"Search results: {response.total_results}")
+        for i, result in enumerate(response.results, 1):
+            print(f"{i}. {result.title}")
+            print(f"   URL: {result.url}")
+            print(f"   Snippet: {result.snippet[:100]}...")
+            print()
+        
+        # Test quick search
+        print("Testing quick search...")
+        snippets = await search_service.quick_search("Python 3.13 æ–°æ©Ÿèƒ½", language="ja")
+        print(f"Quick search returned {len(snippets)} snippets")
+        
+        # Usage statistics
+        stats = search_service.get_usage_statistics()
+        print(f"Usage statistics: {stats}")
+        
+        # Health check
+        health = await search_service.health_check()
+        print(f"Health status: {health}")
+    
+    # Run test
+    asyncio.run(test_ollama_web_search())
\ No newline at end of file
diff --git a/cli.py b/cli.py
new file mode 100644
index 0000000..b694e8d
--- /dev/null
+++ b/cli.py
@@ -0,0 +1,483 @@
+#!/usr/bin/env python3
+"""å°†è»ã‚·ã‚¹ãƒ†ãƒ  - CLI (IDE Console Interface)
+
+IDEã®ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ã‹ã‚‰å‘¼ã³å‡ºã›ã‚‹CLIã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã€‚
+
+Usage:
+    shogun repl                     # å¯¾è©±ãƒ¢ãƒ¼ãƒ‰
+    shogun ask "prompt"             # ã‚¿ã‚¹ã‚¯å®Ÿè¡Œ
+    shogun ask -m company "prompt"  # ä¸­éšŠãƒ¢ãƒ¼ãƒ‰ (Â¥0)
+    shogun ask -a taisho "prompt"   # ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆæŒ‡å®š
+    shogun mode                     # ç¾åœ¨ã®ãƒ¢ãƒ¼ãƒ‰è¡¨ç¤º
+    shogun mode company             # ä¸­éšŠãƒ¢ãƒ¼ãƒ‰ã«åˆ‡æ›¿
+    shogun status                   # ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹
+    shogun stats                    # ã‚³ã‚¹ãƒˆçµ±è¨ˆ
+    shogun health                   # ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯
+    shogun server                   # REST APIã‚µãƒ¼ãƒãƒ¼èµ·å‹•
+    shogun pipe                     # stdin ãƒ‘ã‚¤ãƒ—å…¥åŠ›
+    shogun maintenance run          # æœˆæ¬¡ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹å®Ÿè¡Œ
+    shogun maintenance reports      # éå»ãƒ¬ãƒãƒ¼ãƒˆä¸€è¦§
+    shogun maintenance next         # æ¬¡å›ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹æ—¥
+"""
+
+import argparse
+import asyncio
+import os
+import sys
+from pathlib import Path
+
+# Ensure project root on path
+PROJECT_ROOT = Path(__file__).resolve().parent.parent
+if str(PROJECT_ROOT) not in sys.path:
+    sys.path.insert(0, str(PROJECT_ROOT))
+
+
+def setup_logging(verbose: bool = False) -> None:
+    import logging
+    level = logging.DEBUG if verbose else logging.INFO
+    logging.basicConfig(
+        level=level,
+        format="%(message)s",
+        handlers=[logging.StreamHandler(sys.stderr)],
+    )
+    # Suppress noisy libraries
+    logging.getLogger("httpx").setLevel(logging.WARNING)
+    logging.getLogger("httpcore").setLevel(logging.WARNING)
+
+
+def get_controller():
+    from shogun.core.controller import Controller
+    base_dir = str(Path(__file__).resolve().parent)
+    return Controller(base_dir=base_dir)
+
+
+# â”€â”€â”€ Commands â”€â”€â”€
+
+async def cmd_ask(args) -> None:
+    """Execute a task."""
+    ctrl = get_controller()
+    await ctrl.startup()
+
+    prompt = " ".join(args.prompt)
+    mode = args.mode or "battalion"
+    agent = args.agent or ""
+
+    try:
+        result = await ctrl.process_task(
+            prompt=prompt,
+            mode=mode,
+            force_agent=agent,
+        )
+        print(result)
+    except Exception as e:
+        print(f"ã‚¨ãƒ©ãƒ¼: {e}", file=sys.stderr)
+        sys.exit(1)
+    finally:
+        await ctrl.shutdown()
+
+
+async def cmd_pipe(args) -> None:
+    """Read from stdin and process."""
+    ctrl = get_controller()
+    await ctrl.startup()
+
+    stdin_text = sys.stdin.read().strip()
+    if not stdin_text:
+        print("stdin is empty", file=sys.stderr)
+        sys.exit(1)
+
+    prefix = args.prefix or "ä»¥ä¸‹ã®å…¥åŠ›ã‚’åˆ†æã—ã¦ãã ã•ã„"
+    prompt = f"{prefix}:\n\n```\n{stdin_text}\n```"
+    mode = args.mode or "company"  # Default: company for pipe
+
+    try:
+        result = await ctrl.process_task(prompt=prompt, mode=mode)
+        print(result)
+    except Exception as e:
+        print(f"ã‚¨ãƒ©ãƒ¼: {e}", file=sys.stderr)
+        sys.exit(1)
+    finally:
+        await ctrl.shutdown()
+
+
+async def cmd_repl(args) -> None:
+    """Interactive REPL mode."""
+    ctrl = get_controller()
+    await ctrl.startup()
+
+    current_mode = args.mode or "battalion"
+    current_agent = ""
+
+    print("=" * 60)
+    print("  å°†è»ã‚·ã‚¹ãƒ†ãƒ  v7.0 - å¯¾è©±ãƒ¢ãƒ¼ãƒ‰ã€Œã‚¹ãƒ”ãƒ¼ãƒ‰ã‚ˆã‚Šè³ªã€")
+    print("=" * 60)
+    print(f"  ç·¨æˆ: {'å¤§éšŠ' if current_mode == 'battalion' else 'ä¸­éšŠ'}")
+    print()
+    print("  ã‚³ãƒãƒ³ãƒ‰:")
+    print("    /mode [battalion|company]  ãƒ¢ãƒ¼ãƒ‰åˆ‡æ›¿")
+    print("    /agent [taisho|karo|shogun]  ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆæŒ‡å®š")
+    print("    /agent                    ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆæŒ‡å®šè§£é™¤")
+    print("    /status                   ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹è¡¨ç¤º")
+    print("    /stats                    ã‚³ã‚¹ãƒˆçµ±è¨ˆ")
+    print("    /health                   ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯")
+    print("    quit / exit               çµ‚äº†")
+    print("=" * 60)
+    print()
+
+    try:
+        while True:
+            try:
+                mode_label = "å¤§éšŠ" if current_mode == "battalion" else "ä¸­éšŠ"
+                agent_label = f"â†’{current_agent}" if current_agent else ""
+                prompt_str = f"[{mode_label}{agent_label}] > "
+                line = input(prompt_str).strip()
+            except EOFError:
+                break
+
+            if not line:
+                continue
+
+            if line.lower() in ("quit", "exit", "/quit", "/exit"):
+                print("é€€é™£ã€‚")
+                break
+
+            # REPL commands
+            if line.startswith("/"):
+                parts = line.split(maxsplit=1)
+                cmd = parts[0].lower()
+                arg = parts[1] if len(parts) > 1 else ""
+
+                if cmd == "/mode":
+                    if arg in ("battalion", "company", "å¤§éšŠ", "ä¸­éšŠ"):
+                        if arg == "å¤§éšŠ":
+                            arg = "battalion"
+                        elif arg == "ä¸­éšŠ":
+                            arg = "company"
+                        current_mode = arg
+                        label = "å¤§éšŠ" if arg == "battalion" else "ä¸­éšŠ"
+                        print(f"  â†’ ç·¨æˆå¤‰æ›´: {label}")
+                    else:
+                        label = "å¤§éšŠ" if current_mode == "battalion" else "ä¸­éšŠ"
+                        print(f"  ç¾åœ¨: {label}")
+                        print("  ä½¿ç”¨: /mode battalion|company")
+
+                elif cmd == "/agent":
+                    if arg in ("taisho", "karo", "shogun", "ä¾å¤§å°†", "å®¶è€", "å°†è»"):
+                        name_map = {"ä¾å¤§å°†": "taisho", "å®¶è€": "karo", "å°†è»": "shogun"}
+                        current_agent = name_map.get(arg, arg)
+                        print(f"  â†’ ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå›ºå®š: {current_agent}")
+                    elif not arg:
+                        current_agent = ""
+                        print("  â†’ ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå›ºå®šè§£é™¤ (è‡ªå‹•é¸æŠ)")
+                    else:
+                        print("  ä½¿ç”¨: /agent [taisho|karo|shogun]")
+
+                elif cmd == "/status":
+                    st = ctrl.get_status()
+                    mode_label = "å¤§éšŠ" if st["mode"] == "battalion" else "ä¸­éšŠ"
+                    print(f"  ãƒ¢ãƒ¼ãƒ‰: {mode_label}")
+                    print(f"  å¾…æ©Ÿã‚¿ã‚¹ã‚¯: {st['pending_tasks']}")
+                    print(f"  ç·ã‚¿ã‚¹ã‚¯æ•°: {st['total_tasks']}")
+                    ds = st["dashboard"]
+                    print(f"  æœ¬æ—¥å®Œäº†: {ds['completed_today']}")
+                    print(f"  æœ¬æ—¥ã‚³ã‚¹ãƒˆ: Â¥{ds['total_cost_yen']:,}")
+
+                elif cmd == "/stats":
+                    print(ctrl.show_stats())
+
+                elif cmd == "/health":
+                    r1_ok = await ctrl.openvino.health()
+                    cli_ok = await ctrl.claude_cli.check_available()
+                    print(f"  ä¾å¤§å°† R1 (OpenVINO): {'âœ“' if r1_ok else 'âœ—'}")
+                    print(f"  Claude CLI (Proç‰ˆ):    {'âœ“' if cli_ok else 'âœ—'}")
+                    api_ok = ctrl.api_provider is not None
+                    print(f"  Anthropic API:        {'âœ“' if api_ok else 'âœ— (KEYæœªè¨­å®š)'}")
+
+                elif cmd == "/help":
+                    print("  /mode [battalion|company]  ãƒ¢ãƒ¼ãƒ‰åˆ‡æ›¿")
+                    print("  /agent [name]              ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆæŒ‡å®š")
+                    print("  /status                    ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹")
+                    print("  /stats                     ã‚³ã‚¹ãƒˆçµ±è¨ˆ")
+                    print("  /health                    ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯")
+                    print("  quit                       çµ‚äº†")
+
+                else:
+                    print(f"  ä¸æ˜ãªã‚³ãƒãƒ³ãƒ‰: {cmd}")
+                continue
+
+            # Normal prompt
+            try:
+                result = await ctrl.process_task(
+                    prompt=line,
+                    mode=current_mode,
+                    force_agent=current_agent,
+                )
+                print()
+                print(result)
+                print()
+            except Exception as e:
+                print(f"  ã‚¨ãƒ©ãƒ¼: {e}", file=sys.stderr)
+
+    finally:
+        await ctrl.shutdown()
+
+
+async def cmd_status(args) -> None:
+    ctrl = get_controller()
+    st = ctrl.get_status()
+    mode_label = "å¤§éšŠ" if st["mode"] == "battalion" else "ä¸­éšŠ"
+    print(f"ãƒ¢ãƒ¼ãƒ‰: {mode_label}")
+    print(f"å¾…æ©Ÿã‚¿ã‚¹ã‚¯: {st['pending_tasks']}")
+    print(f"ç·ã‚¿ã‚¹ã‚¯æ•°: {st['total_tasks']}")
+
+
+async def cmd_health(args) -> None:
+    ctrl = get_controller()
+    r1_ok = await ctrl.openvino.health()
+    cli_ok = await ctrl.claude_cli.check_available()
+    api_ok = os.environ.get("ANTHROPIC_API_KEY", "") != ""
+
+    print("ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯:")
+    print(f"  ä¾å¤§å°† R1 (OpenVINO): {'âœ“ ç¨¼åƒä¸­' if r1_ok else 'âœ— åœæ­¢'}")
+    print(f"  Claude CLI (Proç‰ˆ):   {'âœ“ åˆ©ç”¨å¯èƒ½' if cli_ok else 'âœ— æœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«'}")
+    print(f"  Anthropic API:       {'âœ“ KEYè¨­å®šæ¸ˆ' if api_ok else 'âœ— KEYæœªè¨­å®š'}")
+    await ctrl.openvino.close()
+
+
+async def cmd_stats(args) -> None:
+    ctrl = get_controller()
+    print(ctrl.show_stats())
+
+
+async def cmd_mode(args) -> None:
+    ctrl = get_controller()
+    if args.target:
+        from shogun.core.task_queue import DeploymentMode
+        target = args.target
+        if target in ("å¤§éšŠ", "battalion"):
+            ctrl.current_mode = DeploymentMode.BATTALION
+            print("â†’ å¤§éšŠãƒ¢ãƒ¼ãƒ‰ã«åˆ‡æ›¿")
+        elif target in ("ä¸­éšŠ", "company"):
+            ctrl.current_mode = DeploymentMode.COMPANY
+            print("â†’ ä¸­éšŠãƒ¢ãƒ¼ãƒ‰ã«åˆ‡æ›¿")
+        else:
+            print(f"ä¸æ˜ãªãƒ¢ãƒ¼ãƒ‰: {target}")
+    else:
+        label = "å¤§éšŠ" if ctrl.current_mode.value == "battalion" else "ä¸­éšŠ"
+        print(f"ç¾åœ¨ã®ãƒ¢ãƒ¼ãƒ‰: {label}")
+
+
+def cmd_server(args) -> None:
+    """Start FastAPI server."""
+    try:
+        import uvicorn
+        from shogun.main import create_app
+        app = create_app()
+        uvicorn.run(
+            app,
+            host=args.host or "0.0.0.0",
+            port=args.port or 8080,
+        )
+    except ImportError as e:
+        print(f"ã‚µãƒ¼ãƒãƒ¼èµ·å‹•ã«å¿…è¦ãªãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ãŒã‚ã‚Šã¾ã›ã‚“: {e}", file=sys.stderr)
+        sys.exit(1)
+
+
+def cmd_maintenance(args) -> None:
+    """Monthly maintenance (åçœä¼š) commands."""
+    from shogun.core.maintenance import MaintenanceManager
+
+    manager = MaintenanceManager(base_dir=Path(__file__).resolve().parent)
+    action = getattr(args, "action", "status")
+
+    if action == "run":
+        print("=" * 60)
+        print("  å°†è»ã‚·ã‚¹ãƒ†ãƒ  - æœˆæ¬¡ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹ (åçœä¼š)")
+        print("=" * 60)
+        print()
+        print("ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹å®Ÿè¡Œä¸­...")
+        print()
+
+        report = manager.run_full_maintenance()
+
+        # Summary
+        summary = report["summary"]
+        print(f"ç·ãƒã‚§ãƒƒã‚¯æ•°: {summary['total']}")
+        print(f"  âœ“ æ­£å¸¸: {summary['passed']}")
+        print(f"  âš  è­¦å‘Š: {summary['warnings']}")
+        print(f"  âœ— ã‚¨ãƒ©ãƒ¼: {summary['errors']}")
+        print()
+
+        # Details
+        for check_id, result in report["checks"].items():
+            status = result.get("status", "unknown")
+            emoji = {"ok": "âœ“", "warning": "âš ", "error": "âœ—"}.get(status, "?")
+            name = result.get("name", check_id)
+            msg = result.get("message", "")
+            print(f"  {emoji} {name}: {msg}")
+
+        print()
+        print(f"ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜: reports/maintenance/maintenance_{report['timestamp']}.md")
+        print()
+
+    elif action == "reports":
+        reports = manager.list_reports(limit=args.limit or 10)
+        if not reports:
+            print("éå»ã®ãƒ¬ãƒãƒ¼ãƒˆãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
+            return
+
+        print("éå»ã®ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹ãƒ¬ãƒãƒ¼ãƒˆ:")
+        print()
+        for r in reports:
+            s = r.get("summary", {})
+            status = "âœ“" if s.get("errors", 0) == 0 else "âœ—"
+            print(f"  {status} {r['date'][:10]}  passed:{s.get('passed',0)}  warnings:{s.get('warnings',0)}  errors:{s.get('errors',0)}")
+            print(f"      â†’ {r['file']}")
+
+    elif action == "next":
+        next_date = manager.get_next_maintenance_date()
+        print(f"æ¬¡å›ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹: {next_date.strftime('%Y-%m-%d %H:%M')}")
+
+    elif action == "check":
+        # Run single check
+        check_name = args.check_name
+        check_methods = {
+            "llm": manager.check_llm_versions,
+            "llm_versions": manager.check_llm_versions,
+            "openvino": manager.check_openvino_model,
+            "mcp": manager.check_mcp_servers,
+            "health": manager.check_system_health,
+            "logs": manager.cleanup_logs,
+            "cost": manager.generate_cost_report,
+        }
+        if check_name in check_methods:
+            result = check_methods[check_name]()
+            print(f"{result.get('name', check_name)}:")
+            print(f"  ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹: {result.get('status', 'unknown')}")
+            print(f"  çµæœ: {result.get('message', '')}")
+            if result.get("updates_available"):
+                print("  æ›´æ–°å¯èƒ½:")
+                for u in result["updates_available"]:
+                    if isinstance(u, dict):
+                        print(f"    - {u.get('package')}: {u.get('current')} â†’ {u.get('latest')}")
+                    else:
+                        print(f"    - {u}")
+        else:
+            print(f"ä¸æ˜ãªãƒã‚§ãƒƒã‚¯é …ç›®: {check_name}")
+            print("åˆ©ç”¨å¯èƒ½: llm, openvino, mcp, health, logs, cost")
+
+    else:
+        # status (default)
+        next_date = manager.get_next_maintenance_date()
+        reports = manager.list_reports(limit=1)
+
+        print("æœˆæ¬¡ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹ (åçœä¼š):")
+        print(f"  æ¬¡å›äºˆå®š: {next_date.strftime('%Y-%m-%d %H:%M')}")
+
+        if reports:
+            last = reports[0]
+            s = last.get("summary", {})
+            status = "æ­£å¸¸" if s.get("errors", 0) == 0 else "è¦ç¢ºèª"
+            print(f"  å‰å›å®Ÿè¡Œ: {last['date'][:10]} ({status})")
+        else:
+            print("  å‰å›å®Ÿè¡Œ: ãªã—")
+
+        print()
+        print("ã‚³ãƒãƒ³ãƒ‰:")
+        print("  shogun maintenance run       # ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹å®Ÿè¡Œ")
+        print("  shogun maintenance reports   # éå»ãƒ¬ãƒãƒ¼ãƒˆä¸€è¦§")
+        print("  shogun maintenance check mcp # å€‹åˆ¥ãƒã‚§ãƒƒã‚¯å®Ÿè¡Œ")
+
+
+# â”€â”€â”€ Main â”€â”€â”€
+
+def main():
+    parser = argparse.ArgumentParser(
+        prog="shogun",
+        description="å°†è»ã‚·ã‚¹ãƒ†ãƒ  v7.0 - éšå±¤å‹ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰AIé–‹ç™ºã‚·ã‚¹ãƒ†ãƒ ã€Œã‚¹ãƒ”ãƒ¼ãƒ‰ã‚ˆã‚Šè³ªã€",
+    )
+    parser.add_argument("-v", "--verbose", action="store_true", help="è©³ç´°ãƒ­ã‚°")
+    sub = parser.add_subparsers(dest="command")
+
+    # ask
+    p_ask = sub.add_parser("ask", help="ã‚¿ã‚¹ã‚¯å®Ÿè¡Œ")
+    p_ask.add_argument("prompt", nargs="+", help="ã‚¿ã‚¹ã‚¯å†…å®¹")
+    p_ask.add_argument("-m", "--mode", default="battalion",
+                       help="battalion (å¤§éšŠ) / company (ä¸­éšŠ)")
+    p_ask.add_argument("-a", "--agent", default="",
+                       help="ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆæŒ‡å®š: taisho/karo/shogun")
+
+    # pipe
+    p_pipe = sub.add_parser("pipe", help="stdin ãƒ‘ã‚¤ãƒ—å…¥åŠ›")
+    p_pipe.add_argument("-m", "--mode", default="company")
+    p_pipe.add_argument("-p", "--prefix", default="ä»¥ä¸‹ã®å…¥åŠ›ã‚’åˆ†æã—ã¦ãã ã•ã„")
+
+    # repl
+    p_repl = sub.add_parser("repl", help="å¯¾è©±ãƒ¢ãƒ¼ãƒ‰")
+    p_repl.add_argument("-m", "--mode", default="battalion")
+
+    # status
+    sub.add_parser("status", help="ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹")
+
+    # health
+    sub.add_parser("health", help="ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯")
+
+    # stats
+    sub.add_parser("stats", help="ã‚³ã‚¹ãƒˆçµ±è¨ˆ")
+
+    # mode
+    p_mode = sub.add_parser("mode", help="ãƒ¢ãƒ¼ãƒ‰è¡¨ç¤º/åˆ‡æ›¿")
+    p_mode.add_argument("target", nargs="?", default="",
+                        help="battalion/company")
+
+    # server
+    p_server = sub.add_parser("server", help="REST APIã‚µãƒ¼ãƒãƒ¼èµ·å‹•")
+    p_server.add_argument("--host", default="0.0.0.0")
+    p_server.add_argument("--port", type=int, default=8080)
+
+    # maintenance (åçœä¼š)
+    p_maint = sub.add_parser("maintenance", help="æœˆæ¬¡ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹ (åçœä¼š)")
+    p_maint_sub = p_maint.add_subparsers(dest="action")
+    p_maint_sub.add_parser("run", help="ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹å®Ÿè¡Œ")
+    p_maint_reports = p_maint_sub.add_parser("reports", help="éå»ãƒ¬ãƒãƒ¼ãƒˆä¸€è¦§")
+    p_maint_reports.add_argument("-n", "--limit", type=int, default=10, help="è¡¨ç¤ºä»¶æ•°")
+    p_maint_sub.add_parser("next", help="æ¬¡å›ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹æ—¥")
+    p_maint_check = p_maint_sub.add_parser("check", help="å€‹åˆ¥ãƒã‚§ãƒƒã‚¯å®Ÿè¡Œ")
+    p_maint_check.add_argument("check_name", help="ãƒã‚§ãƒƒã‚¯é …ç›®: llm, openvino, mcp, health, logs, cost")
+
+    args = parser.parse_args()
+    setup_logging(args.verbose)
+
+    if args.command is None:
+        # Default: REPL
+        args.command = "repl"
+        args.mode = "battalion"
+
+    if args.command == "server":
+        cmd_server(args)
+        return
+
+    if args.command == "maintenance":
+        cmd_maintenance(args)
+        return
+
+    # Async commands
+    cmd_map = {
+        "ask": cmd_ask,
+        "pipe": cmd_pipe,
+        "repl": cmd_repl,
+        "status": cmd_status,
+        "health": cmd_health,
+        "stats": cmd_stats,
+        "mode": cmd_mode,
+    }
+
+    fn = cmd_map.get(args.command)
+    if fn:
+        asyncio.run(fn(args))
+    else:
+        parser.print_help()
+
+
+if __name__ == "__main__":
+    main()
diff --git a/config/mcp_config.json b/config/mcp_config.json
new file mode 100644
index 0000000..eeccd6a
--- /dev/null
+++ b/config/mcp_config.json
@@ -0,0 +1,49 @@
+{
+  "mcpServers": {
+    "filesystem": {
+      "command": "npx",
+      "args": ["-y", "@modelcontextprotocol/server-filesystem", "/home/claude"]
+    },
+    "github": {
+      "command": "npx",
+      "args": ["-y", "@modelcontextprotocol/server-github"],
+      "env": {
+        "GITHUB_TOKEN": "${GITHUB_TOKEN}"
+      }
+    },
+    "fetch": {
+      "command": "npx",
+      "args": ["-y", "@modelcontextprotocol/server-fetch"]
+    },
+    "memory": {
+      "command": "npx",
+      "args": ["-y", "@modelcontextprotocol/server-memory"]
+    },
+    "postgres": {
+      "command": "npx",
+      "args": ["-y", "@modelcontextprotocol/server-postgres"],
+      "env": {
+        "DATABASE_URL": "${DATABASE_URL}"
+      }
+    },
+    "puppeteer": {
+      "command": "npx",
+      "args": ["-y", "@modelcontextprotocol/server-puppeteer"]
+    },
+    "brave-search": {
+      "command": "npx",
+      "args": ["-y", "@modelcontextprotocol/server-brave-search"],
+      "env": {
+        "BRAVE_API_KEY": "${BRAVE_API_KEY}"
+      }
+    },
+    "slack": {
+      "command": "npx",
+      "args": ["-y", "@modelcontextprotocol/server-slack"],
+      "env": {
+        "SLACK_BOT_TOKEN": "${SLACK_BOT_TOKEN}",
+        "SLACK_TEAM_ID": "${SLACK_TEAM_ID}"
+      }
+    }
+  }
+}
diff --git a/config/settings.yaml b/config/settings.yaml
new file mode 100644
index 0000000..cba8c33
--- /dev/null
+++ b/config/settings.yaml
@@ -0,0 +1,708 @@
+# =============================================================
+# å°†è»ã‚·ã‚¹ãƒ†ãƒ  v8.0.1 - Configuration
+# =============================================================
+# ã€Œå®Œå…¨è‡ªå¾‹å‹AIé–‹ç™ºç’°å¢ƒã€- å®Ÿè¨¼ä¸»ç¾©ãƒ»äºŒé‡è¨˜æ†¶ãƒ»è‡ªå¾‹å­¦ç¿’
+# =============================================================
+
+system:
+  name: "å°†è»ã‚·ã‚¹ãƒ†ãƒ "
+  version: "8.0.1"
+  language: "ja"
+  philosophy: "ã‚¹ãƒ”ãƒ¼ãƒ‰ã‚ˆã‚Šè³ª + å®Ÿè¨¼ä¸»ç¾© + çŸ¥è­˜è“„ç©æ´»ç”¨"
+  subtitle: "å®Œå…¨è‡ªå¾‹å‹AIé–‹ç™ºç’°å¢ƒ"
+
+# --- v8.0 æ–°æ©Ÿèƒ½ ---
+v8_features:
+  knowledge_base:
+    enabled: true
+    description: "å¤–éƒ¨çŸ¥è­˜ï¼ˆWebæ¤œç´¢ã€ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆï¼‰ã®RAGä¿å­˜ãƒ»æ¤œç´¢"
+    technology: "Qdrant + Sentence Transformers"
+    users: ["å°†è»", "å®¶è€", "ä¾å¤§å°†"]
+    retention_days: 30
+  
+  activity_memory:
+    enabled: true
+    description: "ä¾å¤§å°†ï¼ˆR1ï¼‰è‡ªèº«ã®åˆ¤æ–­å±¥æ­´ãƒ»å‚™å¿˜éŒ²"
+    technology: "SQLite"
+    user: "ä¾å¤§å°†å°‚ç”¨"
+    retention_days: 90
+    benefits:
+      - "é¡ä¼¼ã‚¿ã‚¹ã‚¯å‡¦ç†æ™‚é–“ -57%"
+      - "æœˆé–“å‡¦ç†æ™‚é–“ -8.7%"
+      - "ä¸€è²«æ€§å¤§å¹…å‘ä¸Š"
+  
+  sandbox_environment:
+    enabled: true
+    description: "ã‚³ãƒ¼ãƒ‰å®Ÿè¡Œãƒ»æ¤œè¨¼ç’°å¢ƒï¼ˆå®Ÿè¨¼ä¸»ç¾©ï¼‰"
+    technology: "LXCï¼ˆã‚ªãƒ³ãƒ‡ãƒãƒ³ãƒ‰èµ·å‹•ï¼‰"
+    effect: "å“è³ª 95â†’99ç‚¹ï¼ˆ+4ç‚¹ï¼‰"
+    languages: ["Python 3.13", "Node.js 22", "Rust 1.83"]
+    security: ["ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯åˆ†é›¢", "30ç§’ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ", "ãƒªã‚½ãƒ¼ã‚¹åˆ¶é™"]
+  
+  web_search_integration:
+    enabled: true
+    description: "æœ€æ–°æƒ…å ±å–å¾—ï¼ˆOllama Web Searchï¼‰"
+    technology: "10ç•ªè¶³è»½ã¨ã—ã¦çµ±åˆ"
+    cost: "Â¥0ï¼ˆç„¡æ–™æ ï¼‰"
+    auto_rag_save: true
+  
+  thought_monitoring:
+    enabled: true
+    description: "R1ã®<think>ä¸­é–“ãƒã‚§ãƒƒã‚¯ï¼ˆè«–ç†çŸ›ç›¾æ¤œå‡ºï¼‰"
+    monitor: "å®¶è€ï¼ˆOpusï¼‰"
+    trigger_modes: ["Complex", "Strategic"]
+
+# --- ãƒªãƒã‚¸ãƒˆãƒªåŒæœŸ ---
+repo:
+  local_base: "/home/claude"
+  github_remote: ""  # git remote ã‹ã‚‰è‡ªå‹•å–å¾—
+  sync_interval_min: 5
+
+# --- å°†è» (ç·å¤§å°†) ---
+# LLM: Claude Sonnet 4.5 (Pro CLIå„ªå…ˆ â†’ APIè£œå®Œ)
+shogun:
+  codename: "Shogun"
+  role: "å°†è» - é›£æ˜“åº¦åˆ¤æ–­ãƒ»å…¨ä½“çµ±æ‹¬ãƒ»Complexè¨­è¨ˆãƒ»æœ€çµ‚å ±å‘Š"
+  cli_model: "sonnet"
+  api_model: "claude-sonnet-4-5-20250514"
+  cost_per_task_yen: 5
+  monthly_usage: 100
+  new_features:
+    - "çŸ¥è­˜åŸºç›¤å‚ç…§ï¼ˆRAGï¼‰"
+    - "æ€è€ƒç›£è¦–å—ã‘å…¥ã‚Œ"
+
+# --- å®¶è€ (å‚è¬€) ---
+# LLM: Claude Opus 4.5 (APIå°‚ç”¨ãƒ»Strategicæ™‚ã®ã¿)
+karo:
+  codename: "Karo"
+  role: "å®¶è€ - æˆ¦è¡“ç«‹æ¡ˆãƒ»ä½œæ¥­å‰²æŒ¯ã‚Šãƒ»å°†è»ã¸ã®åŠ©è¨€ãƒ»æ€è€ƒç›£è¦–"
+  api_model: "claude-opus-4-5-20250514"  # v8.0: æœ€æ–°ç‰ˆ Opus 4.5
+  cost_per_task_yen: 24
+  monthly_usage: 5
+  new_features:
+    - "çŸ¥è­˜åŸºç›¤å‚ç…§ï¼ˆRAGï¼‰"
+    - "R1æ€è€ƒã®ä¸­é–“ãƒã‚§ãƒƒã‚¯"
+    - "è«–ç†çŸ›ç›¾æ¤œå‡ºãƒ»å†è©¦è¡Œåˆ¶å¾¡"
+
+# --- ä¾å¤§å°† (ç¾å ´æŒ‡æ®ãƒ»ç›£æŸ»å½¹) ---
+# LLM: DeepSeek-R1-Distill-Qwen-14B-Japanese (CyberAgentç‰ˆ)
+taisho:
+  url: "http://192.168.1.11:11434"
+  model: "cyberagent/DeepSeek-R1-Distill-Qwen-14B-Japanese"
+  model_url: "https://huggingface.co/cyberagent/DeepSeek-R1-Distill-Qwen-14B-Japanese"
+  quantization: "INT8 (OpenVINO)"
+  kv_cache_precision: "u8"
+  performance: "7.2 tok/s"
+  memory_gb: 20
+  role: "ä¾å¤§å°† - è¶³è»½çµ±ç‡ãƒ»Simple/Mediumè¨­è¨ˆãƒ»å“è³ªç›£æŸ»"
+  features:
+    - "æ—¥æœ¬èªãƒšãƒ©ãƒšãƒ©"
+    - "<think>ã§æ·±ã„åˆ†æ"
+    - "ã˜ã£ãã‚Šè€ƒãˆã‚‹ï¼ˆ60ç§’ï¼‰= é«˜å“è³ªã®æºæ³‰"
+  new_features:
+    - "çŸ¥è­˜åŸºç›¤å‚ç…§ï¼ˆRAGï¼‰"
+    - "é™£ä¸­æ—¥è¨˜å‚ç…§ãƒ»è¨˜éŒ²"
+    - "æ¼”ç¿’å ´é€£æºï¼ˆå®Ÿè¡Œæ¤œè¨¼ï¼‰"
+    - "å‹•çš„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´"
+  dynamic_parameters:
+    default_temperature: 0.7
+    audit_temperature: 0.1  # ç›£æŸ»æ™‚ã¯å³å¯†ã«
+    creative_temperature: 0.9  # å‰µé€ çš„ã‚¿ã‚¹ã‚¯æ™‚
+
+# --- è¶³è»½ Ã— 10 (å®Ÿè¡Œéƒ¨éšŠ) ---
+ashigaru:
+  count: 10  # v8.0: 10ç•ªè¶³è»½è¿½åŠ 
+  max_active: 4
+  memory_each_mb: "50-150"
+  host: "192.168.1.10"
+  selection_algorithm: "dynamic"
+  servers:
+    - id: 1
+      name: "filesystem"
+      mcp: "@modelcontextprotocol/server-filesystem"
+      role: "ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿æ›¸ããƒ»æ¤œç´¢"
+    - id: 2
+      name: "github"
+      mcp: "@modelcontextprotocol/server-github"
+      role: "ãƒªãƒã‚¸ãƒˆãƒªæ“ä½œãƒ»Issueç®¡ç†"
+    - id: 3
+      name: "fetch"
+      mcp: "@modelcontextprotocol/server-fetch"
+      role: "Webãƒšãƒ¼ã‚¸å–å¾—"
+    - id: 4
+      name: "memory"
+      mcp: "@modelcontextprotocol/server-memory"
+      role: "çŸ¥è­˜ä¿å­˜ãƒ»æ¤œç´¢"
+    - id: 5
+      name: "postgres"
+      mcp: "@modelcontextprotocol/server-postgres"
+      role: "ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ“ä½œ"
+    - id: 6
+      name: "puppeteer"
+      mcp: "@modelcontextprotocol/server-puppeteer"
+      role: "ãƒ–ãƒ©ã‚¦ã‚¶è‡ªå‹•åŒ–"
+    - id: 7
+      name: "brave-search"
+      mcp: "@modelcontextprotocol/server-brave-search"
+      role: "æœ€æ–°æƒ…å ±æ¤œç´¢"
+    - id: 8
+      name: "slack"
+      mcp: "@modelcontextprotocol/server-slack"
+      role: "Slackæ“ä½œ"
+    - id: 9
+      name: "groq-recorder"
+      type: "llm"
+      model: "llama-3.3-70b-versatile"
+      role: "è¨˜éŒ²ä¿‚ - ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ è¨˜éŒ²ãƒ»60æ—¥è¦ç´„ãƒ»Notionè»¢é€ãƒ»å®¶è¨“æŠ½å‡º"
+    - id: 10  # v8.0 æ–°è¦è¿½åŠ 
+      name: "ollama-web-search"
+      type: "web_search"
+      api_url: "https://api.ollama.com/web-search"
+      role: "æœ€æ–°æƒ…å ±å°‚é–€ - Webæ¤œç´¢ãƒ»RAGè‡ªå‹•ä¿å­˜"
+      cost: "Â¥0ï¼ˆç„¡æ–™æ ï¼‰"
+      features:
+        - "R1ã®çŸ¥è­˜ã‚®ãƒ£ãƒƒãƒ—è£œå®Œ"
+        - "Claudeï¼ˆå°†è»ãƒ»å®¶è€ï¼‰ã¸ã®æœ€æ–°æƒ…å ±æä¾›"
+        - "æ¤œç´¢çµæœã®è‡ªå‹•RAGä¿å­˜"
+
+# --- v8.0.1 ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–æ§‹æˆ ---
+memory_optimization:
+  ct_consolidation:
+    enabled: true
+    description: "CT 104ã‚’å»ƒæ­¢ã€CT 100ã«çŸ¥è­˜åŸºç›¤çµ±åˆ"
+    ct100_memory_mb: 2560  # 2.5GBï¼ˆçŸ¥è­˜åŸºç›¤çµ±åˆå¾Œï¼‰
+    ct100_components:
+      - "ã‚·ã‚¹ãƒ†ãƒ : 0.5GB"
+      - "Python/Node: 0.5GB"
+      - "Qdrantï¼ˆDockerï¼‰: 1GB"
+      - "SQLite: 0.1GB"
+      - "MCP Ã— 10: 0.4GB"
+    memory_saving_gb: 3  # CT 104å»ƒæ­¢ã«ã‚ˆã‚‹å‰Šæ¸›
+  
+  on_demand_sandbox:
+    enabled: true
+    description: "CT 102ï¼ˆæ¼”ç¿’å ´ï¼‰ã‚’ã‚ªãƒ³ãƒ‡ãƒãƒ³ãƒ‰èµ·å‹•"
+    standby_memory_saving_gb: 2
+    startup_command: "pct start 102"
+    shutdown_command: "pct stop 102"
+    auto_management: true
+  
+  zram_safety:
+    enabled: true
+    description: "ãƒ”ãƒ¼ã‚¯æ™‚ãƒãƒƒãƒ•ã‚¡ç”¨ZRAM"
+    size_gb: 2
+    compression: "lz4"
+    priority: "ä½ï¼ˆæœ€å¾Œã®æ‰‹æ®µï¼‰"
+    effective_capacity_gb: "3-4ï¼ˆåœ§ç¸®æ™‚ï¼‰"
+
+# --- Groq (9ç•ªè¶³è»½) ---
+groq:
+  api_url: "https://api.groq.com/openai/v1"
+  model: "llama-3.3-70b-versatile"
+  cost_per_task_yen: 0
+  free_tier_requests_per_day: 14400
+  speed: "300-500 tok/s"
+  # v8.0: ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾å¿œå¼·åŒ–
+  rate_limiting:
+    rpm_limit: 30  # ãƒªã‚¯ã‚¨ã‚¹ãƒˆ/åˆ†
+    tpm_limit: 14000  # ãƒˆãƒ¼ã‚¯ãƒ³/åˆ†
+    daily_limit: 14400
+    exponential_backoff:
+      enabled: true
+      initial_delay_ms: 1000
+      max_delay_ms: 60000
+      jitter: true
+      max_retries: 5
+
+# --- Ollama Web Search (10ç•ªè¶³è»½) ---
+ollama_web_search:
+  enabled: true
+  api_key: ""  # ç’°å¢ƒå¤‰æ•°ã‹ã‚‰å–å¾—
+  base_url: "https://api.ollama.com"
+  free_tier:
+    requests_per_month: 1000
+    cost: "Â¥0"
+  features:
+    search_languages: ["ja", "en"]
+    result_limit: 5
+    timeout_seconds: 30
+    auto_rag_integration: true
+
+# --- çŸ¥è­˜åŸºç›¤ï¼ˆRAGï¼‰ ---
+knowledge_base:
+  enabled: true
+  technology: "Qdrant"
+  host: "192.168.1.10"  # CT 100ã«çµ±åˆ
+  port: 6333
+  collection_name: "shogun_knowledge"
+  embedding_model: "sentence-transformers/all-mpnet-base-v2"
+  embedding_dimension: 768
+  retention_days: 30
+  data_sources:
+    - "Ollama Web Searchçµæœ"
+    - "æ‰‹å‹•ç™»éŒ²ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ"
+    - "GitHub Issue/PR"
+  search_config:
+    score_threshold: 0.7
+    max_results: 5
+    multilingual: true
+  auto_cleanup:
+    enabled: true
+    schedule: "æ¯æ—¥3:00"
+
+# --- é™£ä¸­æ—¥è¨˜ï¼ˆActivity Memoryï¼‰ ---
+activity_memory:
+  enabled: true
+  technology: "SQLite"
+  database_path: "/opt/shogun/taisho_activity.db"
+  host: "192.168.1.10"  # CT 100ã«çµ±åˆ
+  retention_days: 90
+  schema:
+    tables:
+      - "activities"  # ã‚¿ã‚¹ã‚¯å±¥æ­´
+      - "decisions"   # åˆ¤æ–­å±¥æ­´
+      - "reasoning"   # æ¨è«–éç¨‹
+  data_recorded:
+    - "ã‚¿ã‚¹ã‚¯è¦ç´„"
+    - "<think>è¦ç´„"
+    - "æœ€çµ‚åˆ¤æ–­"
+    - "åˆ¤æ–­ç†ç”±"
+    - "ç¢ºä¿¡åº¦"
+    - "å‡¦ç†æ™‚é–“"
+    - "ä½¿ç”¨ã—ãŸãƒ„ãƒ¼ãƒ«"
+  search_methods:
+    - "ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒãƒ³ã‚°"
+    - "é¡ä¼¼ã‚¿ã‚¹ã‚¯æ¤œç´¢"
+    - "æ™‚ç³»åˆ—ã‚½ãƒ¼ãƒˆ"
+  benefits:
+    similar_task_speedup: "-57%"
+    monthly_time_reduction: "-8.7%"
+    consistency_improvement: "å¤§å¹…å‘ä¸Š"
+  auto_cleanup:
+    enabled: true
+    schedule: "æ¯æ—¥3:00"
+
+# --- æ¼”ç¿’å ´ï¼ˆSandbox Environmentï¼‰ ---
+sandbox:
+  enabled: true
+  technology: "LXC (CT 102)"
+  host: "192.168.1.12"
+  memory_mb: 2048
+  cores: 2
+  startup_mode: "on_demand"  # v8.0.1: ã‚ªãƒ³ãƒ‡ãƒãƒ³ãƒ‰èµ·å‹•
+  languages:
+    python:
+      version: "3.13"
+      packages: ["numpy", "pandas", "requests", "fastapi"]
+    nodejs:
+      version: "22"
+      packages: ["typescript", "express", "@types/node"]
+    rust:
+      version: "1.83"
+      features: ["cargo", "rustfmt", "clippy"]
+  security:
+    network_isolation: true
+    internet_access: false
+    resource_limits:
+      cpu_percent: 80
+      memory_mb: 1800
+      disk_mb: 1000
+    execution_timeout_seconds: 30
+  api:
+    port: 8080
+    endpoints:
+      - "/execute/python"
+      - "/execute/nodejs"
+      - "/execute/rust"
+      - "/health"
+  workflow:
+    description: "R1è¨­è¨ˆ â†’ æ¼”ç¿’å ´å®Ÿè¡Œ â†’ æˆåŠŸ/å¤±æ•— â†’ ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ â†’ ä¿®æ­£ â†’ å†å®Ÿè¡Œ"
+    quality_improvement: "+4ç‚¹ï¼ˆ95â†’99ç‚¹ï¼‰"
+    philosophy: "ç†è«–ã‹ã‚‰å®Ÿè¨¼ã¸"
+
+# --- Notionçµ±åˆ ---
+notion:
+  enabled: true
+  database_id: ""
+  auto_summary_days: 60
+  uses:
+    - "å®¶è¨“ï¼ˆæ±ºå®šäº‹é …ï¼‰ã®æ°¸ç¶šåŒ–"
+    - "ãƒãƒ£ãƒ³ãƒãƒ«è¦ç´„ã®ä¿å­˜"
+    - "ãƒŠãƒ¬ãƒƒã‚¸DBæ§‹ç¯‰"
+  # v8.0: RAGçµ±åˆå¼·åŒ–
+  rag_integration:
+    enabled: true
+    sync_to_knowledge_base: true
+    embedding_updates: "ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ "
+
+# --- 60æ—¥è‡ªå‹•è¦ç´„ ---
+auto_summary:
+  enabled: true
+  retention_days: 60
+  schedule:
+    hour: 3
+    minute: 0
+  groq_model: "llama-3.3-70b-versatile"
+
+# --- Deployment Modes ---
+modes:
+  battalion:
+    label: "å¤§éšŠ"
+    description: "å°†è» + å®¶è€(è«®å•æ™‚) + ä¾å¤§å°† + è¶³è»½ Ã— 10 + æ¼”ç¿’å ´ + RAG + é™£ä¸­æ—¥è¨˜"
+    api_enabled: true
+    trigger: "@shogun-bot"
+    new_features:
+      - "çŸ¥è­˜åŸºç›¤å‚ç…§"
+      - "æ¼”ç¿’å ´æ¤œè¨¼"
+      - "é™£ä¸­æ—¥è¨˜æ´»ç”¨"
+      - "æ€è€ƒç›£è¦–"
+  company:
+    label: "ä¸­éšŠ"
+    description: "ä¾å¤§å°† + è¶³è»½ Ã— 10 + é™£ä¸­æ—¥è¨˜ (APIä¸ä½¿ç”¨)"
+    api_enabled: false
+    cost: 0
+    trigger: "@shogun-bot-light"
+    new_features:
+      - "é™£ä¸­æ—¥è¨˜å‚ç…§ï¼ˆé«˜é€ŸåŒ–ï¼‰"
+      - "æ¼”ç¿’å ´æ¤œè¨¼ï¼ˆå“è³ªä¿è¨¼ï¼‰"
+  platoon:
+    label: "å°éšŠ"
+    description: "ä¾å¤§å°† + å¿…è¦ãªè¶³è»½ã®ã¿ï¼ˆå‹•çš„é¸æŠï¼‰+ é™£ä¸­æ—¥è¨˜"
+    api_enabled: false
+    cost: 0
+    trigger: "HA OSéŸ³å£°"
+    response_time_sec: "30-60"
+
+# --- Platoon Mode Settings ---
+platoon_modes:
+  voice_query:
+    max_ashigaru: 2
+    response_time_target: 30
+  quick_info:
+    max_ashigaru: 1
+    response_time_target: 15
+  file_check:
+    max_ashigaru: 1
+    response_time_target: 10
+
+# --- Slack Bots (12å€‹) ---
+slack:
+  bots:
+    - name: "shogun-bot"
+      role: "å°†è»"
+      mode: "battalion"
+    - name: "karo-bot"
+      role: "å®¶è€"
+    - name: "taisho-bot"
+      role: "ä¾å¤§å°†"
+    - name: "ashigaru-1-bot"
+      role: "1ç•ªè¶³è»½ (filesystem)"
+    - name: "ashigaru-2-bot"
+      role: "2ç•ªè¶³è»½ (github)"
+    - name: "ashigaru-3-bot"
+      role: "3ç•ªè¶³è»½ (fetch)"
+    - name: "ashigaru-4-bot"
+      role: "4ç•ªè¶³è»½ (memory)"
+    - name: "ashigaru-5-bot"
+      role: "5ç•ªè¶³è»½ (postgres)"
+    - name: "ashigaru-6-bot"
+      role: "6ç•ªè¶³è»½ (puppeteer)"
+    - name: "ashigaru-7-bot"
+      role: "7ç•ªè¶³è»½ (brave-search)"
+    - name: "ashigaru-8-bot"
+      role: "8ç•ªè¶³è»½ (slack)"
+    - name: "ashigaru-9-bot"
+      role: "9ç•ªè¶³è»½ (groqè¨˜éŒ²ä¿‚)"
+    - name: "ashigaru-10-bot"  # v8.0 æ–°è¦è¿½åŠ 
+      role: "10ç•ªè¶³è»½ (ollama-web-search)"
+    - name: "shogun-bot-light"
+      role: "ä¸­éšŠãƒ¢ãƒ¼ãƒ‰å°‚ç”¨"
+      mode: "company"
+
+# --- HA OS (éŸ³å£°ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹) ---
+ha_os:
+  url: "http://192.168.1.20:8123"
+  mode: "platoon"
+  whisper: "ãƒ­ãƒ¼ã‚«ãƒ«éŸ³å£°èªè­˜"
+  tts: "Piper"
+
+# --- Proxmox LXC (v8.0.1 æœ€é©åŒ–æ§‹æˆ) ---
+proxmox:
+  optimization_version: "v8.0.1"
+  total_memory_gb: 24
+  memory_allocation:
+    standby_mode:
+      proxmox_host: 1.5
+      ct100: 2.5  # çŸ¥è­˜åŸºç›¤çµ±åˆå¾Œ
+      ct101: 20.0  # R1 + HugePages
+      ct102: 0  # åœæ­¢ä¸­
+      total: 24.0
+      available: 0
+    execution_mode:
+      proxmox_host: 1.5
+      ct100: 2.5
+      ct101: 20.0
+      ct102: 2.0  # æ¼”ç¿’å ´ç¨¼åƒä¸­
+      total: 26.0
+      physical: 24.0
+      zram_buffer: 2.0
+      effective: "26-28GB"
+  
+  containers:
+    ct100:
+      name: "honmaru-control-knowledge"  # v8.0: çŸ¥è­˜åŸºç›¤çµ±åˆ
+      memory_mb: 2560  # v8.0.1: 2.5GB
+      cores: 2
+      ip: "192.168.1.10"
+      role: "æœ¬é™£ + çŸ¥è­˜åŸºç›¤ - ã‚¿ã‚¹ã‚¯ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°, MCPç®¡ç†, Slack Bot Ã— 12, Qdrant, é™£ä¸­æ—¥è¨˜"
+      services:
+        - "MCP Ã— 10ã‚µãƒ¼ãƒãƒ¼"
+        - "Slack Bot Ã— 12"
+        - "Qdrantï¼ˆDockerï¼‰"
+        - "SQLiteï¼ˆé™£ä¸­æ—¥è¨˜ï¼‰"
+        - "claude-cliï¼ˆå°†è»ï¼‰"
+        - "HA OS API"
+        - "Notionçµ±åˆ"
+    ct101:
+      name: "taisho-r1-japanese"
+      memory_mb: 20480
+      cores: 6
+      ip: "192.168.1.11"
+      role: "ä¾å¤§å°†R1 - OpenVINOæ¨è«–ã‚µãƒ¼ãƒãƒ¼ (æ—¥æœ¬èªç‰ˆ)"
+      cpu_type: "host"
+      numa: true
+    ct102:
+      name: "sandbox-executor"  # v8.0: æ¼”ç¿’å ´
+      memory_mb: 2048
+      cores: 2
+      ip: "192.168.1.12"
+      role: "æ¼”ç¿’å ´ - ã‚³ãƒ¼ãƒ‰å®Ÿè¡Œãƒ»æ¤œè¨¼ï¼ˆã‚ªãƒ³ãƒ‡ãƒãƒ³ãƒ‰èµ·å‹•ï¼‰"
+      startup_mode: "on_demand"
+      languages: ["Python 3.13", "Node.js 22", "Rust 1.83"]
+      security_isolation: true
+      internet_access: false
+    # ct104: å»ƒæ­¢ï¼ˆCT 100ã«çµ±åˆï¼‰
+
+  hugepages:
+    total_pages: 10240  # 20GB
+    page_size: "2MB"
+    allocation: "R1æ¨è«–å°‚ç”¨"
+
+  zram:
+    enabled: true
+    size_gb: 2
+    algorithm: "lz4"
+    priority: 100
+    usage_scenario: "æ¼”ç¿’å ´ç¨¼åƒæ™‚ã®ãƒ”ãƒ¼ã‚¯å¯¾å¿œ"
+
+# --- Network ---
+network:
+  controller_host: "0.0.0.0"
+  controller_port: 8080
+
+# --- Cost Budget (v8.0) ---
+budget:
+  monthly_target_yen: 4220  # v8.0: +Â¥270
+  breakdown:
+    pro_subscription_yen: 3000
+    electricity_yen: 950  # +Â¥150ï¼ˆCT102+104åˆ†ï¼‰
+    api_usage_yen: 255  # +Â¥105ï¼ˆOpusæœ€æ–°ç‰ˆã€Web Searchï¼‰
+    web_search_yen: 15  # è¶…éåˆ†äºˆå‚™
+  comparison:
+    vs_v7_0: "+6.8% (+Â¥270)"
+    vs_claude_code: "-45% (-Â¥3,481)"
+
+# --- Quality Metrics (v8.0 ç›®æ¨™) ---
+quality:
+  total_score: 99.5  # v8.0 ç›®æ¨™
+  breakdown:
+    simple: 94
+    medium: 99
+    complex: 99.5  # æ¼”ç¿’å ´åŠ¹æœ
+    strategic: 99   # æ€è€ƒç›£è¦–åŠ¹æœ
+    knowledge: 99.5 # RAGåŠ¹æœ
+    security: 99
+    consistency: 95 # é™£ä¸­æ—¥è¨˜åŠ¹æœ
+  improvements_vs_v7:
+    sandbox_verification: "+4ç‚¹"
+    rag_knowledge: "+0.5ç‚¹"
+    activity_memory: "ä¸€è²«æ€§å¤§å¹…å‘ä¸Š"
+    thought_monitoring: "å®‰å®šæ€§å‘ä¸Š"
+
+# --- Performance Metrics (v8.0) ---
+performance:
+  processing_time:
+    first_time_task: 70  # ç§’ï¼ˆå¤‰åŒ–ãªã—ï¼‰
+    repeated_same_task: 30  # ç§’ï¼ˆ-57%ã€é™£ä¸­æ—¥è¨˜åŠ¹æœï¼‰
+    similar_task: 53  # ç§’ï¼ˆ-24%ã€é™£ä¸­æ—¥è¨˜åŠ¹æœï¼‰
+    monthly_reduction_percent: -8.7
+  
+  execution_success_rate:
+    v7_0: 95
+    v8_0: 99  # æ¼”ç¿’å ´æ¤œè¨¼åŠ¹æœ
+    improvement: "+4%"
+  
+  consistency:
+    same_question_match: "95%"  # é™£ä¸­æ—¥è¨˜åŠ¹æœ
+    vs_v7_0: "+20%"
+    vs_claude_code: "+30%ï¼ˆæ¨å®šï¼‰"
+
+# --- Monthly Maintenance (åçœä¼š) ---
+maintenance:
+  schedule:
+    day_of_month: 1
+    hour: 9
+    minute: 0
+    timezone: "Asia/Tokyo"
+  checks:
+    # v7.0 ç¶™ç¶šé …ç›®
+    - id: "llm_versions"
+      name: "LLMãƒãƒ¼ã‚¸ãƒ§ãƒ³ç¢ºèª"
+      description: "Claude CLI / API ãƒ¢ãƒ‡ãƒ«ã®æœ€æ–°ãƒã‚§ãƒƒã‚¯"
+      auto_update: false
+    - id: "r1_japanese"
+      name: "R1æ—¥æœ¬èªãƒ¢ãƒ‡ãƒ«ç¢ºèª"
+      description: "CyberAgentç‰ˆ R1 ã®æ›´æ–°ç¢ºèª"
+      auto_update: false
+    - id: "mcp_servers"
+      name: "MCPã‚µãƒ¼ãƒãƒ¼æ›´æ–°"
+      description: "npm MCP ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®æ›´æ–°"
+      auto_update: true
+    - id: "groq_status"
+      name: "Groq APIç¢ºèª"
+      description: "9ç•ªè¶³è»½ï¼ˆè¨˜éŒ²ä¿‚ï¼‰ã®å‹•ä½œç¢ºèª"
+      auto_update: false
+    - id: "notion_sync"
+      name: "NotionåŒæœŸç¢ºèª"
+      description: "ãƒŠãƒ¬ãƒƒã‚¸DBåŒæœŸçŠ¶æ…‹ã®ç¢ºèª"
+      auto_update: false
+    - id: "system_health"
+      name: "ã‚·ã‚¹ãƒ†ãƒ ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯"
+      description: "å„ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®å‹•ä½œç¢ºèª"
+      auto_update: false
+    - id: "log_cleanup"
+      name: "ãƒ­ã‚°ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"
+      description: "30æ—¥ä»¥ä¸Šã®ãƒ­ã‚°å‰Šé™¤"
+      auto_update: true
+    - id: "cost_report"
+      name: "æœˆæ¬¡ã‚³ã‚¹ãƒˆãƒ¬ãƒãƒ¼ãƒˆ"
+      description: "APIä½¿ç”¨é‡ãƒ»ã‚³ã‚¹ãƒˆé›†è¨ˆ"
+      auto_update: true
+    # v8.0 æ–°è¦é …ç›®
+    - id: "knowledge_base_health"
+      name: "çŸ¥è­˜åŸºç›¤ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯"
+      description: "Qdrantå‹•ä½œçŠ¶æ³ãƒ»ãƒ‡ãƒ¼ã‚¿å“è³ªç¢ºèª"
+      auto_update: false
+    - id: "activity_memory_analysis"
+      name: "é™£ä¸­æ—¥è¨˜åˆ†æ"
+      description: "åˆ¤æ–­å±¥æ­´ã®çµ±è¨ˆåˆ†æãƒ»ãƒ‘ã‚¿ãƒ¼ãƒ³ç™ºè¦‹"
+      auto_update: false
+    - id: "sandbox_performance"
+      name: "æ¼”ç¿’å ´æ€§èƒ½ãƒ¬ãƒãƒ¼ãƒˆ"
+      description: "å®Ÿè¡ŒæˆåŠŸç‡ãƒ»ã‚¨ãƒ©ãƒ¼åˆ†æ"
+      auto_update: false
+    - id: "ollama_search_usage"
+      name: "Webæ¤œç´¢ä½¿ç”¨çŠ¶æ³"
+      description: "10ç•ªè¶³è»½ã®æ¤œç´¢å±¥æ­´ãƒ»åŠ¹æœæ¸¬å®š"
+      auto_update: false
+    - id: "memory_optimization"
+      name: "ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡æœ€é©åŒ–"
+      description: "CTçµ±åˆãƒ»ZRAMä½¿ç”¨çŠ¶æ³ã®ç¢ºèª"
+      auto_update: false
+    - id: "rag_data_cleanup"
+      name: "RAGãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"
+      description: "30æ—¥ä»¥ä¸Šã®çŸ¥è­˜ãƒ‡ãƒ¼ã‚¿å‰Šé™¤"
+      auto_update: true
+    - id: "activity_data_cleanup"
+      name: "é™£ä¸­æ—¥è¨˜ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"
+      description: "90æ—¥ä»¥ä¸Šã®æ´»å‹•è¨˜éŒ²å‰Šé™¤"
+      auto_update: true
+  
+  notification:
+    slack_channel: "#shogun-maintenance"
+  reports:
+    directory: "reports/maintenance"
+    retention_days: 365
+
+# --- v8.0 Implementation Roadmap ---
+implementation:
+  phases:
+    phase1:
+      name: "åŸºç›¤æ•´å‚™"
+      priority: "æœ€é«˜"
+      duration_hours: "2-3"
+      tasks:
+        - "Proxmoxæœ€é©åŒ–ï¼ˆHugePagesï¼‰"
+        - "CT 102ï¼ˆæ¼”ç¿’å ´ï¼‰æ§‹ç¯‰"
+        - "CTçµ±åˆï¼ˆ104â†’100ï¼‰"
+        - "å‹•ä½œç¢ºèª"
+    
+    phase2:
+      name: "10ç•ªè¶³è»½çµ±åˆ"
+      priority: "æœ€é«˜"
+      duration_hours: "1"
+      tasks:
+        - "Ollama SDKå°å…¥"
+        - "Webæ¤œç´¢æ©Ÿèƒ½å®Ÿè£…"
+        - "RAGè‡ªå‹•ä¿å­˜æ©Ÿèƒ½"
+        - "çµ±åˆãƒ†ã‚¹ãƒˆ"
+    
+    phase3:
+      name: "ä¾å¤§å°†å¼·åŒ–"
+      priority: "æœ€é«˜"
+      duration_hours: "2"
+      tasks:
+        - "çŸ¥è­˜åŸºç›¤çµ±åˆ"
+        - "é™£ä¸­æ—¥è¨˜å®Ÿè£…"
+        - "æ¼”ç¿’å ´é€£æº"
+        - "å‹•çš„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿"
+    
+    phase4:
+      name: "å°†è»ãƒ»å®¶è€å¼·åŒ–"
+      priority: "é«˜"
+      duration_hours: "1.5"
+      tasks:
+        - "å°†è»RAGçµ±åˆ"
+        - "å®¶è€RAGçµ±åˆ"
+        - "æ€è€ƒç›£è¦–æ©Ÿèƒ½"
+    
+    phase5:
+      name: "æœ€é©åŒ–ãƒ»ãƒ†ã‚¹ãƒˆ"
+      priority: "é«˜"
+      duration_hours: "3"
+      tasks:
+        - "å‹•çš„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–"
+        - "è‡ªå‹•ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"
+        - "çµ±åˆãƒ†ã‚¹ãƒˆ"
+        - "ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ›´æ–°"
+  
+  total_implementation_time: "9.5-10.5æ™‚é–“"
+  recommended_schedule:
+    day1: "Phase 1-3ï¼ˆåŸºç›¤ãƒ»è¶³è»½ãƒ»ä¾å¤§å°†ï¼‰5-6æ™‚é–“"
+    day2: "Phase 4-5ï¼ˆå°†è»å®¶è€ãƒ»æœ€é©åŒ–ãƒ»ãƒ†ã‚¹ãƒˆï¼‰4.5æ™‚é–“"
+  
+  critical_path:
+    - "æ¼”ç¿’å ´æ§‹ç¯‰"
+    - "é™£ä¸­æ—¥è¨˜å®Ÿè£…"
+    - "çµ±åˆãƒ†ã‚¹ãƒˆ"
+
+# --- Expected Outcomes ---
+expected_outcomes:
+  technical:
+    - "å“è³ª99.5ç‚¹ï¼ˆæœ¬å®¶Claude Codeè¶…ãˆï¼‰"
+    - "ã‚³ã‚¹ãƒˆÂ¥4,220ï¼ˆæœ¬å®¶æ¯”-45%ï¼‰"
+    - "å®Ÿè¨¼ä¸»ç¾©å°å…¥ï¼ˆæ¼”ç¿’å ´ï¼‰"
+    - "äºŒé‡è¨˜æ†¶ã‚·ã‚¹ãƒ†ãƒ "
+    - "ä¸€è²«æ€§ç¢ºä¿"
+  
+  philosophical:
+    - "ã‚¹ãƒ”ãƒ¼ãƒ‰ã‚ˆã‚Šè³ªã®å®Œæˆå½¢"
+    - "äººé–“æ€è€ƒãƒ—ãƒ­ã‚»ã‚¹æ¨¡å€£"
+    - "ç¶™ç¶šçš„å­¦ç¿’å®Ÿç¾"
+    - "çŸ¥è­˜ã®æ°‘ä¸»åŒ–"
+  
+  practical:
+    - "Omni-P4é–‹ç™ºåŠ é€Ÿ"
+    - "äºˆæ¸¬å¯èƒ½ã‚³ã‚¹ãƒˆ"
+    - "ã‚¹ã‚±ãƒ¼ãƒ©ãƒ–ãƒ«è¨­è¨ˆ"
+    - "é•·æœŸæŠ•è³‡ä¾¡å€¤"
\ No newline at end of file
diff --git a/core/__init__.py b/core/__init__.py
new file mode 100644
index 0000000..e69de29
diff --git a/core/activity_memory.py b/core/activity_memory.py
new file mode 100644
index 0000000..2b06c27
--- /dev/null
+++ b/core/activity_memory.py
@@ -0,0 +1,709 @@
+"""
+å°†è»ã‚·ã‚¹ãƒ†ãƒ  v8.0 - Activity Memory (é™£ä¸­æ—¥è¨˜) System
+æ´»å‹•è¨˜æ†¶: ä¾å¤§å°†ï¼ˆR1ï¼‰å°‚ç”¨ã®åˆ¤æ–­å±¥æ­´ãƒ»å‚™å¿˜éŒ²ã‚·ã‚¹ãƒ†ãƒ 
+
+Features:
+- SQLite database for lightweight storage
+- Task similarity detection
+- Processing time optimization (-57% for similar tasks)
+- Consistency improvement through historical reference
+- Auto-cleanup (90 days retention)
+"""
+
+import sqlite3
+import json
+import logging
+import hashlib
+from datetime import datetime, timedelta
+from typing import List, Dict, Any, Optional, Tuple
+from dataclasses import dataclass, asdict
+from enum import Enum
+import asyncio
+from pathlib import Path
+
+# Configure logging
+logging.basicConfig(level=logging.INFO)
+logger = logging.getLogger(__name__)
+
+
+class TaskComplexity(Enum):
+    """Task complexity levels"""
+    SIMPLE = "simple"
+    MEDIUM = "medium"
+    COMPLEX = "complex"
+    STRATEGIC = "strategic"
+
+
+class DecisionConfidence(Enum):
+    """Decision confidence levels"""
+    LOW = "low"
+    MEDIUM = "medium"
+    HIGH = "high"
+    VERY_HIGH = "very_high"
+
+
+@dataclass
+class TaskRecord:
+    """Individual task record in the activity memory"""
+    id: str
+    task_hash: str  # Hash for similarity detection
+    task_summary: str
+    task_type: TaskComplexity
+    think_summary: str  # Condensed <think> process
+    final_decision: str
+    decision_reasoning: str
+    confidence_level: DecisionConfidence
+    processing_time_seconds: float
+    tools_used: List[str]
+    similar_tasks: List[str]  # IDs of similar previous tasks
+    success: bool
+    error_message: Optional[str] = None
+    metadata: Dict[str, Any] = None
+    created_at: datetime = None
+    
+    def __post_init__(self):
+        if self.created_at is None:
+            self.created_at = datetime.utcnow()
+        if self.metadata is None:
+            self.metadata = {}
+
+
+@dataclass
+class SimilarTaskResult:
+    """Result from similar task search"""
+    task_id: str
+    task_summary: str
+    final_decision: str
+    decision_reasoning: str
+    confidence_level: DecisionConfidence
+    processing_time_seconds: float
+    similarity_score: float
+    days_ago: int
+
+
+class ActivityMemory:
+    """
+    å°†è»ã‚·ã‚¹ãƒ†ãƒ  Activity Memory (é™£ä¸­æ—¥è¨˜) Implementation
+    
+    Provides persistent memory for the Taisho (R1) agent to:
+    - Record all task decisions and reasoning
+    - Find similar past tasks for faster processing
+    - Maintain consistency in decision-making
+    - Learn from past experiences
+    """
+    
+    def __init__(
+        self,
+        database_path: str = "/opt/shogun/taisho_activity.db",
+        retention_days: int = 90
+    ):
+        self.database_path = Path(database_path)
+        self.retention_days = retention_days
+        
+        # Ensure directory exists
+        self.database_path.parent.mkdir(parents=True, exist_ok=True)
+        
+        # Initialize database
+        self._initialize_database()
+        
+        logger.info(f"ActivityMemory initialized - Database: {database_path}")
+    
+    def _initialize_database(self) -> None:
+        """Initialize SQLite database and create tables"""
+        try:
+            with sqlite3.connect(self.database_path) as conn:
+                conn.execute("PRAGMA journal_mode=WAL")  # Enable WAL mode for better concurrency
+                conn.execute("PRAGMA synchronous=NORMAL")  # Balance between safety and speed
+                
+                # Create main activities table
+                conn.execute("""
+                    CREATE TABLE IF NOT EXISTS activities (
+                        id TEXT PRIMARY KEY,
+                        task_hash TEXT NOT NULL,
+                        task_summary TEXT NOT NULL,
+                        task_type TEXT NOT NULL,
+                        think_summary TEXT NOT NULL,
+                        final_decision TEXT NOT NULL,
+                        decision_reasoning TEXT NOT NULL,
+                        confidence_level TEXT NOT NULL,
+                        processing_time_seconds REAL NOT NULL,
+                        tools_used TEXT NOT NULL,  -- JSON array
+                        similar_tasks TEXT,        -- JSON array of similar task IDs
+                        success BOOLEAN NOT NULL,
+                        error_message TEXT,
+                        metadata TEXT,             -- JSON object
+                        created_at TIMESTAMP NOT NULL,
+                        UNIQUE(task_hash, created_at)
+                    )
+                """)
+                
+                # Create index for fast similarity searches
+                conn.execute("""
+                    CREATE INDEX IF NOT EXISTS idx_task_hash ON activities(task_hash)
+                """)
+                
+                conn.execute("""
+                    CREATE INDEX IF NOT EXISTS idx_created_at ON activities(created_at DESC)
+                """)
+                
+                conn.execute("""
+                    CREATE INDEX IF NOT EXISTS idx_task_type ON activities(task_type)
+                """)
+                
+                conn.execute("""
+                    CREATE INDEX IF NOT EXISTS idx_success ON activities(success)
+                """)
+                
+                # Create full-text search table for content search
+                conn.execute("""
+                    CREATE VIRTUAL TABLE IF NOT EXISTS activities_fts USING fts5(
+                        id,
+                        task_summary,
+                        think_summary,
+                        final_decision,
+                        decision_reasoning,
+                        content=activities
+                    )
+                """)
+                
+                # Create trigger to keep FTS table updated
+                conn.execute("""
+                    CREATE TRIGGER IF NOT EXISTS activities_fts_insert AFTER INSERT ON activities
+                    BEGIN
+                        INSERT INTO activities_fts(id, task_summary, think_summary, final_decision, decision_reasoning)
+                        VALUES (NEW.id, NEW.task_summary, NEW.think_summary, NEW.final_decision, NEW.decision_reasoning);
+                    END
+                """)
+                
+                conn.execute("""
+                    CREATE TRIGGER IF NOT EXISTS activities_fts_update AFTER UPDATE ON activities
+                    BEGIN
+                        UPDATE activities_fts SET
+                            task_summary = NEW.task_summary,
+                            think_summary = NEW.think_summary,
+                            final_decision = NEW.final_decision,
+                            decision_reasoning = NEW.decision_reasoning
+                        WHERE id = NEW.id;
+                    END
+                """)
+                
+                conn.execute("""
+                    CREATE TRIGGER IF NOT EXISTS activities_fts_delete AFTER DELETE ON activities
+                    BEGIN
+                        DELETE FROM activities_fts WHERE id = OLD.id;
+                    END
+                """)
+                
+                conn.commit()
+                
+            logger.info("Database initialized successfully")
+            
+        except Exception as e:
+            logger.error(f"Failed to initialize database: {e}")
+            raise
+    
+    def _generate_task_hash(self, task_summary: str, task_type: TaskComplexity) -> str:
+        """Generate hash for task similarity detection"""
+        # Normalize the task summary for consistent hashing
+        normalized = task_summary.lower().strip()
+        # Include task type to differentiate similar tasks of different complexity
+        hash_input = f"{normalized}:{task_type.value}"
+        return hashlib.md5(hash_input.encode()).hexdigest()
+    
+    async def record_activity(self, record: TaskRecord) -> bool:
+        """Record a new task activity"""
+        try:
+            with sqlite3.connect(self.database_path) as conn:
+                # Generate task hash if not provided
+                if not record.task_hash:
+                    record.task_hash = self._generate_task_hash(record.task_summary, record.task_type)
+                
+                # Convert lists and objects to JSON
+                tools_used_json = json.dumps(record.tools_used)
+                similar_tasks_json = json.dumps(record.similar_tasks) if record.similar_tasks else "[]"
+                metadata_json = json.dumps(record.metadata) if record.metadata else "{}"
+                
+                conn.execute("""
+                    INSERT OR REPLACE INTO activities (
+                        id, task_hash, task_summary, task_type, think_summary,
+                        final_decision, decision_reasoning, confidence_level,
+                        processing_time_seconds, tools_used, similar_tasks,
+                        success, error_message, metadata, created_at
+                    ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
+                """, (
+                    record.id,
+                    record.task_hash,
+                    record.task_summary,
+                    record.task_type.value,
+                    record.think_summary,
+                    record.final_decision,
+                    record.decision_reasoning,
+                    record.confidence_level.value,
+                    record.processing_time_seconds,
+                    tools_used_json,
+                    similar_tasks_json,
+                    record.success,
+                    record.error_message,
+                    metadata_json,
+                    record.created_at.isoformat()
+                ))
+                
+                conn.commit()
+                
+            logger.info(f"Recorded activity: {record.id} - {record.task_summary[:50]}...")
+            return True
+            
+        except Exception as e:
+            logger.error(f"Failed to record activity {record.id}: {e}")
+            return False
+    
+    async def find_similar_tasks(
+        self,
+        task_summary: str,
+        task_type: TaskComplexity,
+        limit: int = 5,
+        min_confidence: DecisionConfidence = DecisionConfidence.MEDIUM,
+        success_only: bool = True
+    ) -> List[SimilarTaskResult]:
+        """
+        Find similar tasks from the activity history
+        
+        Args:
+            task_summary: Current task description
+            task_type: Task complexity level
+            limit: Maximum number of results
+            min_confidence: Minimum confidence level for results
+            success_only: Only return successful tasks
+        
+        Returns:
+            List of similar task results with similarity scores
+        """
+        try:
+            task_hash = self._generate_task_hash(task_summary, task_type)
+            
+            with sqlite3.connect(self.database_path) as conn:
+                conn.row_factory = sqlite3.Row
+                
+                # Build query conditions
+                conditions = ["task_type = ?"]
+                params = [task_type.value]
+                
+                if success_only:
+                    conditions.append("success = 1")
+                
+                # Filter by minimum confidence
+                confidence_levels = {
+                    DecisionConfidence.LOW: 0,
+                    DecisionConfidence.MEDIUM: 1,
+                    DecisionConfidence.HIGH: 2,
+                    DecisionConfidence.VERY_HIGH: 3
+                }
+                
+                if min_confidence != DecisionConfidence.LOW:
+                    confidence_filter = []
+                    for conf, level in confidence_levels.items():
+                        if level >= confidence_levels[min_confidence]:
+                            confidence_filter.append(f"'{conf.value}'")
+                    
+                    conditions.append(f"confidence_level IN ({','.join(confidence_filter)})")
+                
+                where_clause = " AND ".join(conditions)
+                
+                # First, try exact hash match (same task)
+                cursor = conn.execute(f"""
+                    SELECT * FROM activities 
+                    WHERE task_hash = ? AND {where_clause}
+                    ORDER BY created_at DESC 
+                    LIMIT ?
+                """, [task_hash] + params + [limit])
+                
+                exact_matches = cursor.fetchall()
+                
+                # If no exact matches, use full-text search for similar content
+                if not exact_matches:
+                    # Extract keywords from task summary
+                    keywords = self._extract_keywords(task_summary)
+                    fts_query = " OR ".join(f'"{keyword}"' for keyword in keywords[:5])  # Top 5 keywords
+                    
+                    cursor = conn.execute(f"""
+                        SELECT a.* FROM activities a
+                        JOIN activities_fts fts ON a.id = fts.id
+                        WHERE fts MATCH ? AND {where_clause}
+                        ORDER BY rank, a.created_at DESC
+                        LIMIT ?
+                    """, [fts_query] + params + [limit])
+                    
+                    similar_matches = cursor.fetchall()
+                else:
+                    similar_matches = exact_matches
+                
+                # Convert to SimilarTaskResult objects
+                results = []
+                now = datetime.utcnow()
+                
+                for row in similar_matches:
+                    created_at = datetime.fromisoformat(row['created_at'])
+                    days_ago = (now - created_at).days
+                    
+                    # Calculate similarity score
+                    if row['task_hash'] == task_hash:
+                        similarity_score = 1.0  # Exact match
+                    else:
+                        # Simple similarity based on keyword overlap
+                        similarity_score = self._calculate_text_similarity(
+                            task_summary, row['task_summary']
+                        )
+                    
+                    result = SimilarTaskResult(
+                        task_id=row['id'],
+                        task_summary=row['task_summary'],
+                        final_decision=row['final_decision'],
+                        decision_reasoning=row['decision_reasoning'],
+                        confidence_level=DecisionConfidence(row['confidence_level']),
+                        processing_time_seconds=row['processing_time_seconds'],
+                        similarity_score=similarity_score,
+                        days_ago=days_ago
+                    )
+                    
+                    results.append(result)
+                
+                # Sort by similarity score and recency
+                results.sort(key=lambda x: (x.similarity_score, -x.days_ago), reverse=True)
+                
+                logger.info(f"Found {len(results)} similar tasks for: '{task_summary[:50]}...'")
+                return results
+                
+        except Exception as e:
+            logger.error(f"Failed to find similar tasks: {e}")
+            return []
+    
+    def _extract_keywords(self, text: str) -> List[str]:
+        """Extract important keywords from text"""
+        # Simple keyword extraction - could be improved with NLP
+        import re
+        
+        # Remove common words (stop words)
+        stop_words = {
+            'ã®', 'ã‚’', 'ã«', 'ãŒ', 'ã¯', 'ã§', 'ã¨', 'ã‹ã‚‰', 'ã¾ã§', 'ã‚ˆã‚Š',
+            'ã—ã¦', 'ã™ã‚‹', 'ã—ãŸ', 'ã•ã‚Œã‚‹', 'ã§ã‚ã‚‹', 'ã§ã™', 'ã¾ã™',
+            'the', 'is', 'at', 'which', 'on', 'a', 'an', 'and', 'or', 'but',
+            'in', 'with', 'to', 'for', 'of', 'as', 'by', 'that', 'this'
+        }
+        
+        # Extract words (alphanumeric sequences)
+        words = re.findall(r'\w+', text.lower())
+        
+        # Filter out stop words and short words
+        keywords = [word for word in words if word not in stop_words and len(word) > 2]
+        
+        # Return unique keywords, sorted by length (longer words first)
+        return sorted(list(set(keywords)), key=len, reverse=True)
+    
+    def _calculate_text_similarity(self, text1: str, text2: str) -> float:
+        """Calculate simple text similarity based on keyword overlap"""
+        keywords1 = set(self._extract_keywords(text1))
+        keywords2 = set(self._extract_keywords(text2))
+        
+        if not keywords1 or not keywords2:
+            return 0.0
+        
+        # Jaccard similarity
+        intersection = keywords1.intersection(keywords2)
+        union = keywords1.union(keywords2)
+        
+        return len(intersection) / len(union)
+    
+    async def get_task_by_id(self, task_id: str) -> Optional[TaskRecord]:
+        """Retrieve a specific task record"""
+        try:
+            with sqlite3.connect(self.database_path) as conn:
+                conn.row_factory = sqlite3.Row
+                
+                cursor = conn.execute(
+                    "SELECT * FROM activities WHERE id = ?",
+                    (task_id,)
+                )
+                
+                row = cursor.fetchone()
+                if not row:
+                    return None
+                
+                # Convert back to TaskRecord
+                record = TaskRecord(
+                    id=row['id'],
+                    task_hash=row['task_hash'],
+                    task_summary=row['task_summary'],
+                    task_type=TaskComplexity(row['task_type']),
+                    think_summary=row['think_summary'],
+                    final_decision=row['final_decision'],
+                    decision_reasoning=row['decision_reasoning'],
+                    confidence_level=DecisionConfidence(row['confidence_level']),
+                    processing_time_seconds=row['processing_time_seconds'],
+                    tools_used=json.loads(row['tools_used']),
+                    similar_tasks=json.loads(row['similar_tasks']) if row['similar_tasks'] else [],
+                    success=bool(row['success']),
+                    error_message=row['error_message'],
+                    metadata=json.loads(row['metadata']) if row['metadata'] else {},
+                    created_at=datetime.fromisoformat(row['created_at'])
+                )
+                
+                return record
+                
+        except Exception as e:
+            logger.error(f"Failed to retrieve task {task_id}: {e}")
+            return None
+    
+    async def search_tasks(
+        self,
+        query: str,
+        task_type: Optional[TaskComplexity] = None,
+        limit: int = 10
+    ) -> List[TaskRecord]:
+        """Search tasks using full-text search"""
+        try:
+            with sqlite3.connect(self.database_path) as conn:
+                conn.row_factory = sqlite3.Row
+                
+                # Build query
+                conditions = ["fts MATCH ?"]
+                params = [query]
+                
+                if task_type:
+                    conditions.append("a.task_type = ?")
+                    params.append(task_type.value)
+                
+                where_clause = " AND ".join(conditions)
+                
+                cursor = conn.execute(f"""
+                    SELECT a.* FROM activities a
+                    JOIN activities_fts fts ON a.id = fts.id
+                    WHERE {where_clause}
+                    ORDER BY rank, a.created_at DESC
+                    LIMIT ?
+                """, params + [limit])
+                
+                rows = cursor.fetchall()
+                
+                # Convert to TaskRecord objects
+                records = []
+                for row in rows:
+                    record = TaskRecord(
+                        id=row['id'],
+                        task_hash=row['task_hash'],
+                        task_summary=row['task_summary'],
+                        task_type=TaskComplexity(row['task_type']),
+                        think_summary=row['think_summary'],
+                        final_decision=row['final_decision'],
+                        decision_reasoning=row['decision_reasoning'],
+                        confidence_level=DecisionConfidence(row['confidence_level']),
+                        processing_time_seconds=row['processing_time_seconds'],
+                        tools_used=json.loads(row['tools_used']),
+                        similar_tasks=json.loads(row['similar_tasks']) if row['similar_tasks'] else [],
+                        success=bool(row['success']),
+                        error_message=row['error_message'],
+                        metadata=json.loads(row['metadata']) if row['metadata'] else {},
+                        created_at=datetime.fromisoformat(row['created_at'])
+                    )
+                    records.append(record)
+                
+                logger.info(f"Search '{query}' returned {len(records)} results")
+                return records
+                
+        except Exception as e:
+            logger.error(f"Search failed for query '{query}': {e}")
+            return []
+    
+    async def cleanup_old_records(self) -> int:
+        """Remove records older than retention_days"""
+        try:
+            cutoff_date = datetime.utcnow() - timedelta(days=self.retention_days)
+            
+            with sqlite3.connect(self.database_path) as conn:
+                cursor = conn.execute(
+                    "DELETE FROM activities WHERE created_at < ?",
+                    (cutoff_date.isoformat(),)
+                )
+                
+                deleted_count = cursor.rowcount
+                conn.commit()
+                
+                if deleted_count > 0:
+                    # Rebuild FTS index after cleanup
+                    conn.execute("INSERT INTO activities_fts(activities_fts) VALUES('rebuild')")
+                    conn.commit()
+                
+                logger.info(f"Cleanup: Deleted {deleted_count} old records (older than {self.retention_days} days)")
+                return deleted_count
+                
+        except Exception as e:
+            logger.error(f"Cleanup failed: {e}")
+            return 0
+    
+    async def get_statistics(self) -> Dict[str, Any]:
+        """Get activity memory statistics"""
+        try:
+            with sqlite3.connect(self.database_path) as conn:
+                # Total records
+                cursor = conn.execute("SELECT COUNT(*) FROM activities")
+                total_records = cursor.fetchone()[0]
+                
+                # Records by type
+                cursor = conn.execute("""
+                    SELECT task_type, COUNT(*) as count
+                    FROM activities
+                    GROUP BY task_type
+                """)
+                type_distribution = dict(cursor.fetchall())
+                
+                # Success rate
+                cursor = conn.execute("""
+                    SELECT success, COUNT(*) as count
+                    FROM activities
+                    GROUP BY success
+                """)
+                success_stats = dict(cursor.fetchall())
+                
+                success_rate = (
+                    success_stats.get(1, 0) / total_records * 100
+                    if total_records > 0 else 0
+                )
+                
+                # Average processing time by type
+                cursor = conn.execute("""
+                    SELECT task_type, AVG(processing_time_seconds) as avg_time
+                    FROM activities
+                    WHERE success = 1
+                    GROUP BY task_type
+                """)
+                avg_processing_times = dict(cursor.fetchall())
+                
+                # Recent activity (last 7 days)
+                week_ago = (datetime.utcnow() - timedelta(days=7)).isoformat()
+                cursor = conn.execute(
+                    "SELECT COUNT(*) FROM activities WHERE created_at > ?",
+                    (week_ago,)
+                )
+                recent_activity = cursor.fetchone()[0]
+                
+                stats = {
+                    "total_records": total_records,
+                    "type_distribution": type_distribution,
+                    "success_rate_percent": round(success_rate, 2),
+                    "avg_processing_times": avg_processing_times,
+                    "recent_activity_7_days": recent_activity,
+                    "retention_days": self.retention_days,
+                    "database_size_mb": round(self.database_path.stat().st_size / 1024 / 1024, 2)
+                }
+                
+                return stats
+                
+        except Exception as e:
+            logger.error(f"Failed to get statistics: {e}")
+            return {}
+    
+    async def export_decisions(
+        self,
+        output_file: str,
+        task_type: Optional[TaskComplexity] = None,
+        days_back: int = 30
+    ) -> bool:
+        """Export decisions to JSON file for analysis"""
+        try:
+            cutoff_date = (datetime.utcnow() - timedelta(days=days_back)).isoformat()
+            
+            with sqlite3.connect(self.database_path) as conn:
+                conn.row_factory = sqlite3.Row
+                
+                conditions = ["created_at > ?"]
+                params = [cutoff_date]
+                
+                if task_type:
+                    conditions.append("task_type = ?")
+                    params.append(task_type.value)
+                
+                where_clause = " AND ".join(conditions)
+                
+                cursor = conn.execute(f"""
+                    SELECT * FROM activities
+                    WHERE {where_clause}
+                    ORDER BY created_at DESC
+                """, params)
+                
+                records = []
+                for row in cursor.fetchall():
+                    record_dict = dict(row)
+                    # Parse JSON fields
+                    record_dict['tools_used'] = json.loads(record_dict['tools_used'])
+                    record_dict['similar_tasks'] = json.loads(record_dict['similar_tasks'] or '[]')
+                    record_dict['metadata'] = json.loads(record_dict['metadata'] or '{}')
+                    records.append(record_dict)
+                
+                # Write to file
+                with open(output_file, 'w', encoding='utf-8') as f:
+                    json.dump(records, f, ensure_ascii=False, indent=2, default=str)
+                
+                logger.info(f"Exported {len(records)} decisions to {output_file}")
+                return True
+                
+        except Exception as e:
+            logger.error(f"Export failed: {e}")
+            return False
+
+
+# Factory function
+def create_activity_memory(config: Dict[str, Any]) -> ActivityMemory:
+    """Create ActivityMemory instance from configuration"""
+    return ActivityMemory(
+        database_path=config.get("database_path", "/opt/shogun/taisho_activity.db"),
+        retention_days=config.get("retention_days", 90)
+    )
+
+
+# Example usage and testing
+if __name__ == "__main__":
+    async def test_activity_memory():
+        """Test the activity memory functionality"""
+        am = ActivityMemory(database_path="test_activity.db")
+        
+        # Test record
+        test_record = TaskRecord(
+            id="task_001",
+            task_hash="",  # Will be generated
+            task_summary="I2Sè¨­å®šã®æœ€é©åŒ–",
+            task_type=TaskComplexity.MEDIUM,
+            think_summary="ãƒãƒƒãƒ•ã‚¡ã‚µã‚¤ã‚ºã¨ã‚µãƒ³ãƒ—ãƒ«ãƒ¬ãƒ¼ãƒˆã‚’æ¤œè¨ã€‚æ—¢å­˜ã®è¨­å®šã‚’ç¢ºèªã—ã€ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œã€‚",
+            final_decision="buffer_size=1024, sample_rate=44100ã«è¨­å®š",
+            decision_reasoning="44.1kHzã¯æ¨™æº–çš„ã§äº’æ›æ€§ãŒé«˜ãã€1024ãƒã‚¤ãƒˆãƒãƒƒãƒ•ã‚¡ã¯é…å»¶ã¨ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã®ãƒãƒ©ãƒ³ã‚¹ãŒè‰¯ã„",
+            confidence_level=DecisionConfidence.HIGH,
+            processing_time_seconds=65.3,
+            tools_used=["filesystem", "github"],
+            similar_tasks=[],
+            success=True,
+            metadata={"hardware": "ESP32", "audio_codec": "MAX98357"}
+        )
+        
+        # Record activity
+        success = await am.record_activity(test_record)
+        print(f"Record activity: {success}")
+        
+        # Find similar tasks
+        similar = await am.find_similar_tasks("I2S ãƒãƒƒãƒ•ã‚¡ã‚µã‚¤ã‚ºè¨­å®š", TaskComplexity.MEDIUM)
+        print(f"Found {len(similar)} similar tasks")
+        for task in similar:
+            print(f"  - {task.task_summary} (similarity: {task.similarity_score:.3f})")
+        
+        # Search tasks
+        search_results = await am.search_tasks("I2S")
+        print(f"Search results: {len(search_results)}")
+        
+        # Statistics
+        stats = await am.get_statistics()
+        print(f"Statistics: {stats}")
+        
+        # Cleanup test database
+        import os
+        if os.path.exists("test_activity.db"):
+            os.remove("test_activity.db")
+    
+    # Run test
+    asyncio.run(test_activity_memory())
\ No newline at end of file
diff --git a/core/ashigaru_selection.py b/core/ashigaru_selection.py
new file mode 100644
index 0000000..9478015
--- /dev/null
+++ b/core/ashigaru_selection.py
@@ -0,0 +1,496 @@
+"""Dynamic Ashigaru Selection System
+
+Intelligent selection of the most suitable è¶³è»½ (ashigaru) agents for each task.
+
+Features:
+  - Dynamic selection based on task requirements
+  - Maximum 4 concurrent ashigaru limit
+  - Load balancing and performance monitoring
+  - Priority-based assignment
+  - Automatic failover and replacement
+
+This system ensures optimal resource utilization while maintaining the traditional
+shogun hierarchy structure.
+"""
+
+import asyncio
+import logging
+from datetime import datetime, timedelta
+from typing import Dict, List, Optional, Set, Tuple, Any
+from dataclasses import dataclass
+from enum import Enum
+import json
+
+
+logger = logging.getLogger("shogun.ashigaru_selection")
+
+
+class TaskComplexity(Enum):
+    """Task complexity levels for ashigaru selection."""
+    SIMPLE = "simple"
+    MODERATE = "moderate" 
+    COMPLEX = "complex"
+    CRITICAL = "critical"
+
+
+class AshigaruStatus(Enum):
+    """Ashigaru operational status."""
+    AVAILABLE = "available"
+    BUSY = "busy"
+    MAINTENANCE = "maintenance"
+    ERROR = "error"
+    OFFLINE = "offline"
+
+
+@dataclass
+class AshigaruAgent:
+    """Represents a single ashigaru agent."""
+    id: int
+    name: str
+    role: str
+    capabilities: List[str]
+    current_load: float = 0.0
+    status: AshigaruStatus = AshigaruStatus.AVAILABLE
+    last_used: Optional[datetime] = None
+    success_rate: float = 1.0
+    avg_response_time: float = 0.0
+    memory_usage_mb: int = 0
+    current_task: Optional[str] = None
+
+
+@dataclass
+class TaskRequirement:
+    """Task requirements for ashigaru selection."""
+    task_id: str
+    complexity: TaskComplexity
+    required_capabilities: List[str]
+    preferred_agents: List[str] = None
+    max_duration: Optional[timedelta] = None
+    priority: int = 5  # 1-10, higher is more urgent
+
+
+class AshigaruSelectionManager:
+    """Manages dynamic selection of ashigaru agents."""
+
+    def __init__(self, config: Dict[str, Any]):
+        self.config = config
+        self.max_active = config.get("max_active", 4)
+        self.total_count = config.get("count", 8)
+        
+        # Initialize ashigaru agents
+        self.agents: Dict[int, AshigaruAgent] = {}
+        self.active_tasks: Dict[str, Set[int]] = {}  # task_id -> agent_ids
+        self.task_queue: List[TaskRequirement] = []
+        
+        # Statistics
+        self.stats = {
+            "selections_made": 0,
+            "tasks_completed": 0,
+            "tasks_failed": 0,
+            "avg_selection_time": 0.0,
+            "load_balanced_count": 0,
+            "failover_count": 0,
+        }
+        
+        self._initialize_agents()
+        
+    def _initialize_agents(self) -> None:
+        """Initialize ashigaru agents from configuration."""
+        servers = self.config.get("servers", [])
+        
+        for server_config in servers:
+            agent_id = server_config.get("id")
+            if agent_id is None:
+                continue
+                
+            # Map server roles to capabilities
+            capabilities = self._map_role_to_capabilities(server_config.get("role", ""))
+            
+            agent = AshigaruAgent(
+                id=agent_id,
+                name=server_config.get("name", f"ashigaru_{agent_id}"),
+                role=server_config.get("role", "æ±ç”¨"),
+                capabilities=capabilities,
+            )
+            
+            self.agents[agent_id] = agent
+            
+        logger.info("[è¶³è»½é¸æŠœ] åˆæœŸåŒ–å®Œäº†: %dåã®è¶³è»½", len(self.agents))
+    
+    def _map_role_to_capabilities(self, role: str) -> List[str]:
+        """Map server role to specific capabilities."""
+        capability_map = {
+            "ãƒ•ã‚¡ã‚¤ãƒ«æ“ä½œ": ["file_read", "file_write", "file_management"],
+            "Git/GitHubæ“ä½œ": ["git", "github", "version_control"],
+            "Webæƒ…å ±å–å¾—": ["web_fetch", "http_requests", "api_calls"],
+            "é•·æœŸè¨˜æ†¶": ["memory", "storage", "persistence"],
+            "ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹": ["database", "sql", "data_management"],
+            "ãƒ–ãƒ©ã‚¦ã‚¶è‡ªå‹•åŒ–": ["browser", "automation", "web_scraping"],
+            "Webæ¤œç´¢": ["search", "information_retrieval"],
+            "ãƒãƒ¼ãƒ é€£æº": ["communication", "notifications", "slack"],
+        }
+        
+        return capability_map.get(role, ["general"])
+    
+    async def select_ashigaru(
+        self, 
+        task_req: TaskRequirement,
+        exclude_agents: Optional[Set[int]] = None
+    ) -> List[int]:
+        """Select optimal ashigaru agents for a task."""
+        start_time = datetime.now()
+        
+        try:
+            # Check if we're at capacity
+            current_active = self._count_active_agents()
+            if current_active >= self.max_active:
+                logger.warning("[è¶³è»½é¸æŠœ] æœ€å¤§åŒæ™‚å®Ÿè¡Œæ•°ã«åˆ°é” (%d/%d)", current_active, self.max_active)
+                # Queue the task if at capacity
+                self.task_queue.append(task_req)
+                return []
+            
+            # Filter available agents
+            available_agents = self._get_available_agents(exclude_agents)
+            if not available_agents:
+                logger.warning("[è¶³è»½é¸æŠœ] åˆ©ç”¨å¯èƒ½ãªè¶³è»½ãªã—")
+                return []
+            
+            # Score and rank agents
+            scored_agents = self._score_agents(available_agents, task_req)
+            
+            # Select optimal number based on task complexity
+            optimal_count = self._get_optimal_agent_count(task_req.complexity, current_active)
+            selected_count = min(optimal_count, len(scored_agents))
+            
+            # Select top agents
+            selected_agents = [agent_id for agent_id, score in scored_agents[:selected_count]]
+            
+            # Mark agents as busy
+            for agent_id in selected_agents:
+                self.agents[agent_id].status = AshigaruStatus.BUSY
+                self.agents[agent_id].current_task = task_req.task_id
+                self.agents[agent_id].last_used = datetime.now()
+            
+            # Track task assignment
+            self.active_tasks[task_req.task_id] = set(selected_agents)
+            
+            # Update statistics
+            selection_time = (datetime.now() - start_time).total_seconds()
+            self.stats["selections_made"] += 1
+            self.stats["avg_selection_time"] = (
+                self.stats["avg_selection_time"] * (self.stats["selections_made"] - 1) + selection_time
+            ) / self.stats["selections_made"]
+            
+            logger.info("[è¶³è»½é¸æŠœ] é¸æŠœå®Œäº†: %s -> %s (%.2fs)", 
+                       task_req.task_id, [self.agents[aid].name for aid in selected_agents], selection_time)
+            
+            return selected_agents
+            
+        except Exception as e:
+            logger.error("[è¶³è»½é¸æŠœ] é¸æŠœã‚¨ãƒ©ãƒ¼: %s", e)
+            return []
+    
+    def _get_available_agents(self, exclude_agents: Optional[Set[int]] = None) -> List[int]:
+        """Get list of available agent IDs."""
+        available = []
+        exclude = exclude_agents or set()
+        
+        for agent_id, agent in self.agents.items():
+            if (agent_id not in exclude and 
+                agent.status == AshigaruStatus.AVAILABLE and
+                agent.current_load < 0.9):  # Don't overload agents
+                available.append(agent_id)
+        
+        return available
+    
+    def _score_agents(
+        self, 
+        agent_ids: List[int], 
+        task_req: TaskRequirement
+    ) -> List[Tuple[int, float]]:
+        """Score agents based on task requirements."""
+        scored = []
+        
+        for agent_id in agent_ids:
+            agent = self.agents[agent_id]
+            score = 0.0
+            
+            # Capability match score (40%)
+            capability_score = self._calculate_capability_score(agent, task_req.required_capabilities)
+            score += capability_score * 0.4
+            
+            # Performance score (30%)
+            performance_score = agent.success_rate * (1.0 - agent.current_load)
+            score += performance_score * 0.3
+            
+            # Availability score (20%)
+            # Prefer agents that haven't been used recently
+            recency_score = 1.0
+            if agent.last_used:
+                hours_since = (datetime.now() - agent.last_used).total_seconds() / 3600
+                recency_score = min(1.0, hours_since / 24.0)  # Full score after 24h
+            score += recency_score * 0.2
+            
+            # Priority boost (10%)
+            if task_req.preferred_agents and agent.name in task_req.preferred_agents:
+                score += 0.1
+            
+            scored.append((agent_id, score))
+        
+        # Sort by score (descending)
+        return sorted(scored, key=lambda x: x[1], reverse=True)
+    
+    def _calculate_capability_score(self, agent: AshigaruAgent, required_caps: List[str]) -> float:
+        """Calculate how well agent capabilities match requirements."""
+        if not required_caps:
+            return 1.0  # No specific requirements
+        
+        agent_caps = set(agent.capabilities)
+        required_caps_set = set(required_caps)
+        
+        # Calculate intersection over union
+        intersection = len(agent_caps.intersection(required_caps_set))
+        union = len(agent_caps.union(required_caps_set))
+        
+        return intersection / max(union, 1)
+    
+    def _get_optimal_agent_count(self, complexity: TaskComplexity, current_active: int) -> int:
+        """Determine optimal number of agents for task complexity."""
+        remaining_slots = self.max_active - current_active
+        
+        optimal_counts = {
+            TaskComplexity.SIMPLE: 1,
+            TaskComplexity.MODERATE: 2,
+            TaskComplexity.COMPLEX: 3,
+            TaskComplexity.CRITICAL: 4,
+        }
+        
+        return min(optimal_counts.get(complexity, 1), remaining_slots)
+    
+    def _count_active_agents(self) -> int:
+        """Count currently active (busy) agents."""
+        return sum(1 for agent in self.agents.values() if agent.status == AshigaruStatus.BUSY)
+    
+    async def complete_task(
+        self, 
+        task_id: str, 
+        success: bool = True,
+        performance_metrics: Optional[Dict] = None
+    ) -> None:
+        """Mark task as complete and free up agents."""
+        if task_id not in self.active_tasks:
+            logger.warning("[è¶³è»½é¸æŠœ] ä¸æ˜ãªã‚¿ã‚¹ã‚¯å®Œäº†: %s", task_id)
+            return
+        
+        agent_ids = self.active_tasks[task_id]
+        
+        for agent_id in agent_ids:
+            if agent_id in self.agents:
+                agent = self.agents[agent_id]
+                agent.status = AshigaruStatus.AVAILABLE
+                agent.current_task = None
+                agent.current_load = 0.0
+                
+                # Update performance metrics
+                if performance_metrics and agent_id in performance_metrics:
+                    metrics = performance_metrics[agent_id]
+                    
+                    # Update success rate (exponential moving average)
+                    task_success = 1.0 if success else 0.0
+                    agent.success_rate = agent.success_rate * 0.9 + task_success * 0.1
+                    
+                    # Update response time
+                    if "response_time" in metrics:
+                        agent.avg_response_time = (
+                            agent.avg_response_time * 0.8 + metrics["response_time"] * 0.2
+                        )
+                    
+                    # Update memory usage
+                    if "memory_usage" in metrics:
+                        agent.memory_usage_mb = metrics["memory_usage"]
+        
+        # Remove from active tasks
+        del self.active_tasks[task_id]
+        
+        # Update statistics
+        if success:
+            self.stats["tasks_completed"] += 1
+        else:
+            self.stats["tasks_failed"] += 1
+        
+        logger.info("[è¶³è»½é¸æŠœ] ã‚¿ã‚¹ã‚¯å®Œäº†: %s (%s) - è¶³è»½è§£æ”¾: %då", 
+                   task_id, "æˆåŠŸ" if success else "å¤±æ•—", len(agent_ids))
+        
+        # Process queued tasks
+        await self._process_task_queue()
+    
+    async def handle_agent_failure(self, agent_id: int, task_id: str) -> List[int]:
+        """Handle agent failure and select replacement if needed."""
+        if agent_id not in self.agents:
+            return []
+        
+        agent = self.agents[agent_id]
+        agent.status = AshigaruStatus.ERROR
+        agent.current_task = None
+        agent.current_load = 0.0
+        agent.success_rate *= 0.8  # Reduce success rate
+        
+        logger.warning("[è¶³è»½é¸æŠœ] è¶³è»½éšœå®³: %s (ã‚¿ã‚¹ã‚¯: %s)", agent.name, task_id)
+        
+        # Update active task assignment
+        if task_id in self.active_tasks:
+            self.active_tasks[task_id].discard(agent_id)
+            
+            # Select replacement if task is still active
+            if self.active_tasks[task_id]:  # Other agents still working
+                # Find task requirement (would need to be stored)
+                # For now, just return empty list
+                self.stats["failover_count"] += 1
+                
+        return []
+    
+    async def _process_task_queue(self) -> None:
+        """Process queued tasks if agents become available."""
+        if not self.task_queue:
+            return
+        
+        current_active = self._count_active_agents()
+        if current_active >= self.max_active:
+            return
+        
+        # Try to process queued tasks
+        processed = []
+        for i, task_req in enumerate(self.task_queue):
+            selected = await self.select_ashigaru(task_req)
+            if selected:
+                processed.append(i)
+                logger.info("[è¶³è»½é¸æŠœ] ã‚­ãƒ¥ãƒ¼ã‹ã‚‰ã‚¿ã‚¹ã‚¯å®Ÿè¡Œ: %s", task_req.task_id)
+        
+        # Remove processed tasks from queue (reverse order to maintain indices)
+        for i in reversed(processed):
+            del self.task_queue[i]
+    
+    def rebalance_load(self) -> None:
+        """Rebalance load across available agents."""
+        # Simple rebalancing: reset overloaded agents to maintenance
+        for agent in self.agents.values():
+            if agent.current_load > 0.95 and agent.status == AshigaruStatus.AVAILABLE:
+                agent.status = AshigaruStatus.MAINTENANCE
+                logger.info("[è¶³è»½é¸æŠœ] è² è·è»½æ¸›ã®ãŸã‚ä¸€æ™‚ä¼‘æ­¢: %s", agent.name)
+            elif agent.current_load < 0.1 and agent.status == AshigaruStatus.MAINTENANCE:
+                agent.status = AshigaruStatus.AVAILABLE
+                logger.info("[è¶³è»½é¸æŠœ] ä¼‘æ­¢è§£é™¤: %s", agent.name)
+        
+        self.stats["load_balanced_count"] += 1
+    
+    def get_agent_status(self, agent_id: int) -> Optional[Dict]:
+        """Get detailed status of specific agent."""
+        if agent_id not in self.agents:
+            return None
+        
+        agent = self.agents[agent_id]
+        return {
+            "id": agent.id,
+            "name": agent.name,
+            "role": agent.role,
+            "status": agent.status.value,
+            "current_load": agent.current_load,
+            "success_rate": agent.success_rate,
+            "avg_response_time": agent.avg_response_time,
+            "memory_usage_mb": agent.memory_usage_mb,
+            "current_task": agent.current_task,
+            "capabilities": agent.capabilities,
+            "last_used": agent.last_used.isoformat() if agent.last_used else None,
+        }
+    
+    def get_system_status(self) -> Dict[str, Any]:
+        """Get overall system status."""
+        status_counts = {}
+        total_load = 0.0
+        
+        for agent in self.agents.values():
+            status = agent.status.value
+            status_counts[status] = status_counts.get(status, 0) + 1
+            total_load += agent.current_load
+        
+        return {
+            "total_agents": len(self.agents),
+            "max_active": self.max_active,
+            "currently_active": self._count_active_agents(),
+            "queued_tasks": len(self.task_queue),
+            "status_breakdown": status_counts,
+            "average_load": total_load / len(self.agents) if self.agents else 0.0,
+            "stats": dict(self.stats),
+        }
+    
+    def show_status(self) -> str:
+        """Format system status for display."""
+        status = self.get_system_status()
+        
+        lines = [
+            "=" * 60,
+            "âš”ï¸  è¶³è»½é¸æŠœã‚·ã‚¹ãƒ†ãƒ  çŠ¶æ³å ±å‘Š",
+            "=" * 60,
+            f"ç·è¶³è»½æ•°: {status['total_agents']}å",
+            f"æœ€å¤§åŒæ™‚å®Ÿè¡Œ: {status['max_active']}å",
+            f"ç¾åœ¨æ´»å‹•ä¸­: {status['currently_active']}å",
+            f"å¾…æ©Ÿä¸­ã‚¿ã‚¹ã‚¯: {status['queued_tasks']}ä»¶",
+            f"å¹³å‡è² è·: {status['average_load']:.1%}",
+            "",
+            "çŠ¶æ…‹åˆ¥å†…è¨³:",
+        ]
+        
+        for status_name, count in status["status_breakdown"].items():
+            emoji = {"available": "âœ…", "busy": "ğŸ”¥", "maintenance": "ğŸ”§", "error": "âŒ", "offline": "âš«"}.get(status_name, "â“")
+            lines.append(f"  {emoji} {status_name}: {count}å")
+        
+        lines.extend([
+            "",
+            "çµ±è¨ˆæƒ…å ±:",
+            f"  é¸æŠœå®Ÿè¡Œ: {self.stats['selections_made']}å›",
+            f"  ã‚¿ã‚¹ã‚¯å®Œäº†: {self.stats['tasks_completed']}ä»¶",
+            f"  ã‚¿ã‚¹ã‚¯å¤±æ•—: {self.stats['tasks_failed']}ä»¶",
+            f"  å¹³å‡é¸æŠœæ™‚é–“: {self.stats['avg_selection_time']:.3f}ç§’",
+            f"  è² è·åˆ†æ•£å®Ÿè¡Œ: {self.stats['load_balanced_count']}å›",
+            f"  éšœå®³æ™‚åˆ‡ã‚Šæ›¿ãˆ: {self.stats['failover_count']}å›",
+            "=" * 60,
+        ])
+        
+        return "\n".join(lines)
+
+
+# Utility functions
+def create_task_requirement(
+    task_id: str,
+    task_description: str,
+    complexity: TaskComplexity = TaskComplexity.MODERATE,
+    required_capabilities: List[str] = None
+) -> TaskRequirement:
+    """Create a task requirement from description."""
+    # Simple capability inference from description
+    capabilities = required_capabilities or []
+    
+    if not capabilities:
+        # Infer capabilities from task description
+        description_lower = task_description.lower()
+        
+        if any(word in description_lower for word in ["ãƒ•ã‚¡ã‚¤ãƒ«", "file", "èª­ã¿", "æ›¸ã"]):
+            capabilities.append("file_management")
+        if any(word in description_lower for word in ["git", "github", "commit"]):
+            capabilities.append("git")
+        if any(word in description_lower for word in ["web", "http", "api", "fetch"]):
+            capabilities.append("web_fetch")
+        if any(word in description_lower for word in ["æ¤œç´¢", "search"]):
+            capabilities.append("search")
+        if any(word in description_lower for word in ["ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹", "database", "sql"]):
+            capabilities.append("database")
+        
+        # Default to general if nothing specific found
+        if not capabilities:
+            capabilities = ["general"]
+    
+    return TaskRequirement(
+        task_id=task_id,
+        complexity=complexity,
+        required_capabilities=capabilities,
+    )
\ No newline at end of file
diff --git a/core/complexity.py b/core/complexity.py
new file mode 100644
index 0000000..9923bd1
--- /dev/null
+++ b/core/complexity.py
@@ -0,0 +1,85 @@
+"""Complexity Estimator - ã‚¿ã‚¹ã‚¯è¤‡é›‘åº¦åˆ¤å®šã‚¨ãƒ³ã‚¸ãƒ³
+
+simple   â†’ ä¾å¤§å°†ã®ã¿ (Â¥0)
+medium   â†’ ä¾å¤§å°†ã®ã¿ (Â¥0)
+complex  â†’ ä¾å¤§å°†åˆ†æ â†’ å°†è»å®Ÿè£… (Â¥280)
+strategic â†’ å®¶è€æ±ºè£ (Â¥1,350)
+"""
+
+import re
+from shogun.core.task_queue import Complexity
+
+
+# Keyword patterns for complexity estimation
+STRATEGIC_KEYWORDS = [
+    "å„ªå…ˆé †ä½", "äºˆç®—", "æˆ¦ç•¥", "æ–¹é‡", "ãƒ“ã‚¸ãƒã‚¹", "å¸‚å ´",
+    "ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£å…¨ä½“", "ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ–¹é‡", "ãƒ­ãƒ¼ãƒ‰ãƒãƒƒãƒ—",
+    "priority", "budget", "strategy", "architecture decision",
+]
+
+COMPLEX_KEYWORDS = [
+    "çµ±åˆ", "10ãƒ•ã‚¡ã‚¤ãƒ«", "å¤–éƒ¨api", "å¤§è¦æ¨¡", "ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°",
+    "å…¨ä½“ãƒªãƒ•ã‚¡ã‚¯ã‚¿", "æ–°æ©Ÿèƒ½è¿½åŠ ", "spotifyçµ±åˆ", "èªè¨¼",
+    "ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ç§»è¡Œ", "ãƒãƒ«ãƒã‚¹ãƒ¬ãƒƒãƒ‰", "ä¸¦è¡Œå‡¦ç†",
+    "integration", "refactor", "multi-file", "authentication",
+    "database migration", "concurrent",
+]
+
+MEDIUM_KEYWORDS = [
+    "3ãƒ•ã‚¡ã‚¤ãƒ«", "4ãƒ•ã‚¡ã‚¤ãƒ«", "5ãƒ•ã‚¡ã‚¤ãƒ«", "å®Ÿè£…",
+    "ãƒ†ã‚¹ãƒˆä½œæˆ", "ãƒã‚°ä¿®æ­£", "ãƒ‰ãƒ©ã‚¤ãƒ", "é–¢æ•°è¿½åŠ ",
+    "implement", "test", "fix", "driver", "add function",
+]
+
+# Heuristic: count actionable items (e.g., numbered lists, bullet points)
+ACTION_ITEM_PATTERN = re.compile(r"^\s*[\d\-\*ãƒ»]\s*", re.MULTILINE)
+
+
+def estimate_complexity(task: str) -> Complexity:
+    """Estimate task complexity from the prompt text.
+
+    Escalation order:
+        simple â†’ medium â†’ complex â†’ strategic
+
+    Returns:
+        Complexity enum value.
+    """
+    task_lower = task.lower()
+    task_combined = task + task_lower
+
+    # Strategic check
+    if any(kw in task_combined for kw in STRATEGIC_KEYWORDS):
+        return Complexity.STRATEGIC
+
+    # Complex check
+    if any(kw in task_combined for kw in COMPLEX_KEYWORDS):
+        return Complexity.COMPLEX
+
+    # Count action items (more items = more complex)
+    action_items = len(ACTION_ITEM_PATTERN.findall(task))
+    if action_items >= 5:
+        return Complexity.COMPLEX
+
+    # Medium check
+    if any(kw in task_combined for kw in MEDIUM_KEYWORDS):
+        return Complexity.MEDIUM
+
+    if action_items >= 3:
+        return Complexity.MEDIUM
+
+    # Length heuristic
+    if len(task) > 500:
+        return Complexity.MEDIUM
+
+    return Complexity.SIMPLE
+
+
+def estimated_cost_yen(complexity: Complexity) -> int:
+    """Estimate API cost in yen for a given complexity."""
+    costs = {
+        Complexity.SIMPLE: 0,       # ä¾å¤§å°†ã®ã¿
+        Complexity.MEDIUM: 0,       # ä¾å¤§å°†ã®ã¿
+        Complexity.COMPLEX: 280,    # å°†è» Sonnet
+        Complexity.STRATEGIC: 1350, # å®¶è€ Opus
+    }
+    return costs.get(complexity, 0)
diff --git a/core/controller.py b/core/controller.py
new file mode 100644
index 0000000..254bc12
--- /dev/null
+++ b/core/controller.py
@@ -0,0 +1,548 @@
+"""Controller - æœ¬é™£ (çµ±åˆåˆ¶å¾¡)
+
+å¤§éšŠãƒ¢ãƒ¼ãƒ‰ / ä¸­éšŠãƒ¢ãƒ¼ãƒ‰ ã®åˆ‡æ›¿ã¨ã€ã‚¿ã‚¹ã‚¯ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã‚’åˆ¶å¾¡ã™ã‚‹ã€‚
+
+å‡¦ç†ãƒ•ãƒ­ãƒ¼:
+  1. ã‚¿ã‚¹ã‚¯å—ä¿¡ â†’ è¤‡é›‘åº¦åˆ¤å®š
+  2. ãƒ¢ãƒ¼ãƒ‰ã«å¿œã˜ãŸãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°
+     - ä¸­éšŠ: ä¾å¤§å°† + è¶³è»½(MCP) ã®ã¿ (Â¥0)
+     - å¤§éšŠ: è¤‡é›‘åº¦ã«å¿œã˜ã¦ ä¾å¤§å°† â†’ å®¶è€ â†’ å°†è»
+
+ã‚¯ãƒ©ã‚¦ãƒ‰ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®å®Ÿè¡Œ:
+  Primary: claude-cli (Proç‰ˆ, npm)
+  Fallback: Anthropic API (console.anthropic.com, èª²é‡‘)
+"""
+
+import asyncio
+import logging
+import os
+import subprocess
+from datetime import datetime
+from pathlib import Path
+from typing import Any
+
+import yaml
+
+from shogun.core.task_queue import (
+    Task, TaskQueue, TaskStatus, Complexity, DeploymentMode,
+)
+from shogun.core.complexity import estimate_complexity, estimated_cost_yen
+from shogun.core.escalation import (
+    get_handler, get_next_escalation, should_escalate,
+    can_handle_in_company_mode, build_escalation_context,
+    build_taisho_analysis_prompt, AGENT_COST,
+)
+from shogun.core.dashboard import Dashboard
+from shogun.core.mcp_manager import MCPManager
+from shogun.providers.claude_cli import ClaudeCLIProvider, CLIResult
+from shogun.providers.openvino_client import OpenVINOClient
+
+logger = logging.getLogger("shogun.controller")
+
+
+def _load_config(path: str) -> dict:
+    with open(path, encoding="utf-8") as f:
+        return yaml.safe_load(f)
+
+
+class Controller:
+    """æœ¬é™£: Central controller for the Shogun system.
+
+    Coordinates all layers:
+      - Cloud: claude-cli (Pro) â†’ Anthropic API (fallback)
+      - Local: OpenVINO R1 (ä¾å¤§å°†)
+      - Tools: MCP servers Ã— 8 (è¶³è»½)
+    """
+
+    def __init__(self, base_dir: str, config_path: str | None = None):
+        self.base_dir = Path(base_dir)
+
+        # Load config
+        cfg_path = config_path or str(self.base_dir / "config" / "settings.yaml")
+        self.config = _load_config(cfg_path) if Path(cfg_path).exists() else {}
+
+        # Current deployment mode
+        self.current_mode = DeploymentMode.BATTALION
+
+        # Task queue
+        self.queue = TaskQueue(str(self.base_dir))
+
+        # Dashboard
+        self.dashboard = Dashboard(str(self.base_dir))
+
+        # MCP Manager (è¶³è»½)
+        mcp_config = str(self.base_dir / "config" / "mcp_config.json")
+        self.mcp = MCPManager(mcp_config if Path(mcp_config).exists() else None)
+
+        # Providers
+        self.claude_cli = ClaudeCLIProvider()
+
+        r1_url = self.config.get("taisho", {}).get(
+            "url", "http://192.168.1.11:11434"
+        )
+        self.openvino = OpenVINOClient(base_url=r1_url)
+
+        # API fallback (lazy init)
+        self._api_provider = None
+
+        # Stats
+        self.stats = {
+            "taisho_r1": 0,
+            "karo_sonnet": 0,
+            "shogun_opus": 0,
+            "battalion": 0,
+            "company": 0,
+            "api_fallback": 0,
+            "total_cost_yen": 0,
+        }
+
+        # Repo path for git sync
+        # GitHub username/ ä»¥é™ã¨ /home/claude/ ä»¥é™ã‚’åŒæœŸ
+        repo_cfg = self.config.get("repo", {})
+        self.repo_local_base = repo_cfg.get("local_base", "/home/claude")
+        self.repo_path = self._detect_repo_path()
+
+    @property
+    def api_provider(self):
+        """Lazy-init Anthropic API provider."""
+        if self._api_provider is None:
+            api_key = os.environ.get("ANTHROPIC_API_KEY", "")
+            if api_key:
+                from shogun.providers.anthropic_api import AnthropicAPIProvider
+                self._api_provider = AnthropicAPIProvider(api_key=api_key)
+        return self._api_provider
+
+    # â”€â”€â”€ Main Entry Point â”€â”€â”€
+
+    async def process_task(
+        self,
+        prompt: str,
+        mode: str = "battalion",
+        force_agent: str = "",
+    ) -> str:
+        """Process a task through the Shogun system.
+
+        Args:
+            prompt: Task description.
+            mode: "battalion" or "company".
+            force_agent: Force a specific agent (bypass routing).
+
+        Returns:
+            Result text.
+        """
+        # Create task
+        deploy_mode = DeploymentMode(mode)
+        task = Task(prompt=prompt, mode=deploy_mode)
+        task.complexity = estimate_complexity(prompt)
+        self.queue.enqueue(task)
+
+        logger.info(
+            "[æœ¬é™£] ä»»å‹™å—é ˜: %s (è¤‡é›‘åº¦: %s, ç·¨æˆ: %s)",
+            task.id, task.complexity.value, deploy_mode.value,
+        )
+
+        # Sync repo
+        await self._sync_repo()
+
+        # Dashboard update
+        self.dashboard.add_in_progress(
+            f"[{task.id}] {prompt[:50]}... ({deploy_mode.value})"
+        )
+
+        try:
+            if force_agent:
+                result = await self._dispatch_to_agent(task, force_agent)
+            elif deploy_mode == DeploymentMode.COMPANY:
+                result = await self._process_company(task)
+            else:
+                result = await self._process_battalion(task)
+
+            # Complete
+            self.queue.complete_task(task.id, result, task.cost_yen)
+            self.dashboard.remove_in_progress(
+                f"[{task.id}] {prompt[:50]}... ({deploy_mode.value})"
+            )
+            self.dashboard.add_completed(
+                task.id, prompt[:60], "å®Œäº†", task.cost_yen
+            )
+            return result
+
+        except Exception as e:
+            task.status = TaskStatus.FAILED
+            task.error = str(e)
+            self.queue.update_task(task)
+            self.dashboard.remove_in_progress(
+                f"[{task.id}] {prompt[:50]}... ({deploy_mode.value})"
+            )
+            self.dashboard.add_action_required(
+                f"[{task.id}] å¤±æ•—: {str(e)[:80]}"
+            )
+            raise
+
+    # â”€â”€â”€ Company Mode (ä¸­éšŠ) â”€â”€â”€
+
+    async def _process_company(self, task: Task) -> str:
+        """ä¸­éšŠãƒ¢ãƒ¼ãƒ‰: ä¾å¤§å°† + è¶³è»½(MCP) ã®ã¿ã€‚APIä¸ä½¿ç”¨ (Â¥0)ã€‚"""
+        self.stats["company"] += 1
+        logger.info("[ä¸­éšŠ] å‡ºé™£: ä¾å¤§å°† + è¶³è»½ Ã— 8")
+
+        # Check if task is within company capabilities
+        if not can_handle_in_company_mode(task.complexity):
+            logger.warning(
+                "[ä¸­éšŠ] è¤‡é›‘åº¦ %s ã¯ä¸­éšŠã®èƒ½åŠ›ç¯„å›²å¤–ã€‚å¤§éšŠãƒ¢ãƒ¼ãƒ‰æ¨å¥¨ã€‚",
+                task.complexity.value,
+            )
+            # Still try with Taisho, but add warning
+            task.context["warning"] = "å¤§éšŠãƒ¢ãƒ¼ãƒ‰æ¨å¥¨ï¼ˆèƒ½åŠ›è¶…éã®å¯èƒ½æ€§ï¼‰"
+
+        result = await self._call_taisho(task, company_mode=True)
+        task.cost_yen = 0
+
+        # Check if Taisho recommends battalion escalation
+        if "å¤§éšŠãƒ¢ãƒ¼ãƒ‰æ¨å¥¨" in result:
+            self.dashboard.add_action_required(
+                f"[{task.id}] ä¸­éšŠèƒ½åŠ›è¶…é: å¤§éšŠãƒ¢ãƒ¼ãƒ‰ã¸ã®åˆ‡æ›¿ã‚’æ¨å¥¨"
+            )
+
+        return result
+
+    # â”€â”€â”€ Battalion Mode (å¤§éšŠ) â”€â”€â”€
+
+    async def _process_battalion(self, task: Task) -> str:
+        """å¤§éšŠãƒ¢ãƒ¼ãƒ‰: è¤‡é›‘åº¦ã«å¿œã˜ã¦ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã€‚"""
+        self.stats["battalion"] += 1
+        logger.info(
+            "[å¤§éšŠ] å‡ºé™£æº–å‚™ (è¤‡é›‘åº¦: %s)", task.complexity.value
+        )
+
+        handler = get_handler(task.complexity)
+
+        if handler == "taisho":
+            # Simple/Medium â†’ ä¾å¤§å°†ã®ã¿ (Â¥0)
+            result = await self._call_taisho(task)
+            task.cost_yen = 0
+            return result
+
+        elif handler == "karo":
+            # Complex â†’ ä¾å¤§å°†åˆ†æ â†’ å®¶è€(Sonnet)ãŒä½œæ¥­å‰²æŒ¯ã‚Š
+            taisho_analysis = await self._call_taisho_analysis(task)
+            task.context["taisho_analysis"] = taisho_analysis
+            result = await self._call_cloud(
+                task, agent="karo", model="sonnet"
+            )
+            task.cost_yen = AGENT_COST["karo"]
+            return result
+
+        else:
+            # Strategic â†’ å°†è»(Opus)ãŒæœ€çµ‚æ±ºè£
+            taisho_analysis = await self._call_taisho_analysis(task)
+            task.context["taisho_analysis"] = taisho_analysis
+            result = await self._call_cloud(
+                task, agent="shogun", model="opus"
+            )
+            task.cost_yen = AGENT_COST["shogun"]
+            return result
+
+    # â”€â”€â”€ Agent Dispatch â”€â”€â”€
+
+    async def _dispatch_to_agent(self, task: Task, agent: str) -> str:
+        """Dispatch to a specific agent (force mode)."""
+        if agent == "taisho":
+            result = await self._call_taisho(task)
+            task.cost_yen = 0
+        elif agent == "karo":
+            result = await self._call_cloud(task, "karo", "sonnet")
+            task.cost_yen = AGENT_COST["karo"]
+        elif agent == "shogun":
+            result = await self._call_cloud(task, "shogun", "opus")
+            task.cost_yen = AGENT_COST["shogun"]
+        else:
+            raise ValueError(f"Unknown agent: {agent}")
+        return result
+
+    # â”€â”€â”€ Taisho (ä¾å¤§å°†) â”€â”€â”€
+
+    async def _call_taisho(self, task: Task, company_mode: bool = False) -> str:
+        """Call ä¾å¤§å°† R1 (OpenVINO)."""
+        self.stats["taisho_r1"] += 1
+        task.assigned_agent = "taisho"
+        task.status = TaskStatus.IN_PROGRESS
+        self.queue.update_task(task)
+
+        mode_label = "ä¸­éšŠ" if company_mode else "å¤§éšŠ"
+        logger.info("[ä¾å¤§å°†] æ¨è«–é–‹å§‹ (%s)", mode_label)
+
+        system = (
+            "ã‚ãªãŸã¯ä¾å¤§å°†ã§ã™ã€‚ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®è¨­è¨ˆãƒ»æ¨è«–æ‹…å½“ã€‚\n"
+            "<think>ã‚¿ã‚°ã§æ—¥æœ¬èªã§æ·±ãæ¨è«–ã—ã€è«–ç†çš„ãªçµè«–ã‚’å°ã„ã¦ãã ã•ã„ã€‚\n"
+        )
+        if company_mode:
+            system += (
+                "ä¸­éšŠãƒ¢ãƒ¼ãƒ‰ã§ã™ã€‚å°†è»ãƒ»å®¶è€ã¯ä¸åœ¨ã€‚ä¾å¤§å°†ã¨è¶³è»½ã®ã¿ã§å®Œçµã—ã¦ãã ã•ã„ã€‚\n"
+                "èƒ½åŠ›ã‚’è¶…ãˆã‚‹å ´åˆã¯ã€Œå¤§éšŠãƒ¢ãƒ¼ãƒ‰æ¨å¥¨ã€ã¨å ±å‘Šã—ã¦ãã ã•ã„ã€‚\n"
+            )
+
+        context_str = ""
+        if task.context:
+            context_str = "\n".join(
+                f"[{k}]: {v}" for k, v in task.context.items()
+            )
+            context_str = f"\n## ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ\n{context_str}\n"
+
+        prompt = f"{context_str}\n## ä»»å‹™\n{task.prompt}"
+
+        try:
+            result = await self.openvino.generate(
+                prompt=prompt,
+                system=system,
+                max_tokens=2000,
+                temperature=0.6,
+            )
+            logger.info("[ä¾å¤§å°†] å®Œäº† (Â¥0)")
+            return result
+        except Exception as e:
+            logger.error("[ä¾å¤§å°†] ã‚¨ãƒ©ãƒ¼: %s", e)
+            raise
+
+    async def _call_taisho_analysis(self, task: Task) -> str:
+        """Call Taisho for analysis only (before escalation)."""
+        self.stats["taisho_r1"] += 1
+        logger.info("[ä¾å¤§å°†] åˆ†æé–‹å§‹ï¼ˆä¸Šä½ã¸ã®å ±å‘Šç”¨ï¼‰")
+
+        prompt = build_taisho_analysis_prompt(task)
+        try:
+            result = await self.openvino.generate(
+                prompt=prompt,
+                max_tokens=1500,
+                temperature=0.6,
+            )
+            logger.info("[ä¾å¤§å°†] åˆ†æå®Œäº†")
+            return result
+        except Exception as e:
+            logger.warning("[ä¾å¤§å°†] åˆ†æå¤±æ•—: %s (ã‚¹ã‚­ãƒƒãƒ—)", e)
+            return f"(ä¾å¤§å°†åˆ†æã‚¹ã‚­ãƒƒãƒ—: {e})"
+
+    # â”€â”€â”€ Cloud Agents (å®¶è€/å°†è») â”€â”€â”€
+
+    async def _call_cloud(
+        self, task: Task, agent: str, model: str,
+    ) -> str:
+        """Call cloud agent (claude-cli â†’ API fallback).
+
+        Flow:
+          1. Try claude-cli (Proç‰ˆ)
+          2. If rate limited â†’ fallback to Anthropic API
+        """
+        task.assigned_agent = agent
+        task.status = TaskStatus.IN_PROGRESS
+        self.queue.update_task(task)
+
+        agent_label = "å°†è»" if agent == "shogun" else "å®¶è€"
+        logger.info("[%s] %s å®Ÿè¡Œé–‹å§‹ (model=%s)", agent_label, agent, model)
+
+        # Build prompt with context
+        parts = []
+        if task.context.get("taisho_analysis"):
+            parts.append(f"## ä¾å¤§å°†ã®åˆ†æ\n{task.context['taisho_analysis']}")
+        if task.context.get("escalation"):
+            parts.append(task.context["escalation"])
+
+        system = self._get_system_prompt(agent)
+        full_prompt = "\n\n".join(parts + [f"## ä»»å‹™\n{task.prompt}"])
+
+        # Try claude-cli first (Proç‰ˆ)
+        result = await self.claude_cli.generate(
+            prompt=full_prompt,
+            model=model,
+            system_prompt=system,
+            cwd=self.repo_path,
+        )
+
+        if result.success:
+            cost = AGENT_COST.get(agent, 0)
+            self.stats["total_cost_yen"] += cost
+            if agent == "karo":
+                self.stats["karo_sonnet"] += 1
+            else:
+                self.stats["shogun_opus"] += 1
+            logger.info(
+                "[%s] å®Œäº† (claude-cli, Â¥%d)", agent_label, cost
+            )
+            return result.text
+
+        # Rate limited â†’ API fallback
+        if result.rate_limited:
+            logger.warning(
+                "[%s] Proç‰ˆåˆ¶é™ã€‚APIãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã€‚", agent_label
+            )
+            return await self._call_cloud_api(task, agent, model, full_prompt, system)
+
+        # Other error â†’ try API fallback anyway
+        logger.warning(
+            "[%s] CLI error: %s â†’ APIãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯", agent_label, result.error
+        )
+        return await self._call_cloud_api(task, agent, model, full_prompt, system)
+
+    async def _call_cloud_api(
+        self, task: Task, agent: str, model: str,
+        prompt: str, system: str,
+    ) -> str:
+        """Fallback to Anthropic API (console.anthropic.com)."""
+        self.stats["api_fallback"] += 1
+        agent_label = "å°†è»" if agent == "shogun" else "å®¶è€"
+
+        if not self.api_provider:
+            raise RuntimeError(
+                f"[{agent_label}] APIãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ä¸å¯: ANTHROPIC_API_KEY æœªè¨­å®š"
+            )
+
+        logger.info("[%s] APIå®Ÿè¡Œ (èª²é‡‘)", agent_label)
+        text = await self.api_provider.generate(
+            prompt=prompt,
+            model=model,
+            system=system,
+            max_tokens=4096,
+            temperature=0.3,
+        )
+
+        cost = AGENT_COST.get(agent, 0)
+        self.stats["total_cost_yen"] += cost
+        if agent == "karo":
+            self.stats["karo_sonnet"] += 1
+        else:
+            self.stats["shogun_opus"] += 1
+        logger.info("[%s] APIå®Œäº† (Â¥%d)", agent_label, cost)
+        return text
+
+    # â”€â”€â”€ System Prompts â”€â”€â”€
+
+    def _get_system_prompt(self, agent: str) -> str:
+        if agent == "shogun":
+            return (
+                "ã‚ãªãŸã¯ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®å°†è»ï¼ˆShogunï¼‰ã§ã™ã€‚\n"
+                "æœ€é«˜æ„æ€æ±ºå®šè€…ã¨ã—ã¦ã€ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå…¨ä½“ã®æˆ¦ç•¥çš„åˆ¤æ–­ã¨æœ€çµ‚æ±ºè£ã‚’è¡Œã„ã¾ã™ã€‚\n"
+                "é…ä¸‹ã®å®¶è€ãƒ»ä¾å¤§å°†ãŒè§£æ±ºã§ããªã‹ã£ãŸé›£å•ãŒã‚¨ã‚¹ã‚«ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã¨ã—ã¦å±Šãã¾ã™ã€‚\n"
+                "å‰ä»»è€…ã®åˆ†æçµæœã‚’è¸ã¾ãˆã¤ã¤ã€æ ¹æœ¬çš„ãªè§£æ±ºç­–ã‚’æç¤ºã—ã¦ãã ã•ã„ã€‚\n"
+            )
+        elif agent == "karo":
+            return (
+                "ã‚ãªãŸã¯ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®å®¶è€ï¼ˆKaroï¼‰ã§ã™ã€‚\n"
+                "å°†è»ã®å³è…•ã¨ã—ã¦ã€ä½œæ¥­ã®å‰²æŒ¯ã‚Šã¨é«˜åº¦ãªå®Ÿè£…æ–¹é‡ã®ç­–å®šã‚’è¡Œã„ã¾ã™ã€‚\n"
+                "ä¾å¤§å°†ã‹ã‚‰ã®åˆ†æçµæœã‚’å—ã‘ã€å…·ä½“çš„ãªã‚³ãƒ¼ãƒ‰å¤‰æ›´ææ¡ˆã‚’è¡Œã£ã¦ãã ã•ã„ã€‚\n"
+                "è¤‡é›‘ãªçµ±åˆã‚¿ã‚¹ã‚¯ã‚’åˆ†è§£ã—ã€æ˜ç¢ºãªå®Ÿè£…æ‰‹é †ã‚’ç¤ºã—ã¦ãã ã•ã„ã€‚\n"
+            )
+        return ""
+
+    # â”€â”€â”€ Git Sync â”€â”€â”€
+
+    def _detect_repo_path(self) -> str:
+        """Detect local repo path from git remote URL.
+
+        Maps: github.com/{user}/{repo} â†’ {local_base}/{repo}
+        """
+        base = self.repo_local_base
+        # Try to read git remote from current directory
+        try:
+            import subprocess
+            result = subprocess.run(
+                ["git", "remote", "get-url", "origin"],
+                capture_output=True, text=True, timeout=5,
+                cwd=str(self.base_dir),
+            )
+            if result.returncode == 0:
+                url = result.stdout.strip()
+                # Extract repo name from URL
+                # https://github.com/user/repo.git â†’ repo
+                # git@github.com:user/repo.git â†’ repo
+                repo_name = url.rstrip("/").rstrip(".git").rsplit("/", 1)[-1]
+                if repo_name:
+                    path = f"{base}/{repo_name}"
+                    logger.info("[æœ¬é™£] ãƒªãƒã‚¸ãƒˆãƒªæ¤œå‡º: %s â†’ %s", url, path)
+                    return path
+        except Exception:
+            pass
+
+        # Fallback: use base dir directly
+        logger.info("[æœ¬é™£] ãƒªãƒã‚¸ãƒˆãƒªãƒ‘ã‚¹: %s", base)
+        return base
+
+    async def _sync_repo(self) -> None:
+        """Git sync (ãƒªãƒã‚¸ãƒˆãƒªåŒæœŸ)."""
+        sync_script = str(self.base_dir / "setup" / "auto_sync.sh")
+        if not Path(sync_script).exists():
+            # Fallback: direct git pull
+            if Path(self.repo_path).exists():
+                try:
+                    proc = await asyncio.create_subprocess_exec(
+                        "git", "-C", self.repo_path, "pull", "--rebase",
+                        stdout=asyncio.subprocess.PIPE,
+                        stderr=asyncio.subprocess.PIPE,
+                    )
+                    await asyncio.wait_for(proc.communicate(), timeout=30)
+                    logger.info("[æœ¬é™£] GitåŒæœŸå®Œäº†")
+                except Exception as e:
+                    logger.warning("[æœ¬é™£] GitåŒæœŸã‚¹ã‚­ãƒƒãƒ—: %s", e)
+            return
+
+        try:
+            proc = await asyncio.create_subprocess_exec(
+                "bash", sync_script,
+                stdout=asyncio.subprocess.PIPE,
+                stderr=asyncio.subprocess.PIPE,
+            )
+            await asyncio.wait_for(proc.communicate(), timeout=30)
+            logger.info("[æœ¬é™£] GitåŒæœŸå®Œäº†")
+        except Exception as e:
+            logger.warning("[æœ¬é™£] GitåŒæœŸã‚¹ã‚­ãƒƒãƒ—: %s", e)
+
+    # â”€â”€â”€ Status â”€â”€â”€
+
+    def get_status(self) -> dict:
+        return {
+            "mode": self.current_mode.value,
+            "stats": dict(self.stats),
+            "pending_tasks": len(self.queue.get_pending()),
+            "total_tasks": len(self.queue.get_all_tasks()),
+            "dashboard": self.dashboard.get_summary(),
+            "mcp_servers": self.mcp.get_status(),
+        }
+
+    def show_stats(self) -> str:
+        """Format stats for display."""
+        s = self.stats
+        lines = [
+            "=" * 50,
+            "ğŸ“Š æˆ¦æœçµ±è¨ˆ",
+            "=" * 50,
+            f"å¤§éšŠãƒ¢ãƒ¼ãƒ‰: {s['battalion']}å›",
+            f"ä¸­éšŠãƒ¢ãƒ¼ãƒ‰: {s['company']}å›",
+            "",
+            "å†…è¨³:",
+            f"  ä¾å¤§å°†R1:  {s['taisho_r1']}å› (Â¥0)",
+            f"  å®¶è€Sonnet: {s['karo_sonnet']}å› (Â¥{s['karo_sonnet'] * 280:,})",
+            f"  å°†è»Opus:  {s['shogun_opus']}å› (Â¥{s['shogun_opus'] * 1350:,})",
+            f"  APIãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: {s['api_fallback']}å›",
+            "",
+            f"åˆè¨ˆã‚³ã‚¹ãƒˆ: Â¥{s['total_cost_yen']:,}",
+            "=" * 50,
+        ]
+        return "\n".join(lines)
+
+    # â”€â”€â”€ Lifecycle â”€â”€â”€
+
+    async def startup(self) -> None:
+        """System startup."""
+        logger.info("[æœ¬é™£] å°†è»ã‚·ã‚¹ãƒ†ãƒ èµ·å‹•...")
+        self.queue.reset_all_workers()
+        self.dashboard.init()
+        self.queue.load_from_disk()
+        logger.info("[æœ¬é™£] èµ·å‹•å®Œäº†")
+
+    async def shutdown(self) -> None:
+        """System shutdown."""
+        logger.info("[æœ¬é™£] å°†è»ã‚·ã‚¹ãƒ†ãƒ åœæ­¢...")
+        await self.mcp.stop_all()
+        await self.openvino.close()
+        if self._api_provider:
+            await self._api_provider.close()
+        logger.info("[æœ¬é™£] åœæ­¢å®Œäº†")
diff --git a/core/dashboard.py b/core/dashboard.py
new file mode 100644
index 0000000..d950677
--- /dev/null
+++ b/core/dashboard.py
@@ -0,0 +1,136 @@
+"""Dashboard - æˆ¦æ³å ±å‘Š (æœ¬å®¶ multi-agent-shogun äº’æ›)
+
+dashboard.md ã‚’ç”Ÿæˆãƒ»æ›´æ–°ã™ã‚‹ã€‚
+æœ¬å®¶ãƒ«ãƒ¼ãƒ«: dashboard ã®æ›´æ–°æ¨©é™ã¯å®¶è€ã®ã¿ï¼ˆæœ¬ã‚·ã‚¹ãƒ†ãƒ ã§ã¯ControllerãŒä»£è¡Œï¼‰ã€‚
+"""
+
+import logging
+from datetime import datetime
+from pathlib import Path
+from typing import Any
+
+logger = logging.getLogger("shogun.dashboard")
+
+
+class Dashboard:
+    """Markdown dashboard generator (dashboard.md)."""
+
+    def __init__(self, base_dir: str):
+        self.path = Path(base_dir) / "status" / "dashboard.md"
+        self.path.parent.mkdir(parents=True, exist_ok=True)
+        self._action_required: list[str] = []
+        self._in_progress: list[str] = []
+        self._completed: list[dict] = []
+        self._skill_candidates: list[dict] = []
+        self._questions: list[str] = []
+
+    def init(self) -> None:
+        """Initialize a fresh dashboard."""
+        self._action_required = []
+        self._in_progress = []
+        self._completed = []
+        self._skill_candidates = []
+        self._questions = []
+        self._write()
+        logger.info("[Dashboard] Initialized: %s", self.path)
+
+    def add_action_required(self, item: str) -> None:
+        self._action_required.append(item)
+        self._write()
+
+    def add_in_progress(self, item: str) -> None:
+        self._in_progress.append(item)
+        self._write()
+
+    def remove_in_progress(self, item: str) -> None:
+        self._in_progress = [x for x in self._in_progress if x != item]
+        self._write()
+
+    def add_completed(self, task_id: str, mission: str, result: str, cost: int = 0) -> None:
+        self._completed.append({
+            "time": datetime.now().strftime("%H:%M"),
+            "task_id": task_id,
+            "mission": mission[:60],
+            "result": result[:80],
+            "cost": cost,
+        })
+        self._write()
+
+    def add_skill_candidate(self, name: str, description: str) -> None:
+        self._skill_candidates.append({"name": name, "description": description})
+        self._write()
+
+    def add_question(self, question: str) -> None:
+        self._questions.append(question)
+        self._write()
+
+    def _write(self) -> None:
+        """Write dashboard.md."""
+        now = datetime.now().strftime("%Y-%m-%d %H:%M")
+        lines = [
+            "# ğŸ“Š æˆ¦æ³å ±å‘Š (Battle Status Report)",
+            f"æœ€çµ‚æ›´æ–°: {now}",
+            "",
+        ]
+
+        # ğŸš¨ è¦å¯¾å¿œ
+        lines.append("## ğŸš¨ è¦å¯¾å¿œ - æ®¿ã®ã”åˆ¤æ–­ã‚’ãŠå¾…ã¡ã—ã¦ãŠã‚Šã¾ã™")
+        if self._action_required:
+            for item in self._action_required:
+                lines.append(f"- {item}")
+        else:
+            lines.append("_ãªã—_")
+        lines.append("")
+
+        # ğŸ”„ é€²è¡Œä¸­
+        lines.append("## ğŸ”„ é€²è¡Œä¸­")
+        if self._in_progress:
+            for item in self._in_progress:
+                lines.append(f"- {item}")
+        else:
+            lines.append("_ãªã—_")
+        lines.append("")
+
+        # âœ… æˆ¦æœ
+        lines.append("## âœ… æœ¬æ—¥ã®æˆ¦æœ")
+        lines.append("| æ™‚åˆ» | ä»»å‹™ID | ä»»å‹™ | çµæœ | ã‚³ã‚¹ãƒˆ |")
+        lines.append("|------|--------|------|------|--------|")
+        if self._completed:
+            for c in self._completed:
+                cost_str = f"Â¥{c['cost']:,}" if c["cost"] > 0 else "Â¥0"
+                lines.append(
+                    f"| {c['time']} | {c['task_id']} | {c['mission']} | {c['result']} | {cost_str} |"
+                )
+        lines.append("")
+
+        # ğŸ¯ ã‚¹ã‚­ãƒ«åŒ–å€™è£œ
+        lines.append("## ğŸ¯ ã‚¹ã‚­ãƒ«åŒ–å€™è£œ")
+        if self._skill_candidates:
+            for sc in self._skill_candidates:
+                lines.append(f"- **{sc['name']}**: {sc['description']}")
+        else:
+            lines.append("_ãªã—_")
+        lines.append("")
+
+        # â“ ä¼ºã„äº‹é …
+        lines.append("## â“ ä¼ºã„äº‹é …")
+        if self._questions:
+            for q in self._questions:
+                lines.append(f"- {q}")
+        else:
+            lines.append("_ãªã—_")
+        lines.append("")
+
+        self.path.write_text("\n".join(lines), encoding="utf-8")
+
+    def get_summary(self) -> dict:
+        """Get dashboard summary as dict."""
+        total_cost = sum(c.get("cost", 0) for c in self._completed)
+        return {
+            "action_required": len(self._action_required),
+            "in_progress": len(self._in_progress),
+            "completed_today": len(self._completed),
+            "total_cost_yen": total_cost,
+            "skill_candidates": len(self._skill_candidates),
+            "questions": len(self._questions),
+        }
diff --git a/core/error_handling.py b/core/error_handling.py
new file mode 100644
index 0000000..317acb5
--- /dev/null
+++ b/core/error_handling.py
@@ -0,0 +1,525 @@
+"""Enhanced Error Handling System for ShogunAI v7.0
+
+Comprehensive error handling, retry policies, circuit breakers, and fallback mechanisms
+to ensure system resilience and graceful degradation.
+"""
+
+import asyncio
+import logging
+import time
+import random
+from dataclasses import dataclass, field
+from enum import Enum
+from typing import Any, Callable, Dict, Optional, Union, List
+from contextlib import asynccontextmanager
+import json
+from pathlib import Path
+
+
+logger = logging.getLogger("shogun.error_handling")
+
+
+class ErrorSeverity(Enum):
+    """Error severity levels."""
+    LOW = "low"
+    MEDIUM = "medium"
+    HIGH = "high"
+    CRITICAL = "critical"
+
+
+class FallbackStrategy(Enum):
+    """Fallback strategies for different failure scenarios."""
+    LOCAL_R1 = "local_r1"
+    SIMPLIFIED_RESPONSE = "simplified_response"
+    SKIP_RECORDING = "skip_recording"
+    LOCAL_CACHE = "local_cache"
+    RETRY_LATER = "retry_later"
+    MANUAL_INTERVENTION = "manual_intervention"
+
+
+@dataclass
+class ErrorContext:
+    """Context information for error handling."""
+    operation: str
+    component: str
+    attempt: int
+    max_attempts: int
+    error_type: str
+    error_message: str
+    timestamp: float = field(default_factory=time.time)
+    metadata: Dict[str, Any] = field(default_factory=dict)
+
+
+@dataclass
+class CircuitBreakerState:
+    """Circuit breaker state tracking."""
+    name: str
+    failures: int = 0
+    last_failure: Optional[float] = None
+    state: str = "CLOSED"  # CLOSED, OPEN, HALF_OPEN
+    last_success: Optional[float] = None
+    half_open_attempts: int = 0
+
+
+class ShogunErrorHandler:
+    """Centralized error handling system for ShogunAI."""
+    
+    def __init__(self, config: Dict[str, Any]):
+        self.config = config.get("error_handling", {})
+        self.retry_config = self.config.get("retry_policy", {})
+        self.timeout_config = self.config.get("timeout_settings", {})
+        self.fallback_config = self.config.get("fallback_behavior", {})
+        self.circuit_config = self.config.get("circuit_breaker", {})
+        
+        # Circuit breakers for different components
+        self.circuit_breakers: Dict[str, CircuitBreakerState] = {}
+        
+        # Error statistics
+        self.error_stats = {
+            "total_errors": 0,
+            "errors_by_component": {},
+            "errors_by_type": {},
+            "recovery_successes": 0,
+            "fallback_activations": 0,
+        }
+        
+        # Initialize circuit breakers
+        self._initialize_circuit_breakers()
+    
+    def _initialize_circuit_breakers(self):
+        """Initialize circuit breakers for key components."""
+        components = [
+            "taisho_r1", "groq_api", "notion_api", "anthropic_api",
+            "slack_api", "github_api", "file_operations"
+        ]
+        
+        for component in components:
+            self.circuit_breakers[component] = CircuitBreakerState(name=component)
+    
+    @asynccontextmanager
+    async def handle_operation(
+        self,
+        operation: str,
+        component: str,
+        timeout: Optional[float] = None,
+        fallback_strategy: Optional[FallbackStrategy] = None,
+        **metadata
+    ):
+        """Context manager for handling operations with comprehensive error handling.
+        
+        Args:
+            operation: Name of the operation
+            component: Component performing the operation
+            timeout: Optional timeout override
+            fallback_strategy: Strategy if operation fails
+            **metadata: Additional context information
+        """
+        start_time = time.time()
+        attempt = 0
+        max_attempts = self.retry_config.get("max_attempts", 3)
+        
+        # Check circuit breaker
+        if not self._check_circuit_breaker(component):
+            raise CircuitBreakerOpenError(
+                f"Circuit breaker OPEN for {component}. Operation blocked."
+            )
+        
+        while attempt < max_attempts:
+            attempt += 1
+            error_context = ErrorContext(
+                operation=operation,
+                component=component,
+                attempt=attempt,
+                max_attempts=max_attempts,
+                error_type="",
+                error_message="",
+                metadata=metadata
+            )
+            
+            try:
+                # Apply timeout if specified
+                operation_timeout = timeout or self.timeout_config.get(
+                    component, self.timeout_config.get("api_request", 60)
+                )
+                
+                async with asyncio.timeout(operation_timeout):
+                    yield error_context
+                    
+                # Success - record and reset circuit breaker
+                self._record_success(component)
+                elapsed = time.time() - start_time
+                logger.info(
+                    f"[ErrorHandler] âœ… {operation} successful in {elapsed:.2f}s "
+                    f"(attempt {attempt}/{max_attempts})"
+                )
+                return
+                
+            except asyncio.TimeoutError as e:
+                error_context.error_type = "timeout"
+                error_context.error_message = f"Operation timed out after {operation_timeout}s"
+                await self._handle_error(error_context, e)
+                
+            except CircuitBreakerOpenError:
+                raise  # Don't retry if circuit breaker is open
+                
+            except Exception as e:
+                error_context.error_type = type(e).__name__
+                error_context.error_message = str(e)
+                await self._handle_error(error_context, e)
+            
+            # Delay before retry (with exponential backoff and jitter)
+            if attempt < max_attempts:
+                delay = self._calculate_retry_delay(attempt)
+                logger.info(f"[ErrorHandler] â³ Retrying in {delay:.1f}s...")
+                await asyncio.sleep(delay)
+        
+        # All attempts failed - apply fallback strategy
+        if fallback_strategy:
+            logger.warning(
+                f"[ErrorHandler] ğŸ”„ All attempts failed, applying fallback: {fallback_strategy.value}"
+            )
+            return await self._apply_fallback_strategy(
+                fallback_strategy, error_context
+            )
+        else:
+            raise OperationFailedError(
+                f"Operation {operation} failed after {max_attempts} attempts"
+            )
+    
+    async def _handle_error(self, context: ErrorContext, exception: Exception):
+        """Handle individual error occurrence."""
+        self.error_stats["total_errors"] += 1
+        
+        # Track by component
+        component_stats = self.error_stats["errors_by_component"]
+        component_stats[context.component] = component_stats.get(context.component, 0) + 1
+        
+        # Track by error type
+        type_stats = self.error_stats["errors_by_type"]
+        type_stats[context.error_type] = type_stats.get(context.error_type, 0) + 1
+        
+        # Determine severity
+        severity = self._determine_error_severity(context, exception)
+        
+        # Log error with appropriate level
+        log_msg = (
+            f"[ErrorHandler] âŒ {context.operation} failed "
+            f"(attempt {context.attempt}/{context.max_attempts}): "
+            f"{context.error_message}"
+        )
+        
+        if severity == ErrorSeverity.CRITICAL:
+            logger.critical(log_msg)
+        elif severity == ErrorSeverity.HIGH:
+            logger.error(log_msg)
+        elif severity == ErrorSeverity.MEDIUM:
+            logger.warning(log_msg)
+        else:
+            logger.info(log_msg)
+        
+        # Update circuit breaker
+        self._record_failure(context.component)
+        
+        # Send alerts for high/critical errors
+        if severity in [ErrorSeverity.HIGH, ErrorSeverity.CRITICAL]:
+            await self._send_alert(context, severity)
+    
+    def _determine_error_severity(
+        self, context: ErrorContext, exception: Exception
+    ) -> ErrorSeverity:
+        """Determine error severity based on context and exception type."""
+        # Critical errors that affect core functionality
+        critical_errors = [
+            "AuthenticationError", "PermissionError", "ConfigurationError"
+        ]
+        
+        # High severity errors that impact user experience
+        high_severity_errors = [
+            "ConnectionError", "ServiceUnavailableError", "RateLimitError"
+        ]
+        
+        # Medium severity errors that are recoverable
+        medium_severity_errors = [
+            "TimeoutError", "ValidationError", "ParseError"
+        ]
+        
+        error_type = context.error_type
+        
+        if error_type in critical_errors:
+            return ErrorSeverity.CRITICAL
+        elif error_type in high_severity_errors:
+            return ErrorSeverity.HIGH
+        elif error_type in medium_severity_errors:
+            return ErrorSeverity.MEDIUM
+        else:
+            return ErrorSeverity.LOW
+    
+    def _calculate_retry_delay(self, attempt: int) -> float:
+        """Calculate retry delay with exponential backoff and jitter."""
+        base_delay = self.retry_config.get("base_delay", 1.0)
+        max_delay = self.retry_config.get("max_delay", 30.0)
+        strategy = self.retry_config.get("backoff_strategy", "exponential")
+        use_jitter = self.retry_config.get("jitter", True)
+        
+        if strategy == "exponential":
+            delay = base_delay * (2 ** (attempt - 1))
+        elif strategy == "linear":
+            delay = base_delay * attempt
+        else:  # constant
+            delay = base_delay
+        
+        # Apply maximum delay limit
+        delay = min(delay, max_delay)
+        
+        # Add jitter to prevent thundering herd
+        if use_jitter:
+            jitter_range = delay * 0.1
+            delay += random.uniform(-jitter_range, jitter_range)
+        
+        return max(delay, 0.1)  # Minimum 0.1s delay
+    
+    def _check_circuit_breaker(self, component: str) -> bool:
+        """Check if circuit breaker allows operation."""
+        if not self.circuit_config.get("enabled", False):
+            return True
+        
+        breaker = self.circuit_breakers.get(component)
+        if not breaker:
+            return True
+        
+        failure_threshold = self.circuit_config.get("failure_threshold", 5)
+        recovery_timeout = self.circuit_config.get("recovery_timeout", 300)
+        current_time = time.time()
+        
+        if breaker.state == "CLOSED":
+            return True
+        elif breaker.state == "OPEN":
+            # Check if recovery timeout has passed
+            if (breaker.last_failure and 
+                current_time - breaker.last_failure > recovery_timeout):
+                breaker.state = "HALF_OPEN"
+                breaker.half_open_attempts = 0
+                logger.info(f"[Circuit Breaker] {component} transitioning to HALF_OPEN")
+                return True
+            return False
+        elif breaker.state == "HALF_OPEN":
+            # Allow limited requests in half-open state
+            max_half_open = self.circuit_config.get("half_open_requests", 3)
+            if breaker.half_open_attempts < max_half_open:
+                breaker.half_open_attempts += 1
+                return True
+            return False
+        
+        return False
+    
+    def _record_success(self, component: str):
+        """Record successful operation for circuit breaker."""
+        breaker = self.circuit_breakers.get(component)
+        if not breaker:
+            return
+        
+        current_time = time.time()
+        breaker.last_success = current_time
+        
+        if breaker.state in ["HALF_OPEN", "OPEN"]:
+            breaker.state = "CLOSED"
+            breaker.failures = 0
+            breaker.half_open_attempts = 0
+            self.error_stats["recovery_successes"] += 1
+            logger.info(f"[Circuit Breaker] {component} recovered, state: CLOSED")
+    
+    def _record_failure(self, component: str):
+        """Record failed operation for circuit breaker."""
+        breaker = self.circuit_breakers.get(component)
+        if not breaker:
+            return
+        
+        current_time = time.time()
+        breaker.failures += 1
+        breaker.last_failure = current_time
+        
+        failure_threshold = self.circuit_config.get("failure_threshold", 5)
+        
+        if breaker.failures >= failure_threshold:
+            breaker.state = "OPEN"
+            logger.warning(f"[Circuit Breaker] {component} opened due to failures")
+        elif breaker.state == "HALF_OPEN":
+            # Failed during half-open, go back to open
+            breaker.state = "OPEN"
+            logger.warning(f"[Circuit Breaker] {component} back to OPEN from HALF_OPEN")
+    
+    async def _apply_fallback_strategy(
+        self, strategy: FallbackStrategy, context: ErrorContext
+    ) -> Any:
+        """Apply fallback strategy when operation fails."""
+        self.error_stats["fallback_activations"] += 1
+        
+        logger.info(
+            f"[ErrorHandler] ğŸ”„ Applying fallback strategy: {strategy.value} "
+            f"for {context.operation}"
+        )
+        
+        if strategy == FallbackStrategy.LOCAL_R1:
+            return await self._fallback_to_local_r1(context)
+        elif strategy == FallbackStrategy.SIMPLIFIED_RESPONSE:
+            return self._fallback_simplified_response(context)
+        elif strategy == FallbackStrategy.SKIP_RECORDING:
+            logger.info(f"[ErrorHandler] Skipping recording for {context.operation}")
+            return None
+        elif strategy == FallbackStrategy.LOCAL_CACHE:
+            return await self._fallback_local_cache(context)
+        elif strategy == FallbackStrategy.RETRY_LATER:
+            await self._schedule_retry(context)
+            return None
+        else:
+            raise NotImplementedError(f"Fallback strategy {strategy.value} not implemented")
+    
+    async def _fallback_to_local_r1(self, context: ErrorContext) -> str:
+        """Fallback to local R1 model when cloud APIs fail."""
+        logger.info("[ErrorHandler] ğŸ  Falling back to local Japanese R1 model")
+        
+        try:
+            # This would integrate with the local R1 client
+            # For now, return a placeholder response
+            return (
+                "ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ãŒã€ã‚¯ãƒ©ã‚¦ãƒ‰APIãŒåˆ©ç”¨ã§ããªã„ãŸã‚ã€"
+                "ãƒ­ãƒ¼ã‚«ãƒ«ã®æ—¥æœ¬èªR1ãƒ¢ãƒ‡ãƒ«ã§å¯¾å¿œã„ãŸã—ã¾ã™ã€‚"
+                "å®Œå…¨ãªæ©Ÿèƒ½ã§ã¯ã‚ã‚Šã¾ã›ã‚“ãŒã€åŸºæœ¬çš„ãªã‚µãƒãƒ¼ãƒˆã‚’æä¾›ã§ãã¾ã™ã€‚"
+            )
+        except Exception as e:
+            logger.error(f"[ErrorHandler] Local R1 fallback also failed: {e}")
+            return self._fallback_simplified_response(context)
+    
+    def _fallback_simplified_response(self, context: ErrorContext) -> str:
+        """Provide a simplified response when all else fails."""
+        logger.info("[ErrorHandler] ğŸ“ Providing simplified fallback response")
+        
+        return (
+            f"ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ã€‚ç¾åœ¨ã€{context.component}ã«æŠ€è¡“çš„ãªå•é¡ŒãŒç™ºç”Ÿã—ã¦ã„ã¾ã™ã€‚"
+            "ã—ã°ã‚‰ãã—ã¦ã‹ã‚‰å†åº¦ãŠè©¦ã—ãã ã•ã„ã€‚"
+            f"ã‚¨ãƒ©ãƒ¼è©³ç´°: {context.error_message[:100]}..."
+        )
+    
+    async def _fallback_local_cache(self, context: ErrorContext) -> Optional[str]:
+        """Attempt to use local cache when external service fails."""
+        logger.info("[ErrorHandler] ğŸ’¾ Attempting local cache fallback")
+        
+        # Implementation would check local cache
+        # For now, return None to indicate no cached data
+        return None
+    
+    async def _schedule_retry(self, context: ErrorContext):
+        """Schedule operation for retry later."""
+        logger.info(f"[ErrorHandler] â° Scheduling retry for {context.operation}")
+        
+        # Implementation would add to retry queue
+        # This is a placeholder
+        pass
+    
+    async def _send_alert(self, context: ErrorContext, severity: ErrorSeverity):
+        """Send alert for high severity errors."""
+        alert_msg = (
+            f"ğŸš¨ ShogunAI Alert [{severity.value.upper()}]\n"
+            f"Operation: {context.operation}\n"
+            f"Component: {context.component}\n"
+            f"Error: {context.error_message}\n"
+            f"Attempt: {context.attempt}/{context.max_attempts}"
+        )
+        
+        logger.critical(alert_msg)
+        
+        # In a real implementation, this would:
+        # - Send Slack notification
+        # - Write to alert log
+        # - Potentially trigger PagerDuty/email alerts
+    
+    def get_health_status(self) -> Dict[str, Any]:
+        """Get current health status of all components."""
+        status = {
+            "overall_health": "healthy",
+            "circuit_breakers": {},
+            "error_stats": dict(self.error_stats),
+            "timestamp": time.time()
+        }
+        
+        unhealthy_components = 0
+        
+        for name, breaker in self.circuit_breakers.items():
+            breaker_status = {
+                "state": breaker.state,
+                "failures": breaker.failures,
+                "last_failure": breaker.last_failure,
+                "last_success": breaker.last_success
+            }
+            status["circuit_breakers"][name] = breaker_status
+            
+            if breaker.state == "OPEN":
+                unhealthy_components += 1
+        
+        # Determine overall health
+        if unhealthy_components > 0:
+            if unhealthy_components >= len(self.circuit_breakers) / 2:
+                status["overall_health"] = "critical"
+            else:
+                status["overall_health"] = "degraded"
+        
+        return status
+    
+    def reset_error_stats(self):
+        """Reset error statistics (useful for testing or maintenance)."""
+        self.error_stats = {
+            "total_errors": 0,
+            "errors_by_component": {},
+            "errors_by_type": {},
+            "recovery_successes": 0,
+            "fallback_activations": 0,
+        }
+        logger.info("[ErrorHandler] Error statistics reset")
+    
+    def export_error_report(self, filepath: str):
+        """Export detailed error report to file."""
+        report = {
+            "timestamp": time.time(),
+            "error_stats": self.error_stats,
+            "circuit_breakers": {
+                name: {
+                    "state": breaker.state,
+                    "failures": breaker.failures,
+                    "last_failure": breaker.last_failure,
+                    "last_success": breaker.last_success
+                }
+                for name, breaker in self.circuit_breakers.items()
+            },
+            "configuration": self.config
+        }
+        
+        with open(filepath, 'w', encoding='utf-8') as f:
+            json.dump(report, f, indent=2, ensure_ascii=False)
+        
+        logger.info(f"[ErrorHandler] Error report exported to {filepath}")
+
+
+# Custom Exception Classes
+class ShogunError(Exception):
+    """Base exception class for ShogunAI."""
+    pass
+
+
+class OperationFailedError(ShogunError):
+    """Raised when an operation fails after all retries."""
+    pass
+
+
+class CircuitBreakerOpenError(ShogunError):
+    """Raised when circuit breaker is open."""
+    pass
+
+
+class FallbackError(ShogunError):
+    """Raised when fallback strategy fails."""
+    pass
+
+
+class ConfigurationError(ShogunError):
+    """Raised when there's a configuration issue."""
+    pass
\ No newline at end of file
diff --git a/core/escalation.py b/core/escalation.py
new file mode 100644
index 0000000..0b68cfc
--- /dev/null
+++ b/core/escalation.py
@@ -0,0 +1,108 @@
+"""Escalation Engine - ã‚¨ã‚¹ã‚«ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³åˆ¶å¾¡
+
+ã‚¨ã‚¹ã‚«ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³é€£é–:
+  è¶³è»½(MCP) â†’ ä¾å¤§å°†(R1) â†’ å®¶è€(Sonnet) â†’ å°†è»(Opus)
+
+ä¸­éšŠãƒ¢ãƒ¼ãƒ‰ã§ã¯ä¾å¤§å°†ãŒä¸Šé™ã€‚èƒ½åŠ›è¶…éæ™‚ã«å¤§éšŠãƒ¢ãƒ¼ãƒ‰æ¨å¥¨ã‚’é€šçŸ¥ã€‚
+"""
+
+import logging
+from shogun.core.task_queue import Task, TaskStatus, Complexity
+
+logger = logging.getLogger("shogun.escalation")
+
+# Escalation chain (lower index = lower tier)
+ESCALATION_CHAIN = [
+    "taisho",   # ä¾å¤§å°† (R1, ãƒ­ãƒ¼ã‚«ãƒ«)
+    "karo",     # å®¶è€ (Sonnet, ä½œæ¥­å‰²æŒ¯ã‚Š)
+    "shogun",   # å°†è» (Opus, æœ€çµ‚æ±ºè£)
+]
+
+# Complexity â†’ default handler
+COMPLEXITY_HANDLER = {
+    Complexity.SIMPLE: "taisho",
+    Complexity.MEDIUM: "taisho",
+    Complexity.COMPLEX: "karo",       # å®¶è€ãŒå°†è»ã«æ¸¡ã™å‰ã«åˆ¤æ–­
+    Complexity.STRATEGIC: "shogun",   # å°†è»ãŒæœ€çµ‚æ±ºè£
+}
+
+# Agent â†’ required cost (yen)
+AGENT_COST = {
+    "taisho": 0,
+    "karo": 280,
+    "shogun": 1350,
+}
+
+
+def get_handler(complexity: Complexity) -> str:
+    """Get the default handler agent for a given complexity."""
+    return COMPLEXITY_HANDLER.get(complexity, "taisho")
+
+
+def get_next_escalation(current_agent: str) -> str | None:
+    """Get the next agent in the escalation chain.
+
+    Returns None if already at the top (Shogun).
+    """
+    try:
+        idx = ESCALATION_CHAIN.index(current_agent)
+    except ValueError:
+        return "taisho"  # Default: start from taisho
+
+    if idx + 1 < len(ESCALATION_CHAIN):
+        next_agent = ESCALATION_CHAIN[idx + 1]
+        logger.info("ã‚¨ã‚¹ã‚«ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³: %s â†’ %s", current_agent, next_agent)
+        return next_agent
+
+    logger.warning("å°†è»ï¼ˆæœ€ä¸Šä½ï¼‰ã€‚ã“ã‚Œä»¥ä¸Šã®ã‚¨ã‚¹ã‚«ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ä¸å¯ã€‚")
+    return None
+
+
+def should_escalate(task: Task) -> bool:
+    """Determine if a failed/blocked task should be escalated."""
+    if task.status not in (TaskStatus.FAILED, TaskStatus.BLOCKED):
+        return False
+    if task.escalation_count >= len(ESCALATION_CHAIN) - 1:
+        return False
+    return True
+
+
+def can_handle_in_company_mode(complexity: Complexity) -> bool:
+    """Check if the task can be handled in company mode (ä¸­éšŠ).
+
+    ä¸­éšŠ = ä¾å¤§å°† + è¶³è»½ã®ã¿ã€‚Simple/Medium ã®ã¿å¯¾å¿œã€‚
+    """
+    return complexity in (Complexity.SIMPLE, Complexity.MEDIUM)
+
+
+def build_escalation_context(task: Task) -> str:
+    """Build context message for escalation target."""
+    parts = [
+        "## ã‚¨ã‚¹ã‚«ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å ±å‘Š",
+        f"**ä»»å‹™ID**: {task.id}",
+        f"**å…ƒã®æŒ‡ç¤º**: {task.prompt}",
+        f"**è¤‡é›‘åº¦**: {task.complexity.value}",
+        f"**å‰ä»»**: {task.assigned_agent}",
+        f"**ã‚¨ã‚¹ã‚«ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å›æ•°**: {task.escalation_count}",
+    ]
+    if task.error:
+        parts.append(f"**ã‚¨ãƒ©ãƒ¼**: {task.error}")
+    if task.result:
+        parts.append(f"**é€”ä¸­çµŒé**:\n{task.result[:2000]}")
+    return "\n".join(parts)
+
+
+def build_taisho_analysis_prompt(task: Task) -> str:
+    """Build prompt for Taisho to analyze before escalation to Karo/Shogun."""
+    return f"""ã‚ãªãŸã¯ä¾å¤§å°†ã§ã™ã€‚ä»¥ä¸‹ã®ä»»å‹™ã‚’åˆ†æã—ã€ä¸Šä½ã¸ã®å ±å‘Šã‚’ã¾ã¨ã‚ã¦ãã ã•ã„ã€‚
+å®Ÿè£…ã¯å®¶è€ãƒ»å°†è»ãŒè¡Œã„ã¾ã™ã€‚ã‚ãªãŸã¯åˆ†æã¨æ–¹é‡ææ¡ˆã®ã¿è¡Œã£ã¦ãã ã•ã„ã€‚
+
+## ä»»å‹™
+{task.prompt}
+
+## æŒ‡ç¤º
+1. <think>ã‚¿ã‚°ã§å¾¹åº•çš„ã«æ€è€ƒã—ã¦ãã ã•ã„
+2. å•é¡Œã®æœ¬è³ªã‚’ç‰¹å®šã—ã¦ãã ã•ã„
+3. æ¨å¥¨ã™ã‚‹è§£æ±ºæ–¹é‡ã‚’ææ¡ˆã—ã¦ãã ã•ã„
+4. è¶³è»½ï¼ˆMCPãƒ„ãƒ¼ãƒ«ï¼‰ã§é›†ã‚ãŸæƒ…å ±ãŒã‚ã‚Œã°æ•´ç†ã—ã¦ãã ã•ã„
+"""
diff --git a/core/knowledge_base.py b/core/knowledge_base.py
new file mode 100644
index 0000000..4fb10c1
--- /dev/null
+++ b/core/knowledge_base.py
@@ -0,0 +1,480 @@
+"""
+å°†è»ã‚·ã‚¹ãƒ†ãƒ  v8.0 - Knowledge Base (RAG) System
+çŸ¥è­˜åŸºç›¤: å¤–éƒ¨çŸ¥è­˜ã®ä¿å­˜ãƒ»æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ 
+
+Features:
+- Qdrant vector database integration
+- Sentence Transformers embeddings
+- Multi-language support
+- Auto-cleanup (30 days retention)
+- Real-time search with scoring
+"""
+
+import asyncio
+import json
+import logging
+from datetime import datetime, timedelta
+from typing import List, Dict, Any, Optional, Tuple
+from dataclasses import dataclass
+from enum import Enum
+
+import numpy as np
+from qdrant_client import QdrantClient
+from qdrant_client.http import models
+from qdrant_client.http.models import Distance, VectorParams
+from sentence_transformers import SentenceTransformer
+import httpx
+
+# Configure logging
+logging.basicConfig(level=logging.INFO)
+logger = logging.getLogger(__name__)
+
+
+class DataSource(Enum):
+    """Data source types for knowledge base entries"""
+    OLLAMA_WEB_SEARCH = "ollama_web_search"
+    MANUAL_DOCUMENT = "manual_document"  
+    GITHUB_ISSUE = "github_issue"
+    GITHUB_PR = "github_pr"
+    NOTION_PAGE = "notion_page"
+
+
+@dataclass
+class KnowledgeEntry:
+    """Knowledge base entry structure"""
+    id: str
+    title: str
+    content: str
+    source: DataSource
+    url: Optional[str] = None
+    metadata: Dict[str, Any] = None
+    language: str = "ja"
+    created_at: datetime = None
+    relevance_score: Optional[float] = None
+    
+    def __post_init__(self):
+        if self.created_at is None:
+            self.created_at = datetime.utcnow()
+        if self.metadata is None:
+            self.metadata = {}
+
+
+class KnowledgeBase:
+    """
+    å°†è»ã‚·ã‚¹ãƒ†ãƒ  Knowledge Base (RAG) Implementation
+    
+    Provides semantic search capabilities for the Shogun system using:
+    - Qdrant for vector storage
+    - Sentence Transformers for embeddings
+    - Automatic data lifecycle management
+    """
+    
+    def __init__(
+        self,
+        host: str = "192.168.1.10",
+        port: int = 6333,
+        collection_name: str = "shogun_knowledge",
+        embedding_model: str = "sentence-transformers/all-mpnet-base-v2",
+        retention_days: int = 30
+    ):
+        self.host = host
+        self.port = port
+        self.collection_name = collection_name
+        self.retention_days = retention_days
+        
+        # Initialize Qdrant client
+        self.client = QdrantClient(host=host, port=port)
+        
+        # Initialize embedding model
+        logger.info(f"Loading embedding model: {embedding_model}")
+        self.embedding_model = SentenceTransformer(embedding_model)
+        self.embedding_dimension = self.embedding_model.get_sentence_embedding_dimension()
+        
+        # Initialize collection
+        asyncio.create_task(self._initialize_collection())
+        
+        logger.info(f"KnowledgeBase initialized - Host: {host}:{port}, Collection: {collection_name}")
+    
+    async def _initialize_collection(self) -> None:
+        """Initialize Qdrant collection if it doesn't exist"""
+        try:
+            collections = self.client.get_collections()
+            collection_names = [col.name for col in collections.collections]
+            
+            if self.collection_name not in collection_names:
+                logger.info(f"Creating collection: {self.collection_name}")
+                self.client.create_collection(
+                    collection_name=self.collection_name,
+                    vectors_config=VectorParams(
+                        size=self.embedding_dimension,
+                        distance=Distance.COSINE
+                    )
+                )
+                logger.info(f"Collection {self.collection_name} created successfully")
+            else:
+                logger.info(f"Collection {self.collection_name} already exists")
+                
+        except Exception as e:
+            logger.error(f"Failed to initialize collection: {e}")
+            raise
+    
+    def _generate_embedding(self, text: str) -> np.ndarray:
+        """Generate embedding vector for text"""
+        try:
+            embedding = self.embedding_model.encode(text)
+            return embedding.tolist()
+        except Exception as e:
+            logger.error(f"Failed to generate embedding: {e}")
+            raise
+    
+    async def add_entry(self, entry: KnowledgeEntry) -> bool:
+        """Add a knowledge entry to the database"""
+        try:
+            # Generate embedding
+            combined_text = f"{entry.title} {entry.content}"
+            embedding = self._generate_embedding(combined_text)
+            
+            # Prepare payload
+            payload = {
+                "id": entry.id,
+                "title": entry.title,
+                "content": entry.content,
+                "source": entry.source.value,
+                "url": entry.url,
+                "metadata": entry.metadata,
+                "language": entry.language,
+                "created_at": entry.created_at.isoformat(),
+                "text": combined_text  # For full-text search
+            }
+            
+            # Insert into Qdrant
+            self.client.upsert(
+                collection_name=self.collection_name,
+                points=[
+                    models.PointStruct(
+                        id=entry.id,
+                        vector=embedding,
+                        payload=payload
+                    )
+                ]
+            )
+            
+            logger.info(f"Added knowledge entry: {entry.id} - {entry.title[:50]}...")
+            return True
+            
+        except Exception as e:
+            logger.error(f"Failed to add entry {entry.id}: {e}")
+            return False
+    
+    async def search(
+        self,
+        query: str,
+        max_results: int = 5,
+        score_threshold: float = 0.7,
+        source_filter: Optional[List[DataSource]] = None,
+        language_filter: Optional[str] = None
+    ) -> List[KnowledgeEntry]:
+        """
+        Search knowledge base using semantic similarity
+        
+        Args:
+            query: Search query text
+            max_results: Maximum number of results
+            score_threshold: Minimum similarity score (0-1)
+            source_filter: Filter by data sources
+            language_filter: Filter by language
+        
+        Returns:
+            List of relevant knowledge entries with scores
+        """
+        try:
+            # Generate query embedding
+            query_embedding = self._generate_embedding(query)
+            
+            # Prepare filters
+            must_conditions = []
+            
+            if source_filter:
+                source_values = [source.value for source in source_filter]
+                must_conditions.append(
+                    models.FieldCondition(
+                        key="source",
+                        match=models.MatchAny(any=source_values)
+                    )
+                )
+            
+            if language_filter:
+                must_conditions.append(
+                    models.FieldCondition(
+                        key="language",
+                        match=models.MatchValue(value=language_filter)
+                    )
+                )
+            
+            query_filter = None
+            if must_conditions:
+                query_filter = models.Filter(must=must_conditions)
+            
+            # Perform search
+            search_results = self.client.search(
+                collection_name=self.collection_name,
+                query_vector=query_embedding,
+                query_filter=query_filter,
+                limit=max_results,
+                score_threshold=score_threshold
+            )
+            
+            # Convert results to KnowledgeEntry objects
+            entries = []
+            for result in search_results:
+                payload = result.payload
+                entry = KnowledgeEntry(
+                    id=payload["id"],
+                    title=payload["title"],
+                    content=payload["content"],
+                    source=DataSource(payload["source"]),
+                    url=payload.get("url"),
+                    metadata=payload.get("metadata", {}),
+                    language=payload["language"],
+                    created_at=datetime.fromisoformat(payload["created_at"]),
+                    relevance_score=result.score
+                )
+                entries.append(entry)
+            
+            logger.info(f"Search query: '{query}' - Found {len(entries)} results")
+            return entries
+            
+        except Exception as e:
+            logger.error(f"Search failed for query '{query}': {e}")
+            return []
+    
+    async def get_entry(self, entry_id: str) -> Optional[KnowledgeEntry]:
+        """Retrieve a specific knowledge entry by ID"""
+        try:
+            result = self.client.retrieve(
+                collection_name=self.collection_name,
+                ids=[entry_id]
+            )
+            
+            if not result:
+                return None
+            
+            payload = result[0].payload
+            entry = KnowledgeEntry(
+                id=payload["id"],
+                title=payload["title"],
+                content=payload["content"],
+                source=DataSource(payload["source"]),
+                url=payload.get("url"),
+                metadata=payload.get("metadata", {}),
+                language=payload["language"],
+                created_at=datetime.fromisoformat(payload["created_at"])
+            )
+            
+            return entry
+            
+        except Exception as e:
+            logger.error(f"Failed to retrieve entry {entry_id}: {e}")
+            return None
+    
+    async def update_entry(self, entry: KnowledgeEntry) -> bool:
+        """Update an existing knowledge entry"""
+        try:
+            return await self.add_entry(entry)  # Upsert operation
+        except Exception as e:
+            logger.error(f"Failed to update entry {entry.id}: {e}")
+            return False
+    
+    async def delete_entry(self, entry_id: str) -> bool:
+        """Delete a knowledge entry"""
+        try:
+            self.client.delete(
+                collection_name=self.collection_name,
+                points_selector=models.PointIdsList(points=[entry_id])
+            )
+            
+            logger.info(f"Deleted knowledge entry: {entry_id}")
+            return True
+            
+        except Exception as e:
+            logger.error(f"Failed to delete entry {entry_id}: {e}")
+            return False
+    
+    async def cleanup_old_entries(self) -> int:
+        """
+        Remove entries older than retention_days
+        Returns number of entries deleted
+        """
+        try:
+            cutoff_date = datetime.utcnow() - timedelta(days=self.retention_days)
+            
+            # Scroll through all entries to find old ones
+            scroll_result = self.client.scroll(
+                collection_name=self.collection_name,
+                limit=1000,  # Process in batches
+                with_payload=True
+            )
+            
+            old_entry_ids = []
+            
+            while True:
+                points = scroll_result[0]
+                
+                for point in points:
+                    created_at = datetime.fromisoformat(point.payload["created_at"])
+                    if created_at < cutoff_date:
+                        old_entry_ids.append(point.id)
+                
+                # Check if there are more entries
+                next_page_offset = scroll_result[1]
+                if next_page_offset is None:
+                    break
+                
+                scroll_result = self.client.scroll(
+                    collection_name=self.collection_name,
+                    limit=1000,
+                    offset=next_page_offset,
+                    with_payload=True
+                )
+            
+            # Delete old entries in batches
+            if old_entry_ids:
+                self.client.delete(
+                    collection_name=self.collection_name,
+                    points_selector=models.PointIdsList(points=old_entry_ids)
+                )
+                
+                logger.info(f"Cleanup: Deleted {len(old_entry_ids)} old entries (older than {self.retention_days} days)")
+            else:
+                logger.info("Cleanup: No old entries to delete")
+            
+            return len(old_entry_ids)
+            
+        except Exception as e:
+            logger.error(f"Cleanup failed: {e}")
+            return 0
+    
+    async def get_stats(self) -> Dict[str, Any]:
+        """Get knowledge base statistics"""
+        try:
+            collection_info = self.client.get_collection(self.collection_name)
+            
+            # Get source distribution
+            search_result = self.client.scroll(
+                collection_name=self.collection_name,
+                limit=10000,  # Large enough to get all entries for stats
+                with_payload=True
+            )
+            
+            source_counts = {}
+            language_counts = {}
+            total_entries = 0
+            
+            for point in search_result[0]:
+                payload = point.payload
+                source = payload.get("source", "unknown")
+                language = payload.get("language", "unknown")
+                
+                source_counts[source] = source_counts.get(source, 0) + 1
+                language_counts[language] = language_counts.get(language, 0) + 1
+                total_entries += 1
+            
+            stats = {
+                "total_entries": total_entries,
+                "collection_status": collection_info.status.value,
+                "vectors_count": collection_info.vectors_count or 0,
+                "indexed_vectors_count": collection_info.indexed_vectors_count or 0,
+                "source_distribution": source_counts,
+                "language_distribution": language_counts,
+                "retention_days": self.retention_days,
+                "embedding_dimension": self.embedding_dimension
+            }
+            
+            return stats
+            
+        except Exception as e:
+            logger.error(f"Failed to get stats: {e}")
+            return {}
+    
+    async def health_check(self) -> Dict[str, Any]:
+        """Perform health check of the knowledge base"""
+        try:
+            # Check Qdrant connection
+            collections = self.client.get_collections()
+            collection_exists = any(col.name == self.collection_name for col in collections.collections)
+            
+            # Check embedding model
+            test_embedding = self._generate_embedding("test")
+            embedding_works = len(test_embedding) == self.embedding_dimension
+            
+            health_status = {
+                "status": "healthy" if (collection_exists and embedding_works) else "unhealthy",
+                "qdrant_connection": "ok" if collection_exists else "failed",
+                "embedding_model": "ok" if embedding_works else "failed",
+                "collection_exists": collection_exists,
+                "host": self.host,
+                "port": self.port,
+                "collection_name": self.collection_name,
+                "checked_at": datetime.utcnow().isoformat()
+            }
+            
+            return health_status
+            
+        except Exception as e:
+            logger.error(f"Health check failed: {e}")
+            return {
+                "status": "error",
+                "error": str(e),
+                "checked_at": datetime.utcnow().isoformat()
+            }
+
+
+# Factory function for easy instantiation
+def create_knowledge_base(config: Dict[str, Any]) -> KnowledgeBase:
+    """Create KnowledgeBase instance from configuration"""
+    return KnowledgeBase(
+        host=config.get("host", "192.168.1.10"),
+        port=config.get("port", 6333),
+        collection_name=config.get("collection_name", "shogun_knowledge"),
+        embedding_model=config.get("embedding_model", "sentence-transformers/all-mpnet-base-v2"),
+        retention_days=config.get("retention_days", 30)
+    )
+
+
+# Example usage and testing
+if __name__ == "__main__":
+    async def test_knowledge_base():
+        """Test the knowledge base functionality"""
+        kb = KnowledgeBase()
+        
+        # Test entry
+        test_entry = KnowledgeEntry(
+            id="test_001",
+            title="React 19.1 æ–°æ©Ÿèƒ½",
+            content="React 19.1ã§ã¯æ–°ã—ã„ã‚³ãƒ³ã‚«ãƒ¬ãƒ³ãƒˆæ©Ÿèƒ½ãŒè¿½åŠ ã•ã‚Œã¾ã—ãŸã€‚Suspenseã®æ”¹å–„ã¨Server Componentsã®å®‰å®šåŒ–ãŒä¸»è¦ãªå¤‰æ›´ç‚¹ã§ã™ã€‚",
+            source=DataSource.OLLAMA_WEB_SEARCH,
+            url="https://react.dev/blog/react-19",
+            metadata={"version": "19.1", "type": "feature_update"},
+            language="ja"
+        )
+        
+        # Add entry
+        success = await kb.add_entry(test_entry)
+        print(f"Add entry: {success}")
+        
+        # Search
+        results = await kb.search("React æ–°æ©Ÿèƒ½", max_results=3)
+        print(f"Search results: {len(results)}")
+        for result in results:
+            print(f"  - {result.title} (score: {result.relevance_score:.3f})")
+        
+        # Stats
+        stats = await kb.get_stats()
+        print(f"Stats: {stats}")
+        
+        # Health check
+        health = await kb.health_check()
+        print(f"Health: {health}")
+    
+    # Run test
+    asyncio.run(test_knowledge_base())
\ No newline at end of file
diff --git a/core/maintenance.py b/core/maintenance.py
new file mode 100644
index 0000000..80140ea
--- /dev/null
+++ b/core/maintenance.py
@@ -0,0 +1,525 @@
+"""
+å°†è»ã‚·ã‚¹ãƒ†ãƒ  v7.0 - Monthly Maintenance (åçœä¼š)
+=================================================
+æœˆæ¬¡ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹æ©Ÿèƒ½:
+- LLMãƒãƒ¼ã‚¸ãƒ§ãƒ³ç¢ºèªãƒ»æ›´æ–°
+- ã‚·ã‚¹ãƒ†ãƒ ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯
+- MCPã‚µãƒ¼ãƒãƒ¼æ›´æ–°
+- ãƒ­ã‚°ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
+- ã‚³ã‚¹ãƒˆãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
+"""
+
+import subprocess
+import json
+import os
+from datetime import datetime, timedelta
+from pathlib import Path
+from typing import Any
+import logging
+import yaml
+import httpx
+
+logger = logging.getLogger(__name__)
+
+
+class MaintenanceManager:
+    """æœˆæ¬¡ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹ï¼ˆåçœä¼šï¼‰ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼"""
+
+    def __init__(self, base_dir: Path | None = None):
+        self.base_dir = base_dir or Path(__file__).parent.parent
+        self.config = self._load_config()
+        self.reports_dir = self.base_dir / self.config.get("reports", {}).get(
+            "directory", "reports/maintenance"
+        )
+        self.reports_dir.mkdir(parents=True, exist_ok=True)
+
+    def _load_config(self) -> dict:
+        """ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹è¨­å®šã‚’èª­ã¿è¾¼ã‚€"""
+        config_path = self.base_dir / "config" / "settings.yaml"
+        if config_path.exists():
+            with open(config_path) as f:
+                settings = yaml.safe_load(f)
+                return settings.get("maintenance", {})
+        return {}
+
+    def run_full_maintenance(self) -> dict[str, Any]:
+        """å…¨ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹é …ç›®ã‚’å®Ÿè¡Œ"""
+        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
+        report = {
+            "timestamp": timestamp,
+            "date": datetime.now().isoformat(),
+            "checks": {},
+            "summary": {
+                "total": 0,
+                "passed": 0,
+                "warnings": 0,
+                "errors": 0,
+            },
+        }
+
+        checks = [
+            ("llm_versions", self.check_llm_versions),
+            ("openvino_model", self.check_openvino_model),
+            ("mcp_servers", self.check_mcp_servers),
+            ("system_health", self.check_system_health),
+            ("log_cleanup", self.cleanup_logs),
+            ("cost_report", self.generate_cost_report),
+        ]
+
+        for check_id, check_func in checks:
+            report["summary"]["total"] += 1
+            try:
+                result = check_func()
+                report["checks"][check_id] = result
+                if result.get("status") == "ok":
+                    report["summary"]["passed"] += 1
+                elif result.get("status") == "warning":
+                    report["summary"]["warnings"] += 1
+                else:
+                    report["summary"]["errors"] += 1
+            except Exception as e:
+                report["checks"][check_id] = {
+                    "status": "error",
+                    "error": str(e),
+                }
+                report["summary"]["errors"] += 1
+                logger.error(f"Maintenance check failed: {check_id}: {e}")
+
+        # ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜
+        report_path = self.reports_dir / f"maintenance_{timestamp}.json"
+        with open(report_path, "w", encoding="utf-8") as f:
+            json.dump(report, f, ensure_ascii=False, indent=2)
+
+        # ã‚µãƒãƒªãƒ¼Markdownç”Ÿæˆ
+        self._generate_markdown_report(report, timestamp)
+
+        logger.info(f"Maintenance complete: {report['summary']}")
+        return report
+
+    def check_llm_versions(self) -> dict[str, Any]:
+        """ã‚¯ãƒ©ã‚¦ãƒ‰LLMãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’ç¢ºèª"""
+        result = {
+            "name": "LLMãƒãƒ¼ã‚¸ãƒ§ãƒ³ç¢ºèª",
+            "status": "ok",
+            "current": {},
+            "latest": {},
+            "updates_available": [],
+        }
+
+        # ç¾åœ¨ä½¿ç”¨ä¸­ã®ãƒ¢ãƒ‡ãƒ«
+        config_path = self.base_dir / "config" / "settings.yaml"
+        if config_path.exists():
+            with open(config_path) as f:
+                settings = yaml.safe_load(f)
+                cloud = settings.get("cloud", {})
+                result["current"]["shogun"] = cloud.get("shogun", {}).get(
+                    "api_model", "unknown"
+                )
+                result["current"]["karo"] = cloud.get("karo", {}).get(
+                    "api_model", "unknown"
+                )
+
+        # Claude CLI ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç¢ºèª
+        try:
+            proc = subprocess.run(
+                ["claude", "--version"],
+                capture_output=True,
+                text=True,
+                timeout=10,
+            )
+            if proc.returncode == 0:
+                result["current"]["claude_cli"] = proc.stdout.strip()
+        except Exception:
+            result["current"]["claude_cli"] = "not installed"
+
+        # npm ã§æœ€æ–°ç‰ˆç¢ºèª
+        try:
+            proc = subprocess.run(
+                ["npm", "view", "@anthropic-ai/claude-code", "version"],
+                capture_output=True,
+                text=True,
+                timeout=30,
+            )
+            if proc.returncode == 0:
+                latest = proc.stdout.strip()
+                result["latest"]["claude_cli"] = latest
+                if result["current"].get("claude_cli", "").find(latest) == -1:
+                    result["updates_available"].append(
+                        f"claude-cli: {result['current'].get('claude_cli')} â†’ {latest}"
+                    )
+        except Exception:
+            pass
+
+        if result["updates_available"]:
+            result["status"] = "warning"
+            result["message"] = f"{len(result['updates_available'])}ä»¶ã®æ›´æ–°ãŒã‚ã‚Šã¾ã™"
+        else:
+            result["message"] = "å…¨ã¦æœ€æ–°ã§ã™"
+
+        return result
+
+    def check_openvino_model(self) -> dict[str, Any]:
+        """ä¾å¤§å°† OpenVINO ãƒ¢ãƒ‡ãƒ«ç¢ºèª"""
+        result = {
+            "name": "OpenVINOãƒ¢ãƒ‡ãƒ«ç¢ºèª",
+            "status": "ok",
+            "taisho_reachable": False,
+            "model_info": {},
+        }
+
+        config_path = self.base_dir / "config" / "settings.yaml"
+        taisho_url = "http://192.168.1.11:11434"
+        if config_path.exists():
+            with open(config_path) as f:
+                settings = yaml.safe_load(f)
+                taisho_url = settings.get("taisho", {}).get("url", taisho_url)
+
+        try:
+            with httpx.Client(timeout=10) as client:
+                resp = client.get(f"{taisho_url}/")
+                if resp.status_code == 200:
+                    data = resp.json()
+                    result["taisho_reachable"] = True
+                    result["model_info"] = data
+                    result["message"] = f"ä¾å¤§å°†ç¨¼åƒä¸­: {data.get('model', 'unknown')}"
+        except Exception as e:
+            result["status"] = "error"
+            result["message"] = f"ä¾å¤§å°†ã«æ¥ç¶šã§ãã¾ã›ã‚“: {e}"
+
+        return result
+
+    def check_mcp_servers(self, auto_update: bool = False) -> dict[str, Any]:
+        """MCPã‚µãƒ¼ãƒãƒ¼ï¼ˆè¶³è»½ï¼‰ã®æ›´æ–°ç¢ºèª"""
+        result = {
+            "name": "MCPã‚µãƒ¼ãƒãƒ¼æ›´æ–°ç¢ºèª",
+            "status": "ok",
+            "servers": [],
+            "updates_available": [],
+            "updated": [],
+        }
+
+        mcp_packages = [
+            "@modelcontextprotocol/server-filesystem",
+            "@modelcontextprotocol/server-github",
+            "@modelcontextprotocol/server-fetch",
+            "@modelcontextprotocol/server-memory",
+            "@modelcontextprotocol/server-postgres",
+            "@modelcontextprotocol/server-puppeteer",
+            "@modelcontextprotocol/server-brave-search",
+            "@modelcontextprotocol/server-slack",
+        ]
+
+        # ç¾åœ¨ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã¨æœ€æ–°ç‰ˆã‚’æ¯”è¼ƒ
+        try:
+            proc = subprocess.run(
+                ["npm", "outdated", "-g", "--json"],
+                capture_output=True,
+                text=True,
+                timeout=60,
+            )
+            if proc.stdout:
+                outdated = json.loads(proc.stdout)
+                for pkg in mcp_packages:
+                    if pkg in outdated:
+                        info = outdated[pkg]
+                        result["updates_available"].append({
+                            "package": pkg,
+                            "current": info.get("current"),
+                            "latest": info.get("latest"),
+                        })
+        except Exception as e:
+            logger.warning(f"Failed to check npm outdated: {e}")
+
+        # è‡ªå‹•æ›´æ–°ãŒæœ‰åŠ¹ãªå ´åˆ
+        check_config = next(
+            (c for c in self.config.get("checks", []) if c["id"] == "mcp_servers"),
+            {},
+        )
+        if auto_update or check_config.get("auto_update", False):
+            if result["updates_available"]:
+                try:
+                    proc = subprocess.run(
+                        ["npm", "update", "-g"] + mcp_packages,
+                        capture_output=True,
+                        text=True,
+                        timeout=300,
+                    )
+                    if proc.returncode == 0:
+                        result["updated"] = [u["package"] for u in result["updates_available"]]
+                        result["message"] = f"{len(result['updated'])}ä»¶æ›´æ–°ã—ã¾ã—ãŸ"
+                except Exception as e:
+                    result["status"] = "warning"
+                    result["message"] = f"æ›´æ–°ã«å¤±æ•—: {e}"
+
+        if result["updates_available"] and not result["updated"]:
+            result["status"] = "warning"
+            result["message"] = f"{len(result['updates_available'])}ä»¶ã®æ›´æ–°ãŒã‚ã‚Šã¾ã™"
+        elif not result["updates_available"]:
+            result["message"] = "å…¨ã¦æœ€æ–°ã§ã™"
+
+        return result
+
+    def check_system_health(self) -> dict[str, Any]:
+        """ã‚·ã‚¹ãƒ†ãƒ å…¨ä½“ã®ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯"""
+        result = {
+            "name": "ã‚·ã‚¹ãƒ†ãƒ ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯",
+            "status": "ok",
+            "components": {},
+        }
+
+        # Python venv
+        venv_path = self.base_dir / ".venv"
+        result["components"]["python_venv"] = {
+            "status": "ok" if venv_path.exists() else "error",
+            "path": str(venv_path),
+        }
+
+        # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«
+        settings_path = self.base_dir / "config" / "settings.yaml"
+        result["components"]["settings"] = {
+            "status": "ok" if settings_path.exists() else "error",
+            "path": str(settings_path),
+        }
+
+        # MCPè¨­å®š
+        mcp_config_path = self.base_dir / "config" / "mcp_config.json"
+        result["components"]["mcp_config"] = {
+            "status": "ok" if mcp_config_path.exists() else "warning",
+            "path": str(mcp_config_path),
+        }
+
+        # ç’°å¢ƒå¤‰æ•°
+        env_path = self.base_dir / ".env"
+        result["components"]["env_file"] = {
+            "status": "ok" if env_path.exists() else "warning",
+            "path": str(env_path),
+        }
+
+        # ãƒ‡ã‚£ã‚¹ã‚¯ä½¿ç”¨é‡
+        try:
+            import shutil
+            total, used, free = shutil.disk_usage(self.base_dir)
+            free_gb = free / (1024**3)
+            result["components"]["disk"] = {
+                "status": "ok" if free_gb > 5 else "warning",
+                "free_gb": round(free_gb, 2),
+            }
+        except Exception:
+            pass
+
+        # ã‚¨ãƒ©ãƒ¼ãƒã‚§ãƒƒã‚¯
+        errors = [
+            k for k, v in result["components"].items() if v.get("status") == "error"
+        ]
+        warnings = [
+            k for k, v in result["components"].items() if v.get("status") == "warning"
+        ]
+
+        if errors:
+            result["status"] = "error"
+            result["message"] = f"ã‚¨ãƒ©ãƒ¼: {', '.join(errors)}"
+        elif warnings:
+            result["status"] = "warning"
+            result["message"] = f"è­¦å‘Š: {', '.join(warnings)}"
+        else:
+            result["message"] = "å…¨ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆæ­£å¸¸"
+
+        return result
+
+    def cleanup_logs(self) -> dict[str, Any]:
+        """å¤ã„ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
+        result = {
+            "name": "ãƒ­ã‚°ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—",
+            "status": "ok",
+            "deleted_count": 0,
+            "deleted_size_mb": 0,
+        }
+
+        retention_days = 30
+        cutoff = datetime.now() - timedelta(days=retention_days)
+        deleted_size = 0
+        deleted_count = 0
+
+        # ãƒ­ã‚°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
+        log_dirs = [
+            self.base_dir / "logs",
+            self.base_dir / "queue" / "reports",
+            Path("/var/log"),
+        ]
+
+        for log_dir in log_dirs:
+            if not log_dir.exists():
+                continue
+            try:
+                for log_file in log_dir.glob("*.log"):
+                    if log_file.stat().st_mtime < cutoff.timestamp():
+                        size = log_file.stat().st_size
+                        log_file.unlink()
+                        deleted_count += 1
+                        deleted_size += size
+            except Exception as e:
+                logger.warning(f"Failed to cleanup {log_dir}: {e}")
+
+        result["deleted_count"] = deleted_count
+        result["deleted_size_mb"] = round(deleted_size / (1024 * 1024), 2)
+        result["message"] = f"{deleted_count}ãƒ•ã‚¡ã‚¤ãƒ« ({result['deleted_size_mb']}MB) å‰Šé™¤"
+
+        return result
+
+    def generate_cost_report(self) -> dict[str, Any]:
+        """æœˆæ¬¡ã‚³ã‚¹ãƒˆãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
+        result = {
+            "name": "æœˆæ¬¡ã‚³ã‚¹ãƒˆãƒ¬ãƒãƒ¼ãƒˆ",
+            "status": "ok",
+            "period": {
+                "start": (datetime.now().replace(day=1) - timedelta(days=1)).replace(
+                    day=1
+                ).strftime("%Y-%m-%d"),
+                "end": (datetime.now().replace(day=1) - timedelta(days=1)).strftime(
+                    "%Y-%m-%d"
+                ),
+            },
+            "costs": {
+                "pro_subscription_yen": 3000,
+                "electricity_yen": 800,
+                "api_usage_yen": 0,
+            },
+            "usage": {
+                "shogun_calls": 0,
+                "karo_calls": 0,
+                "taisho_calls": 0,
+            },
+        }
+
+        # ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã‹ã‚‰ã‚³ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿å–å¾—
+        stats_path = self.base_dir / "status" / "stats.json"
+        if stats_path.exists():
+            try:
+                with open(stats_path) as f:
+                    stats = json.load(f)
+                    result["costs"]["api_usage_yen"] = stats.get("api_cost_yen", 0)
+                    result["usage"] = stats.get("usage", result["usage"])
+            except Exception:
+                pass
+
+        total = sum(result["costs"].values())
+        result["costs"]["total_yen"] = total
+        result["message"] = f"æœˆé–“ã‚³ã‚¹ãƒˆ: Â¥{total:,}"
+
+        # äºˆç®—ãƒã‚§ãƒƒã‚¯
+        budget = 6000
+        if total > budget:
+            result["status"] = "warning"
+            result["message"] += f" (äºˆç®—è¶…é: +Â¥{total - budget:,})"
+
+        return result
+
+    def _generate_markdown_report(self, report: dict, timestamp: str) -> None:
+        """Markdownãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
+        md_path = self.reports_dir / f"maintenance_{timestamp}.md"
+
+        lines = [
+            f"# å°†è»ã‚·ã‚¹ãƒ†ãƒ  æœˆæ¬¡ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹å ±å‘Š",
+            f"",
+            f"**å®Ÿè¡Œæ—¥æ™‚:** {report['date']}",
+            f"",
+            f"## ã‚µãƒãƒªãƒ¼",
+            f"",
+            f"| é …ç›® | ä»¶æ•° |",
+            f"|------|------|",
+            f"| ç·ãƒã‚§ãƒƒã‚¯ | {report['summary']['total']} |",
+            f"| æ­£å¸¸ | {report['summary']['passed']} |",
+            f"| è­¦å‘Š | {report['summary']['warnings']} |",
+            f"| ã‚¨ãƒ©ãƒ¼ | {report['summary']['errors']} |",
+            f"",
+            f"## è©³ç´°",
+            f"",
+        ]
+
+        for check_id, check_result in report["checks"].items():
+            status_emoji = {
+                "ok": "âœ…",
+                "warning": "âš ï¸",
+                "error": "âŒ",
+            }.get(check_result.get("status"), "â“")
+
+            lines.append(f"### {status_emoji} {check_result.get('name', check_id)}")
+            lines.append(f"")
+            lines.append(f"**ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹:** {check_result.get('status', 'unknown')}")
+            if check_result.get("message"):
+                lines.append(f"")
+                lines.append(f"**çµæœ:** {check_result['message']}")
+            lines.append(f"")
+
+        # æ¬¡å›ã‚¢ã‚¯ã‚·ãƒ§ãƒ³
+        lines.extend([
+            f"## æ¬¡å›ã‚¢ã‚¯ã‚·ãƒ§ãƒ³",
+            f"",
+        ])
+
+        # æ›´æ–°ãŒå¿…è¦ãªé …ç›®
+        updates_needed = []
+        for check_id, check_result in report["checks"].items():
+            if check_result.get("updates_available"):
+                updates_needed.extend(check_result["updates_available"])
+
+        if updates_needed:
+            lines.append(f"### æ›´æ–°ãŒå¿…è¦ãªé …ç›®")
+            lines.append(f"")
+            for update in updates_needed:
+                if isinstance(update, dict):
+                    lines.append(f"- {update.get('package', update)}: {update.get('current')} â†’ {update.get('latest')}")
+                else:
+                    lines.append(f"- {update}")
+            lines.append(f"")
+        else:
+            lines.append(f"æ›´æ–°ä¸è¦ã€‚å…¨ã¦æœ€æ–°çŠ¶æ…‹ã§ã™ã€‚")
+            lines.append(f"")
+
+        with open(md_path, "w", encoding="utf-8") as f:
+            f.write("\n".join(lines))
+
+        logger.info(f"Markdown report saved: {md_path}")
+
+    def get_next_maintenance_date(self) -> datetime:
+        """æ¬¡å›ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹æ—¥æ™‚ã‚’å–å¾—"""
+        schedule = self.config.get("schedule", {})
+        day = schedule.get("day_of_month", 1)
+        hour = schedule.get("hour", 9)
+        minute = schedule.get("minute", 0)
+
+        now = datetime.now()
+        next_date = now.replace(day=day, hour=hour, minute=minute, second=0, microsecond=0)
+
+        if next_date <= now:
+            # æ¥æœˆã®åŒæ—¥
+            if now.month == 12:
+                next_date = next_date.replace(year=now.year + 1, month=1)
+            else:
+                next_date = next_date.replace(month=now.month + 1)
+
+        return next_date
+
+    def list_reports(self, limit: int = 10) -> list[dict]:
+        """éå»ã®ãƒ¬ãƒãƒ¼ãƒˆä¸€è¦§"""
+        reports = []
+        for report_file in sorted(
+            self.reports_dir.glob("maintenance_*.json"), reverse=True
+        )[:limit]:
+            try:
+                with open(report_file) as f:
+                    data = json.load(f)
+                    reports.append({
+                        "file": report_file.name,
+                        "date": data.get("date"),
+                        "summary": data.get("summary"),
+                    })
+            except Exception:
+                pass
+        return reports
+
+
+def run_maintenance() -> dict:
+    """ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹ã‚’å®Ÿè¡Œï¼ˆCLIã‹ã‚‰å‘¼ã³å‡ºã—ç”¨ï¼‰"""
+    manager = MaintenanceManager()
+    return manager.run_full_maintenance()
diff --git a/core/mcp_manager.py b/core/mcp_manager.py
new file mode 100644
index 0000000..ac4c90c
--- /dev/null
+++ b/core/mcp_manager.py
@@ -0,0 +1,223 @@
+"""MCP Manager - è¶³è»½ Ã— 8 (MCPã‚µãƒ¼ãƒãƒ¼ç¾¤) ç®¡ç†
+
+è¶³è»½ã¯LLMã§ã¯ãªãMCPã‚µãƒ¼ãƒãƒ¼ (ãƒ„ãƒ¼ãƒ«å®Ÿè¡Œå±¤)ã€‚
+å„è¶³è»½ã¯50-150MBã®è»½é‡ãƒ—ãƒ­ã‚»ã‚¹ã§ã€ä¾å¤§å°†ã®æŒ‡ç¤ºã«å¾“ã„ãƒ„ãƒ¼ãƒ«ã‚’å®Ÿè¡Œã™ã‚‹ã€‚
+
+è¶³è»½ä¸€è¦§:
+  1. filesystem  - ãƒ•ã‚¡ã‚¤ãƒ«æ“ä½œ
+  2. github      - Git/GitHubæ“ä½œ
+  3. fetch       - Webæƒ…å ±å–å¾—
+  4. memory      - é•·æœŸè¨˜æ†¶
+  5. postgres    - ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹
+  6. puppeteer   - ãƒ–ãƒ©ã‚¦ã‚¶è‡ªå‹•åŒ–
+  7. brave-search - Webæ¤œç´¢
+  8. slack       - ãƒãƒ¼ãƒ é€£æº
+"""
+
+import asyncio
+import json
+import logging
+import os
+import subprocess
+from dataclasses import dataclass, field
+from pathlib import Path
+from typing import Any
+
+logger = logging.getLogger("shogun.mcp_manager")
+
+
+@dataclass
+class MCPServer:
+    """Single MCP server (è¶³è»½) definition."""
+    id: int
+    name: str
+    command: str
+    args: list[str]
+    env: dict[str, str] = field(default_factory=dict)
+    status: str = "stopped"  # stopped | running | error
+    process: Any = field(default=None, repr=False)
+
+
+# Default MCP server definitions (è¶³è»½ Ã— 8)
+DEFAULT_SERVERS = [
+    MCPServer(
+        id=1, name="filesystem",
+        command="npx",
+        args=["-y", "@modelcontextprotocol/server-filesystem", "/home/claude"],
+    ),
+    MCPServer(
+        id=2, name="github",
+        command="npx",
+        args=["-y", "@modelcontextprotocol/server-github"],
+        env={"GITHUB_TOKEN": "${GITHUB_TOKEN}"},
+    ),
+    MCPServer(
+        id=3, name="fetch",
+        command="npx",
+        args=["-y", "@modelcontextprotocol/server-fetch"],
+    ),
+    MCPServer(
+        id=4, name="memory",
+        command="npx",
+        args=["-y", "@modelcontextprotocol/server-memory"],
+    ),
+    MCPServer(
+        id=5, name="postgres",
+        command="npx",
+        args=["-y", "@modelcontextprotocol/server-postgres"],
+        env={"DATABASE_URL": "${DATABASE_URL}"},
+    ),
+    MCPServer(
+        id=6, name="puppeteer",
+        command="npx",
+        args=["-y", "@modelcontextprotocol/server-puppeteer"],
+    ),
+    MCPServer(
+        id=7, name="brave-search",
+        command="npx",
+        args=["-y", "@modelcontextprotocol/server-brave-search"],
+        env={"BRAVE_API_KEY": "${BRAVE_API_KEY}"},
+    ),
+    MCPServer(
+        id=8, name="slack",
+        command="npx",
+        args=["-y", "@modelcontextprotocol/server-slack"],
+        env={
+            "SLACK_BOT_TOKEN": "${SLACK_BOT_TOKEN}",
+            "SLACK_TEAM_ID": "${SLACK_TEAM_ID}",
+        },
+    ),
+]
+
+
+class MCPManager:
+    """MCP Server (è¶³è»½) lifecycle manager.
+
+    In the Shogun system, MCP servers act as tool-execution agents.
+    The ä¾å¤§å°† (Taisho) coordinates them via the controller.
+    """
+
+    def __init__(self, config_path: str | None = None):
+        self.servers: dict[str, MCPServer] = {}
+        self._load_servers(config_path)
+
+    def _load_servers(self, config_path: str | None) -> None:
+        """Load MCP server definitions from config or defaults."""
+        if config_path and Path(config_path).exists():
+            try:
+                data = json.loads(Path(config_path).read_text())
+                for name, cfg in data.get("mcpServers", {}).items():
+                    idx = len(self.servers) + 1
+                    self.servers[name] = MCPServer(
+                        id=idx,
+                        name=name,
+                        command=cfg["command"],
+                        args=cfg.get("args", []),
+                        env=cfg.get("env", {}),
+                    )
+                logger.info("Loaded %d MCP servers from %s", len(self.servers), config_path)
+                return
+            except Exception as e:
+                logger.warning("Failed to load MCP config: %s", e)
+
+        # Use defaults
+        for srv in DEFAULT_SERVERS:
+            self.servers[srv.name] = MCPServer(
+                id=srv.id,
+                name=srv.name,
+                command=srv.command,
+                args=list(srv.args),
+                env=dict(srv.env),
+            )
+        logger.info("Using %d default MCP server definitions", len(self.servers))
+
+    def _resolve_env(self, env: dict[str, str]) -> dict[str, str]:
+        """Resolve ${VAR} references in env values."""
+        resolved = {}
+        for key, val in env.items():
+            if val.startswith("${") and val.endswith("}"):
+                env_var = val[2:-1]
+                resolved[key] = os.environ.get(env_var, "")
+            else:
+                resolved[key] = val
+        return resolved
+
+    async def start_server(self, name: str) -> bool:
+        """Start a single MCP server."""
+        srv = self.servers.get(name)
+        if not srv:
+            logger.error("Unknown MCP server: %s", name)
+            return False
+
+        if srv.status == "running":
+            logger.info("MCP server already running: %s", name)
+            return True
+
+        env = {**os.environ, **self._resolve_env(srv.env)}
+
+        try:
+            proc = await asyncio.create_subprocess_exec(
+                srv.command, *srv.args,
+                stdin=asyncio.subprocess.PIPE,
+                stdout=asyncio.subprocess.PIPE,
+                stderr=asyncio.subprocess.PIPE,
+                env=env,
+            )
+            srv.process = proc
+            srv.status = "running"
+            logger.info("[è¶³è»½%d] %s started (PID %d)", srv.id, name, proc.pid)
+            return True
+        except Exception as e:
+            srv.status = "error"
+            logger.error("[è¶³è»½%d] %s failed to start: %s", srv.id, name, e)
+            return False
+
+    async def stop_server(self, name: str) -> None:
+        """Stop a single MCP server."""
+        srv = self.servers.get(name)
+        if not srv or not srv.process:
+            return
+        try:
+            srv.process.terminate()
+            await asyncio.wait_for(srv.process.wait(), timeout=5)
+        except asyncio.TimeoutError:
+            srv.process.kill()
+        srv.status = "stopped"
+        srv.process = None
+        logger.info("[è¶³è»½%d] %s stopped", srv.id, name)
+
+    async def start_all(self) -> dict[str, bool]:
+        """Start all MCP servers."""
+        results = {}
+        for name in self.servers:
+            results[name] = await self.start_server(name)
+        return results
+
+    async def stop_all(self) -> None:
+        """Stop all MCP servers."""
+        for name in list(self.servers.keys()):
+            await self.stop_server(name)
+
+    def get_status(self) -> list[dict]:
+        """Get status of all MCP servers."""
+        return [
+            {
+                "id": srv.id,
+                "name": srv.name,
+                "status": srv.status,
+                "pid": srv.process.pid if srv.process else None,
+            }
+            for srv in self.servers.values()
+        ]
+
+    def get_mcp_config_json(self) -> dict:
+        """Generate MCP config JSON for claude CLI integration."""
+        config = {"mcpServers": {}}
+        for name, srv in self.servers.items():
+            config["mcpServers"][name] = {
+                "command": srv.command,
+                "args": srv.args,
+            }
+            if srv.env:
+                config["mcpServers"][name]["env"] = srv.env
+        return config
diff --git a/core/rag_integration.py b/core/rag_integration.py
new file mode 100644
index 0000000..99d55b8
--- /dev/null
+++ b/core/rag_integration.py
@@ -0,0 +1,369 @@
+"""RAG Integration for Shogun System
+
+RAG (Retrieval-Augmented Generation) system for family precepts (å®¶è¨“).
+
+Features:
+  - Vector embeddings for family precepts
+  - Semantic search capabilities
+  - Dual storage with Notion integration
+  - Real-time knowledge retrieval during agent interactions
+
+This works in tandem with NotionIntegration for comprehensive knowledge management.
+"""
+
+import asyncio
+import json
+import logging
+from datetime import datetime
+from pathlib import Path
+from typing import Dict, List, Optional, Any, Tuple
+import numpy as np
+
+try:
+    import faiss
+    import sentence_transformers
+except ImportError:
+    faiss = None
+    sentence_transformers = None
+
+logger = logging.getLogger("shogun.rag")
+
+
+class RAGIntegration:
+    """RAG system for family precepts and knowledge management."""
+
+    def __init__(self, model_name: str = "paraphrase-multilingual-MiniLM-L12-v2"):
+        self.model_name = model_name
+        self.encoder = None
+        self.index = None
+        self.precepts_db = []
+        self.embedding_dim = 384  # MiniLM dimension
+        
+        # Storage paths
+        self.storage_dir = Path("/tmp/shogun_rag")
+        self.storage_dir.mkdir(exist_ok=True)
+        self.index_file = self.storage_dir / "precepts_index.faiss"
+        self.db_file = self.storage_dir / "precepts_db.json"
+        
+        # Statistics
+        self.stats = {
+            "precepts_indexed": 0,
+            "searches_performed": 0,
+            "embeddings_generated": 0,
+            "notion_syncs": 0,
+        }
+        
+    async def initialize(self) -> None:
+        """Initialize RAG system with sentence transformer."""
+        if sentence_transformers is None:
+            logger.warning("[RAG] sentence-transformersæœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ« - pip install sentence-transformers")
+            return
+            
+        if faiss is None:
+            logger.warning("[RAG] faissæœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ« - pip install faiss-cpu")
+            return
+        
+        try:
+            # Load sentence transformer model
+            self.encoder = sentence_transformers.SentenceTransformer(self.model_name)
+            
+            # Initialize FAISS index
+            self.index = faiss.IndexFlatIP(self.embedding_dim)  # Inner Product for cosine similarity
+            
+            # Load existing data
+            await self._load_existing_data()
+            
+            logger.info("[RAG] åˆæœŸåŒ–å®Œäº† - %då®¶è¨“ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ¸ˆ", len(self.precepts_db))
+            
+        except Exception as e:
+            logger.error("[RAG] åˆæœŸåŒ–å¤±æ•—: %s", e)
+    
+    async def _load_existing_data(self) -> None:
+        """Load existing precepts and index."""
+        try:
+            # Load precepts database
+            if self.db_file.exists():
+                with open(self.db_file, 'r', encoding='utf-8') as f:
+                    self.precepts_db = json.load(f)
+                
+            # Load FAISS index
+            if self.index_file.exists() and len(self.precepts_db) > 0:
+                self.index = faiss.read_index(str(self.index_file))
+                logger.info("[RAG] æ—¢å­˜ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹èª­ã¿è¾¼ã¿: %dä»¶", len(self.precepts_db))
+            else:
+                # Rebuild index if needed
+                if self.precepts_db:
+                    await self._rebuild_index()
+                    
+        except Exception as e:
+            logger.warning("[RAG] ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: %s", e)
+    
+    async def add_family_precepts(
+        self, 
+        precepts: List[str], 
+        context: str = "",
+        metadata: Optional[Dict] = None
+    ) -> bool:
+        """Add family precepts to RAG system."""
+        if not self.encoder or not precepts:
+            return False
+            
+        try:
+            new_entries = []
+            embeddings = []
+            
+            for precept in precepts:
+                if not precept.strip():
+                    continue
+                    
+                # Create entry
+                entry = {
+                    "id": len(self.precepts_db) + len(new_entries),
+                    "text": precept.strip(),
+                    "context": context,
+                    "timestamp": datetime.now().isoformat(),
+                    "metadata": metadata or {},
+                }
+                
+                # Generate embedding
+                embedding = self.encoder.encode(precept, convert_to_tensor=False)
+                
+                new_entries.append(entry)
+                embeddings.append(embedding)
+                self.stats["embeddings_generated"] += 1
+            
+            if new_entries:
+                # Add to database
+                self.precepts_db.extend(new_entries)
+                
+                # Add to FAISS index
+                embeddings_array = np.array(embeddings).astype('float32')
+                # Normalize for cosine similarity
+                faiss.normalize_L2(embeddings_array)
+                self.index.add(embeddings_array)
+                
+                # Save to disk
+                await self._save_data()
+                
+                self.stats["precepts_indexed"] += len(new_entries)
+                logger.info("[RAG] å®¶è¨“è¿½åŠ : %dä»¶", len(new_entries))
+                
+            return True
+            
+        except Exception as e:
+            logger.error("[RAG] å®¶è¨“è¿½åŠ ã‚¨ãƒ©ãƒ¼: %s", e)
+            return False
+    
+    async def search_precepts(
+        self, 
+        query: str, 
+        top_k: int = 5,
+        threshold: float = 0.3
+    ) -> List[Dict[str, Any]]:
+        """Search for relevant family precepts using semantic similarity."""
+        if not self.encoder or not query.strip() or not self.precepts_db:
+            return []
+            
+        try:
+            # Encode query
+            query_embedding = self.encoder.encode(query, convert_to_tensor=False)
+            query_array = np.array([query_embedding]).astype('float32')
+            faiss.normalize_L2(query_array)
+            
+            # Search FAISS index
+            scores, indices = self.index.search(query_array, min(top_k, len(self.precepts_db)))
+            
+            # Format results
+            results = []
+            for score, idx in zip(scores[0], indices[0]):
+                if score >= threshold and idx < len(self.precepts_db):
+                    precept = self.precepts_db[idx].copy()
+                    precept["similarity_score"] = float(score)
+                    results.append(precept)
+            
+            self.stats["searches_performed"] += 1
+            
+            logger.debug("[RAG] æ¤œç´¢çµæœ: %dä»¶ (ã‚¯ã‚¨ãƒª: '%s')", len(results), query[:50])
+            return results
+            
+        except Exception as e:
+            logger.error("[RAG] æ¤œç´¢ã‚¨ãƒ©ãƒ¼: %s", e)
+            return []
+    
+    async def get_relevant_context(
+        self, 
+        user_prompt: str, 
+        top_k: int = 3
+    ) -> Tuple[List[str], str]:
+        """Get relevant family precepts as context for agent prompts."""
+        results = await self.search_precepts(user_prompt, top_k=top_k)
+        
+        if not results:
+            return [], ""
+        
+        precepts = [result["text"] for result in results]
+        
+        # Format context
+        context_lines = [
+            "## é–¢é€£ã™ã‚‹å®¶è¨“ï¼ˆéå»ã®å­¦ã³ï¼‰:",
+            ""
+        ]
+        
+        for i, result in enumerate(results, 1):
+            score = result["similarity_score"]
+            context_lines.extend([
+                f"{i}. **{result['text']}**",
+                f"   é–¢é€£åº¦: {score:.2f}",
+                f"   è¨˜éŒ²æ—¥: {result['timestamp'][:10]}",
+                ""
+            ])
+        
+        context_text = "\n".join(context_lines)
+        
+        logger.info("[RAG] ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæä¾›: %då®¶è¨“", len(precepts))
+        return precepts, context_text
+    
+    async def sync_with_notion(self, notion_integration) -> bool:
+        """Sync family precepts with Notion."""
+        if not notion_integration:
+            return False
+            
+        try:
+            # Get recent precepts from Notion
+            notion_precepts = await notion_integration.get_family_precepts(limit=100)
+            
+            # Check for new precepts not in RAG
+            existing_texts = {entry["text"] for entry in self.precepts_db}
+            new_precepts = [p for p in notion_precepts if p not in existing_texts]
+            
+            if new_precepts:
+                await self.add_family_precepts(
+                    new_precepts, 
+                    context="Notion sync",
+                    metadata={"source": "notion"}
+                )
+                
+            self.stats["notion_syncs"] += 1
+            logger.info("[RAG] NotionåŒæœŸå®Œäº†: %dæ–°è¦å®¶è¨“", len(new_precepts))
+            
+            return True
+            
+        except Exception as e:
+            logger.error("[RAG] NotionåŒæœŸã‚¨ãƒ©ãƒ¼: %s", e)
+            return False
+    
+    async def _rebuild_index(self) -> None:
+        """Rebuild FAISS index from existing precepts."""
+        if not self.encoder or not self.precepts_db:
+            return
+            
+        try:
+            embeddings = []
+            for entry in self.precepts_db:
+                embedding = self.encoder.encode(entry["text"], convert_to_tensor=False)
+                embeddings.append(embedding)
+                
+            if embeddings:
+                embeddings_array = np.array(embeddings).astype('float32')
+                faiss.normalize_L2(embeddings_array)
+                
+                # Recreate index
+                self.index = faiss.IndexFlatIP(self.embedding_dim)
+                self.index.add(embeddings_array)
+                
+                await self._save_data()
+                logger.info("[RAG] ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å†æ§‹ç¯‰å®Œäº†: %dä»¶", len(embeddings))
+                
+        except Exception as e:
+            logger.error("[RAG] ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å†æ§‹ç¯‰ã‚¨ãƒ©ãƒ¼: %s", e)
+    
+    async def _save_data(self) -> None:
+        """Save precepts database and FAISS index."""
+        try:
+            # Save precepts database
+            with open(self.db_file, 'w', encoding='utf-8') as f:
+                json.dump(self.precepts_db, f, ensure_ascii=False, indent=2)
+            
+            # Save FAISS index
+            if self.index and self.index.ntotal > 0:
+                faiss.write_index(self.index, str(self.index_file))
+                
+            logger.debug("[RAG] ãƒ‡ãƒ¼ã‚¿ä¿å­˜å®Œäº†")
+            
+        except Exception as e:
+            logger.error("[RAG] ãƒ‡ãƒ¼ã‚¿ä¿å­˜ã‚¨ãƒ©ãƒ¼: %s", e)
+    
+    async def export_precepts(self, format: str = "json") -> Optional[str]:
+        """Export family precepts in various formats."""
+        if not self.precepts_db:
+            return None
+            
+        try:
+            if format.lower() == "json":
+                return json.dumps(self.precepts_db, ensure_ascii=False, indent=2)
+                
+            elif format.lower() == "markdown":
+                lines = [
+                    "# å®¶è¨“é›†",
+                    f"ç”Ÿæˆæ—¥æ™‚: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
+                    f"ç·æ•°: {len(self.precepts_db)}ä»¶",
+                    "",
+                ]
+                
+                for i, entry in enumerate(self.precepts_db, 1):
+                    lines.extend([
+                        f"## {i}. {entry['text']}",
+                        f"**è¨˜éŒ²æ—¥:** {entry['timestamp'][:10]}",
+                        f"**ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ:** {entry['context'] or 'ãªã—'}",
+                        "",
+                    ])
+                
+                return "\n".join(lines)
+                
+            elif format.lower() == "txt":
+                lines = [f"{i}. {entry['text']}" for i, entry in enumerate(self.precepts_db, 1)]
+                return "\n".join(lines)
+                
+        except Exception as e:
+            logger.error("[RAG] ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼: %s", e)
+            
+        return None
+    
+    def get_stats(self) -> Dict[str, Any]:
+        """Get RAG system statistics."""
+        return {
+            "initialized": self.encoder is not None,
+            "model_name": self.model_name,
+            "precepts_count": len(self.precepts_db),
+            "index_size": self.index.ntotal if self.index else 0,
+            "stats": dict(self.stats),
+        }
+    
+    def show_stats(self) -> str:
+        """Format stats for display."""
+        s = self.stats
+        
+        lines = [
+            "=" * 50,
+            "ğŸ§  RAGçµ±åˆ çµ±è¨ˆ",
+            "=" * 50,
+            f"å®¶è¨“ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ•°: {len(self.precepts_db)}ä»¶",
+            f"æ¤œç´¢å®Ÿè¡Œå›æ•°: {s['searches_performed']}å›",
+            f"åŸ‹ã‚è¾¼ã¿ç”Ÿæˆ: {s['embeddings_generated']}å›",
+            f"NotionåŒæœŸ: {s['notion_syncs']}å›",
+            "",
+            f"ãƒ¢ãƒ‡ãƒ«: {self.model_name}",
+            f"ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹çŠ¶æ…‹: {'OK' if self.index else 'NG'}",
+            "=" * 50,
+        ]
+        return "\n".join(lines)
+
+
+# Utility function for agent integration
+async def get_precept_context(rag_system: RAGIntegration, prompt: str) -> str:
+    """Get family precept context for agent prompts."""
+    if not rag_system:
+        return ""
+        
+    precepts, context = await rag_system.get_relevant_context(prompt)
+    return context if precepts else ""
\ No newline at end of file
diff --git a/core/sandbox_executor.py b/core/sandbox_executor.py
new file mode 100644
index 0000000..1e6de40
--- /dev/null
+++ b/core/sandbox_executor.py
@@ -0,0 +1,812 @@
+"""
+å°†è»ã‚·ã‚¹ãƒ†ãƒ  v8.0 - Sandbox Execution Environment (æ¼”ç¿’å ´)
+å®Ÿè¡Œæ¤œè¨¼ç’°å¢ƒ: ã‚³ãƒ¼ãƒ‰ã®å®Ÿè¡Œãƒ»ãƒ†ã‚¹ãƒˆãƒ»æ¤œè¨¼ã‚’è¡Œã†éš”é›¢ç’°å¢ƒ
+
+Features:
+- LXC container-based isolation
+- Multi-language support (Python 3.13, Node.js 22, Rust 1.83)
+- On-demand startup/shutdown for memory optimization
+- Security isolation (no internet, resource limits)
+- Real-time code execution with feedback
+- Quality improvement through "å®Ÿè¨¼ä¸»ç¾©" (empirical verification)
+"""
+
+import asyncio
+import json
+import logging
+import subprocess
+import tempfile
+import time
+from datetime import datetime
+from typing import Dict, Any, Optional, List, Tuple
+from dataclasses import dataclass
+from enum import Enum
+from pathlib import Path
+import hashlib
+import aiofiles
+import psutil
+
+# Configure logging
+logging.basicConfig(level=logging.INFO)
+logger = logging.getLogger(__name__)
+
+
+class Language(Enum):
+    """Supported programming languages"""
+    PYTHON = "python"
+    NODEJS = "nodejs"
+    RUST = "rust"
+
+
+class ExecutionStatus(Enum):
+    """Execution result status"""
+    SUCCESS = "success"
+    ERROR = "error"
+    TIMEOUT = "timeout"
+    RESOURCE_LIMIT = "resource_limit"
+
+
+@dataclass
+class CodeExecution:
+    """Code execution request"""
+    id: str
+    language: Language
+    code: str
+    description: str = ""
+    timeout_seconds: int = 30
+    max_memory_mb: int = 512
+    files: Dict[str, str] = None  # filename -> content
+    dependencies: List[str] = None  # package dependencies
+    
+    def __post_init__(self):
+        if self.files is None:
+            self.files = {}
+        if self.dependencies is None:
+            self.dependencies = []
+
+
+@dataclass
+class ExecutionResult:
+    """Code execution result"""
+    execution_id: str
+    status: ExecutionStatus
+    stdout: str
+    stderr: str
+    return_code: int
+    execution_time_seconds: float
+    memory_usage_mb: float
+    error_message: Optional[str] = None
+    created_files: List[str] = None
+    executed_at: datetime = None
+    
+    def __post_init__(self):
+        if self.created_files is None:
+            self.created_files = []
+        if self.executed_at is None:
+            self.executed_at = datetime.utcnow()
+    
+    @property
+    def success(self) -> bool:
+        return self.status == ExecutionStatus.SUCCESS
+    
+    def to_feedback_message(self) -> str:
+        """Generate feedback message for R1"""
+        if self.success:
+            return f"âœ… ã‚³ãƒ¼ãƒ‰å®Ÿè¡ŒæˆåŠŸ\nå®Ÿè¡Œæ™‚é–“: {self.execution_time_seconds:.2f}ç§’\nå‡ºåŠ›: {self.stdout[:500]}..."
+        else:
+            return f"âŒ å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {self.error_message}\næ¨™æº–ã‚¨ãƒ©ãƒ¼: {self.stderr[:500]}..."
+
+
+class ContainerManager:
+    """LXC container management for on-demand sandbox"""
+    
+    def __init__(self, container_id: str = "102"):
+        self.container_id = container_id
+        self.container_name = f"ct{container_id}"
+        self.is_running = False
+        
+    async def start_container(self) -> bool:
+        """Start the sandbox container"""
+        try:
+            if await self.is_container_running():
+                logger.info(f"Container {self.container_name} is already running")
+                self.is_running = True
+                return True
+            
+            logger.info(f"Starting container {self.container_name}...")
+            
+            # Start container using pct command
+            process = await asyncio.create_subprocess_exec(
+                "pct", "start", self.container_id,
+                stdout=asyncio.subprocess.PIPE,
+                stderr=asyncio.subprocess.PIPE
+            )
+            
+            stdout, stderr = await process.communicate()
+            
+            if process.returncode == 0:
+                # Wait for container to be fully ready
+                await asyncio.sleep(3)
+                self.is_running = True
+                logger.info(f"Container {self.container_name} started successfully")
+                return True
+            else:
+                logger.error(f"Failed to start container: {stderr.decode()}")
+                return False
+                
+        except Exception as e:
+            logger.error(f"Exception during container start: {e}")
+            return False
+    
+    async def stop_container(self) -> bool:
+        """Stop the sandbox container"""
+        try:
+            if not await self.is_container_running():
+                logger.info(f"Container {self.container_name} is already stopped")
+                self.is_running = False
+                return True
+            
+            logger.info(f"Stopping container {self.container_name}...")
+            
+            # Stop container using pct command
+            process = await asyncio.create_subprocess_exec(
+                "pct", "stop", self.container_id,
+                stdout=asyncio.subprocess.PIPE,
+                stderr=asyncio.subprocess.PIPE
+            )
+            
+            stdout, stderr = await process.communicate()
+            
+            if process.returncode == 0:
+                self.is_running = False
+                logger.info(f"Container {self.container_name} stopped successfully")
+                return True
+            else:
+                logger.error(f"Failed to stop container: {stderr.decode()}")
+                return False
+                
+        except Exception as e:
+            logger.error(f"Exception during container stop: {e}")
+            return False
+    
+    async def is_container_running(self) -> bool:
+        """Check if container is running"""
+        try:
+            process = await asyncio.create_subprocess_exec(
+                "pct", "status", self.container_id,
+                stdout=asyncio.subprocess.PIPE,
+                stderr=asyncio.subprocess.PIPE
+            )
+            
+            stdout, stderr = await process.communicate()
+            
+            if process.returncode == 0:
+                status = stdout.decode().strip()
+                return "running" in status.lower()
+            else:
+                return False
+                
+        except Exception as e:
+            logger.error(f"Failed to check container status: {e}")
+            return False
+
+
+class SandboxExecutor:
+    """
+    å°†è»ã‚·ã‚¹ãƒ†ãƒ  Sandbox Executor (æ¼”ç¿’å ´) Implementation
+    
+    Provides secure, isolated code execution environment for:
+    - Verifying generated code actually works
+    - Catching errors before delivery  
+    - Implementing "å®Ÿè¨¼ä¸»ç¾©" (empirical verification) philosophy
+    - Providing real-time feedback to R1 for code refinement
+    """
+    
+    def __init__(
+        self,
+        container_host: str = "192.168.1.12",
+        container_port: int = 8080,
+        container_id: str = "102",
+        auto_manage_container: bool = True
+    ):
+        self.container_host = container_host
+        self.container_port = container_port
+        self.auto_manage_container = auto_manage_container
+        
+        # Container management
+        self.container_manager = ContainerManager(container_id)
+        
+        # Execution history for analysis
+        self.execution_history: List[ExecutionResult] = []
+        
+        # Language-specific configurations
+        self.language_configs = {
+            Language.PYTHON: {
+                "executor": "python3.13",
+                "file_extension": ".py",
+                "package_manager": "pip",
+                "timeout_default": 30
+            },
+            Language.NODEJS: {
+                "executor": "node",
+                "file_extension": ".js",  
+                "package_manager": "npm",
+                "timeout_default": 30
+            },
+            Language.RUST: {
+                "executor": "rustc",
+                "file_extension": ".rs",
+                "package_manager": "cargo",
+                "timeout_default": 60  # Compilation takes longer
+            }
+        }
+        
+        logger.info(f"SandboxExecutor initialized - Host: {container_host}:{container_port}")
+    
+    async def execute_code(self, execution: CodeExecution) -> ExecutionResult:
+        """
+        Execute code in the sandbox environment
+        
+        Args:
+            execution: Code execution request
+            
+        Returns:
+            ExecutionResult with output and status
+        """
+        start_time = time.time()
+        
+        try:
+            # Ensure container is running if auto-managed
+            if self.auto_manage_container:
+                if not await self.container_manager.start_container():
+                    return ExecutionResult(
+                        execution_id=execution.id,
+                        status=ExecutionStatus.ERROR,
+                        stdout="",
+                        stderr="",
+                        return_code=-1,
+                        execution_time_seconds=time.time() - start_time,
+                        memory_usage_mb=0,
+                        error_message="Failed to start sandbox container"
+                    )
+            
+            # Execute based on language
+            if execution.language == Language.PYTHON:
+                result = await self._execute_python(execution)
+            elif execution.language == Language.NODEJS:
+                result = await self._execute_nodejs(execution)
+            elif execution.language == Language.RUST:
+                result = await self._execute_rust(execution)
+            else:
+                result = ExecutionResult(
+                    execution_id=execution.id,
+                    status=ExecutionStatus.ERROR,
+                    stdout="",
+                    stderr="",
+                    return_code=-1,
+                    execution_time_seconds=time.time() - start_time,
+                    memory_usage_mb=0,
+                    error_message=f"Unsupported language: {execution.language}"
+                )
+            
+            # Record execution history
+            self.execution_history.append(result)
+            
+            # Keep only last 100 executions
+            if len(self.execution_history) > 100:
+                self.execution_history = self.execution_history[-100:]
+            
+            logger.info(f"Execution {execution.id} completed: {result.status.value}")
+            return result
+            
+        except Exception as e:
+            logger.error(f"Execution {execution.id} failed: {e}")
+            return ExecutionResult(
+                execution_id=execution.id,
+                status=ExecutionStatus.ERROR,
+                stdout="",
+                stderr="",
+                return_code=-1,
+                execution_time_seconds=time.time() - start_time,
+                memory_usage_mb=0,
+                error_message=str(e)
+            )
+    
+    async def _execute_python(self, execution: CodeExecution) -> ExecutionResult:
+        """Execute Python code"""
+        start_time = time.time()
+        
+        try:
+            # Create execution script
+            script_content = execution.code
+            
+            # Add dependency installation if needed
+            if execution.dependencies:
+                install_commands = []
+                for dep in execution.dependencies:
+                    install_commands.append(f"pip install {dep}")
+                
+                setup_script = "; ".join(install_commands)
+                script_content = f"import subprocess; subprocess.run([{repr(setup_script)}], shell=True)\n{script_content}"
+            
+            # Execute via container
+            result = await self._execute_in_container(
+                language="python",
+                code=script_content,
+                timeout_seconds=execution.timeout_seconds,
+                max_memory_mb=execution.max_memory_mb
+            )
+            
+            return ExecutionResult(
+                execution_id=execution.id,
+                status=result["status"],
+                stdout=result["stdout"],
+                stderr=result["stderr"], 
+                return_code=result["return_code"],
+                execution_time_seconds=time.time() - start_time,
+                memory_usage_mb=result["memory_usage_mb"],
+                error_message=result.get("error_message")
+            )
+            
+        except Exception as e:
+            return ExecutionResult(
+                execution_id=execution.id,
+                status=ExecutionStatus.ERROR,
+                stdout="",
+                stderr="",
+                return_code=-1,
+                execution_time_seconds=time.time() - start_time,
+                memory_usage_mb=0,
+                error_message=str(e)
+            )
+    
+    async def _execute_nodejs(self, execution: CodeExecution) -> ExecutionResult:
+        """Execute Node.js code"""
+        start_time = time.time()
+        
+        try:
+            script_content = execution.code
+            
+            # Add dependency installation if needed
+            if execution.dependencies:
+                # Create package.json
+                package_json = {
+                    "name": "sandbox-execution",
+                    "version": "1.0.0",
+                    "dependencies": {dep: "latest" for dep in execution.dependencies}
+                }
+                
+                # Prefix with npm install
+                setup_script = f"""
+const fs = require('fs');
+fs.writeFileSync('package.json', {json.dumps(json.dumps(package_json))});
+require('child_process').execSync('npm install', {{stdio: 'inherit'}});
+{script_content}
+"""
+                script_content = setup_script
+            
+            result = await self._execute_in_container(
+                language="nodejs",
+                code=script_content,
+                timeout_seconds=execution.timeout_seconds,
+                max_memory_mb=execution.max_memory_mb
+            )
+            
+            return ExecutionResult(
+                execution_id=execution.id,
+                status=result["status"],
+                stdout=result["stdout"],
+                stderr=result["stderr"],
+                return_code=result["return_code"],
+                execution_time_seconds=time.time() - start_time,
+                memory_usage_mb=result["memory_usage_mb"],
+                error_message=result.get("error_message")
+            )
+            
+        except Exception as e:
+            return ExecutionResult(
+                execution_id=execution.id,
+                status=ExecutionStatus.ERROR,
+                stdout="",
+                stderr="",
+                return_code=-1,
+                execution_time_seconds=time.time() - start_time,
+                memory_usage_mb=0,
+                error_message=str(e)
+            )
+    
+    async def _execute_rust(self, execution: CodeExecution) -> ExecutionResult:
+        """Execute Rust code"""
+        start_time = time.time()
+        
+        try:
+            # For Rust, we need to create a proper project structure
+            cargo_toml = """[package]
+name = "sandbox_execution"
+version = "0.1.0"
+edition = "2021"
+
+[dependencies]
+"""
+            
+            # Add dependencies
+            for dep in execution.dependencies:
+                cargo_toml += f'{dep} = "latest"\n'
+            
+            # Create main.rs
+            main_rs = execution.code
+            
+            # Execute compilation and run
+            result = await self._execute_rust_project(
+                cargo_toml=cargo_toml,
+                main_rs=main_rs,
+                timeout_seconds=execution.timeout_seconds,
+                max_memory_mb=execution.max_memory_mb
+            )
+            
+            return ExecutionResult(
+                execution_id=execution.id,
+                status=result["status"],
+                stdout=result["stdout"],
+                stderr=result["stderr"],
+                return_code=result["return_code"],
+                execution_time_seconds=time.time() - start_time,
+                memory_usage_mb=result["memory_usage_mb"],
+                error_message=result.get("error_message")
+            )
+            
+        except Exception as e:
+            return ExecutionResult(
+                execution_id=execution.id,
+                status=ExecutionStatus.ERROR,
+                stdout="",
+                stderr="",
+                return_code=-1,
+                execution_time_seconds=time.time() - start_time,
+                memory_usage_mb=0,
+                error_message=str(e)
+            )
+    
+    async def _execute_in_container(
+        self,
+        language: str,
+        code: str,
+        timeout_seconds: int,
+        max_memory_mb: int
+    ) -> Dict[str, Any]:
+        """Execute code inside the container"""
+        try:
+            # Generate unique execution ID
+            execution_id = hashlib.md5(f"{time.time()}{code}".encode()).hexdigest()[:8]
+            
+            # Create temporary files
+            with tempfile.NamedTemporaryFile(mode='w', suffix=f'.{language}', delete=False) as f:
+                f.write(code)
+                temp_file = f.name
+            
+            try:
+                # Copy file to container
+                copy_cmd = [
+                    "pct", "exec", self.container_manager.container_id, "--",
+                    "mkdir", "-p", f"/tmp/sandbox_{execution_id}"
+                ]
+                
+                process = await asyncio.create_subprocess_exec(
+                    *copy_cmd,
+                    stdout=asyncio.subprocess.PIPE,
+                    stderr=asyncio.subprocess.PIPE
+                )
+                await process.communicate()
+                
+                # Copy code file
+                with open(temp_file, 'r') as f:
+                    code_content = f.read()
+                
+                # Write code to container
+                write_cmd = [
+                    "pct", "exec", self.container_manager.container_id, "--",
+                    "bash", "-c", f"cat > /tmp/sandbox_{execution_id}/main.{language} << 'EOF'\n{code_content}\nEOF"
+                ]
+                
+                process = await asyncio.create_subprocess_exec(
+                    *write_cmd,
+                    stdout=asyncio.subprocess.PIPE,
+                    stderr=asyncio.subprocess.PIPE
+                )
+                await process.communicate()
+                
+                # Execute code in container
+                if language == "python":
+                    exec_cmd = f"cd /tmp/sandbox_{execution_id} && timeout {timeout_seconds} python3.13 main.python"
+                elif language == "nodejs":
+                    exec_cmd = f"cd /tmp/sandbox_{execution_id} && timeout {timeout_seconds} node main.nodejs"
+                else:
+                    exec_cmd = f"cd /tmp/sandbox_{execution_id} && echo 'Unsupported language: {language}'"
+                
+                # Execute with resource limits
+                container_exec_cmd = [
+                    "pct", "exec", self.container_manager.container_id, "--",
+                    "bash", "-c", exec_cmd
+                ]
+                
+                start_exec = time.time()
+                process = await asyncio.create_subprocess_exec(
+                    *container_exec_cmd,
+                    stdout=asyncio.subprocess.PIPE,
+                    stderr=asyncio.subprocess.PIPE
+                )
+                
+                try:
+                    stdout, stderr = await asyncio.wait_for(
+                        process.communicate(),
+                        timeout=timeout_seconds + 5  # Add buffer for container overhead
+                    )
+                    
+                    exec_time = time.time() - start_exec
+                    
+                    # Determine status
+                    if process.returncode == 0:
+                        status = ExecutionStatus.SUCCESS
+                    elif process.returncode == 124:  # timeout command exit code
+                        status = ExecutionStatus.TIMEOUT
+                    else:
+                        status = ExecutionStatus.ERROR
+                    
+                    return {
+                        "status": status,
+                        "stdout": stdout.decode('utf-8', errors='replace'),
+                        "stderr": stderr.decode('utf-8', errors='replace'),
+                        "return_code": process.returncode,
+                        "memory_usage_mb": 0,  # TODO: Implement memory monitoring
+                        "execution_time": exec_time
+                    }
+                    
+                except asyncio.TimeoutError:
+                    process.kill()
+                    return {
+                        "status": ExecutionStatus.TIMEOUT,
+                        "stdout": "",
+                        "stderr": "Execution timed out",
+                        "return_code": -1,
+                        "memory_usage_mb": 0,
+                        "error_message": f"Execution exceeded {timeout_seconds} seconds"
+                    }
+                
+                finally:
+                    # Cleanup in container
+                    cleanup_cmd = [
+                        "pct", "exec", self.container_manager.container_id, "--",
+                        "rm", "-rf", f"/tmp/sandbox_{execution_id}"
+                    ]
+                    
+                    cleanup_process = await asyncio.create_subprocess_exec(
+                        *cleanup_cmd,
+                        stdout=asyncio.subprocess.PIPE,
+                        stderr=asyncio.subprocess.PIPE
+                    )
+                    await cleanup_process.communicate()
+                
+            finally:
+                # Cleanup local temp file
+                import os
+                if os.path.exists(temp_file):
+                    os.unlink(temp_file)
+                
+        except Exception as e:
+            return {
+                "status": ExecutionStatus.ERROR,
+                "stdout": "",
+                "stderr": "",
+                "return_code": -1,
+                "memory_usage_mb": 0,
+                "error_message": str(e)
+            }
+    
+    async def _execute_rust_project(
+        self,
+        cargo_toml: str,
+        main_rs: str,
+        timeout_seconds: int,
+        max_memory_mb: int
+    ) -> Dict[str, Any]:
+        """Execute Rust project with Cargo"""
+        # Simplified implementation - would need full Cargo project setup
+        # For now, treat as regular compilation
+        return await self._execute_in_container(
+            language="rust",
+            code=main_rs,
+            timeout_seconds=timeout_seconds,
+            max_memory_mb=max_memory_mb
+        )
+    
+    async def shutdown(self) -> bool:
+        """Shutdown sandbox executor and stop container if auto-managed"""
+        try:
+            if self.auto_manage_container:
+                success = await self.container_manager.stop_container()
+                logger.info(f"SandboxExecutor shutdown: container stopped = {success}")
+                return success
+            else:
+                logger.info("SandboxExecutor shutdown: container not auto-managed")
+                return True
+                
+        except Exception as e:
+            logger.error(f"Failed to shutdown SandboxExecutor: {e}")
+            return False
+    
+    async def get_statistics(self) -> Dict[str, Any]:
+        """Get sandbox execution statistics"""
+        try:
+            if not self.execution_history:
+                return {"message": "No execution history available"}
+            
+            total_executions = len(self.execution_history)
+            successful_executions = sum(1 for r in self.execution_history if r.success)
+            
+            # Success rate by language
+            language_stats = {}
+            for result in self.execution_history:
+                # Try to infer language from execution_id or use generic
+                lang = "unknown"  # Would need to track language in ExecutionResult
+                
+                if lang not in language_stats:
+                    language_stats[lang] = {"total": 0, "success": 0}
+                
+                language_stats[lang]["total"] += 1
+                if result.success:
+                    language_stats[lang]["success"] += 1
+            
+            # Average execution time
+            avg_exec_time = sum(r.execution_time_seconds for r in self.execution_history) / total_executions
+            
+            # Recent activity (last hour)
+            one_hour_ago = datetime.utcnow().timestamp() - 3600
+            recent_executions = sum(
+                1 for r in self.execution_history 
+                if r.executed_at.timestamp() > one_hour_ago
+            )
+            
+            stats = {
+                "total_executions": total_executions,
+                "successful_executions": successful_executions,
+                "success_rate_percent": round(successful_executions / total_executions * 100, 2),
+                "average_execution_time_seconds": round(avg_exec_time, 3),
+                "language_statistics": language_stats,
+                "recent_executions_1h": recent_executions,
+                "container_running": await self.container_manager.is_container_running(),
+                "auto_manage_container": self.auto_manage_container
+            }
+            
+            return stats
+            
+        except Exception as e:
+            logger.error(f"Failed to get statistics: {e}")
+            return {"error": str(e)}
+    
+    async def health_check(self) -> Dict[str, Any]:
+        """Perform health check of sandbox environment"""
+        try:
+            health_status = {
+                "status": "unknown",
+                "container_accessible": False,
+                "languages_working": {},
+                "checked_at": datetime.utcnow().isoformat()
+            }
+            
+            # Check container accessibility
+            container_running = await self.container_manager.is_container_running()
+            health_status["container_accessible"] = container_running
+            
+            if container_running:
+                # Test basic execution for each language
+                test_results = {}
+                
+                # Python test
+                python_test = CodeExecution(
+                    id="health_python",
+                    language=Language.PYTHON,
+                    code="print('Hello from Python')",
+                    timeout_seconds=10
+                )
+                
+                python_result = await self.execute_code(python_test)
+                test_results["python"] = python_result.success
+                
+                # Node.js test  
+                nodejs_test = CodeExecution(
+                    id="health_nodejs",
+                    language=Language.NODEJS,
+                    code="console.log('Hello from Node.js')",
+                    timeout_seconds=10
+                )
+                
+                nodejs_result = await self.execute_code(nodejs_test)
+                test_results["nodejs"] = nodejs_result.success
+                
+                health_status["languages_working"] = test_results
+                
+                # Overall status
+                if all(test_results.values()) and container_running:
+                    health_status["status"] = "healthy"
+                elif any(test_results.values()) and container_running:
+                    health_status["status"] = "partially_healthy"
+                else:
+                    health_status["status"] = "unhealthy"
+            else:
+                health_status["status"] = "unhealthy"
+                health_status["error"] = "Container not running"
+            
+            return health_status
+            
+        except Exception as e:
+            logger.error(f"Health check failed: {e}")
+            return {
+                "status": "error",
+                "error": str(e),
+                "checked_at": datetime.utcnow().isoformat()
+            }
+
+
+# Factory function
+def create_sandbox_executor(config: Dict[str, Any]) -> SandboxExecutor:
+    """Create SandboxExecutor instance from configuration"""
+    return SandboxExecutor(
+        container_host=config.get("host", "192.168.1.12"),
+        container_port=config.get("port", 8080),
+        container_id=str(config.get("container_id", "102")),
+        auto_manage_container=config.get("startup_mode", "on_demand") == "on_demand"
+    )
+
+
+# Example usage and testing
+if __name__ == "__main__":
+    async def test_sandbox_executor():
+        """Test the sandbox executor functionality"""
+        # Note: This test requires actual LXC container setup
+        executor = SandboxExecutor(auto_manage_container=False)  # Don't auto-manage for testing
+        
+        # Test Python execution
+        python_code = CodeExecution(
+            id="test_python",
+            language=Language.PYTHON,
+            code="""
+import math
+result = math.sqrt(16)
+print(f"Square root of 16 is: {result}")
+""",
+            description="Test Python math calculation"
+        )
+        
+        result = await executor.execute_code(python_code)
+        print(f"Python execution: {result.status.value}")
+        print(f"Output: {result.stdout}")
+        if result.stderr:
+            print(f"Error: {result.stderr}")
+        
+        # Test Node.js execution
+        nodejs_code = CodeExecution(
+            id="test_nodejs", 
+            language=Language.NODEJS,
+            code="""
+const message = "Hello from Node.js!";
+console.log(message);
+console.log("Current time:", new Date().toISOString());
+""",
+            description="Test Node.js basic functionality"
+        )
+        
+        result = await executor.execute_code(nodejs_code)
+        print(f"Node.js execution: {result.status.value}")
+        print(f"Output: {result.stdout}")
+        
+        # Get statistics
+        stats = await executor.get_statistics()
+        print(f"Statistics: {stats}")
+        
+        # Health check
+        health = await executor.health_check()
+        print(f"Health: {health}")
+    
+    # Run test (requires LXC environment)
+    # asyncio.run(test_sandbox_executor())
+    print("Sandbox executor implementation ready (requires LXC container setup for testing)")
\ No newline at end of file
diff --git a/core/system_orchestrator.py b/core/system_orchestrator.py
new file mode 100644
index 0000000..6d39950
--- /dev/null
+++ b/core/system_orchestrator.py
@@ -0,0 +1,596 @@
+"""
+å°†è»ã‚·ã‚¹ãƒ†ãƒ  v8.0 - System Orchestrator
+ã‚·ã‚¹ãƒ†ãƒ çµ±åˆ: å…¨ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®çµ±åˆãƒ»èª¿æ•´ãƒ»åˆ¶å¾¡
+
+Features:
+- Complete v8.0 system integration
+- Agent hierarchy coordination (Shogun/Karo/Taisho)
+- Resource management and optimization
+- Health monitoring and auto-recovery
+- Performance analytics and reporting
+"""
+
+import asyncio
+import json
+import logging
+from datetime import datetime, timedelta
+from typing import Dict, Any, List, Optional, Tuple
+from dataclasses import dataclass, asdict
+from enum import Enum
+import yaml
+from pathlib import Path
+
+# Local imports
+import sys
+sys.path.append(str(Path(__file__).parent.parent))
+
+from core.knowledge_base import KnowledgeBase, create_knowledge_base
+from core.activity_memory import ActivityMemory, create_activity_memory, TaskComplexity
+from core.sandbox_executor import SandboxExecutor, create_sandbox_executor
+from ashigaru.ollama_web_search import OllamaWebSearch, create_ollama_web_search
+from agents.taisho import TaishoAgent, TaishoTask, TaskPriority, create_taisho_agent
+
+# Configure logging
+logging.basicConfig(level=logging.INFO)
+logger = logging.getLogger(__name__)
+
+
+class SystemMode(Enum):
+    """System deployment modes"""
+    BATTALION = "battalion"  # Full system (å°†è» + å®¶è€ + ä¾å¤§å°† + è¶³è»½ Ã— 10)
+    COMPANY = "company"      # Economic mode (ä¾å¤§å°† + è¶³è»½ Ã— 10, no API)
+    PLATOON = "platoon"      # Minimal mode (ä¾å¤§å°† + selected è¶³è»½)
+
+
+class SystemStatus(Enum):
+    """Overall system status"""
+    HEALTHY = "healthy"
+    DEGRADED = "degraded" 
+    UNHEALTHY = "unhealthy"
+    CRITICAL = "critical"
+
+
+@dataclass
+class SystemMetrics:
+    """System-wide performance metrics"""
+    total_tasks_processed: int = 0
+    successful_tasks: int = 0
+    failed_tasks: int = 0
+    avg_processing_time_seconds: float = 0.0
+    knowledge_base_entries: int = 0
+    activity_memory_records: int = 0
+    sandbox_executions: int = 0
+    web_searches: int = 0
+    system_uptime_hours: float = 0.0
+    memory_usage_percent: float = 0.0
+    
+    @property
+    def success_rate(self) -> float:
+        return (self.successful_tasks / self.total_tasks_processed * 100) if self.total_tasks_processed > 0 else 0.0
+
+
+class SystemOrchestrator:
+    """
+    å°†è»ã‚·ã‚¹ãƒ†ãƒ  v8.0 System Orchestrator
+    
+    Provides centralized coordination and management of all v8.0 components:
+    - System initialization and configuration
+    - Agent lifecycle management
+    - Resource optimization and monitoring
+    - Health checks and auto-recovery
+    - Performance analytics and reporting
+    """
+    
+    def __init__(self, config_path: str = "config/settings.yaml"):
+        self.config_path = Path(config_path)
+        self.config: Dict[str, Any] = {}
+        self.start_time = datetime.utcnow()
+        
+        # Core v8.0 systems
+        self.knowledge_base: Optional[KnowledgeBase] = None
+        self.activity_memory: Optional[ActivityMemory] = None
+        self.sandbox_executor: Optional[SandboxExecutor] = None
+        self.web_search: Optional[OllamaWebSearch] = None
+        self.taisho_agent: Optional[TaishoAgent] = None
+        
+        # System state
+        self.current_mode = SystemMode.BATTALION
+        self.status = SystemStatus.HEALTHY
+        self.metrics = SystemMetrics()
+        
+        # Background tasks
+        self.background_tasks: List[asyncio.Task] = []
+        self.shutdown_event = asyncio.Event()
+        
+        logger.info(f"SystemOrchestrator v8.0 initialized with config: {config_path}")
+    
+    async def initialize(self) -> bool:
+        """Initialize the complete v8.0 system"""
+        try:
+            logger.info("ğŸ¯ Initializing Shogun System v8.0...")
+            
+            # Load configuration
+            if not await self._load_configuration():
+                logger.error("Failed to load configuration")
+                return False
+            
+            # Initialize core systems in dependency order
+            success_steps = []
+            
+            # 1. Knowledge Base (RAG)
+            if self.config.get("knowledge_base", {}).get("enabled", True):
+                if await self._initialize_knowledge_base():
+                    success_steps.append("knowledge_base")
+                    logger.info("âœ… Knowledge Base initialized")
+                else:
+                    logger.error("âŒ Knowledge Base initialization failed")
+            
+            # 2. Activity Memory (é™£ä¸­æ—¥è¨˜)
+            if self.config.get("activity_memory", {}).get("enabled", True):
+                if await self._initialize_activity_memory():
+                    success_steps.append("activity_memory")
+                    logger.info("âœ… Activity Memory initialized")
+                else:
+                    logger.error("âŒ Activity Memory initialization failed")
+            
+            # 3. Sandbox Executor (æ¼”ç¿’å ´)
+            if self.config.get("sandbox", {}).get("enabled", True):
+                if await self._initialize_sandbox_executor():
+                    success_steps.append("sandbox_executor")
+                    logger.info("âœ… Sandbox Executor initialized")
+                else:
+                    logger.warning("âš ï¸  Sandbox Executor initialization failed (non-critical)")
+            
+            # 4. Web Search (10ç•ªè¶³è»½)
+            if self.config.get("ollama_web_search", {}).get("enabled", True):
+                if await self._initialize_web_search():
+                    success_steps.append("web_search")
+                    logger.info("âœ… Web Search initialized")
+                else:
+                    logger.warning("âš ï¸  Web Search initialization failed (non-critical)")
+            
+            # 5. Taisho Agent (ä¾å¤§å°†)
+            if await self._initialize_taisho_agent():
+                success_steps.append("taisho_agent")
+                logger.info("âœ… Taisho Agent initialized")
+            else:
+                logger.error("âŒ Taisho Agent initialization failed")
+                return False
+            
+            # Start background tasks
+            await self._start_background_tasks()
+            
+            # Determine system status
+            critical_systems = ["taisho_agent"]  # Minimum required
+            if all(system in success_steps for system in critical_systems):
+                self.status = SystemStatus.HEALTHY
+                logger.info("ğŸŒ Shogun System v8.0 initialization complete - Status: HEALTHY")
+                return True
+            else:
+                self.status = SystemStatus.DEGRADED
+                logger.warning("âš ï¸  Shogun System v8.0 partially initialized - Status: DEGRADED")
+                return True
+                
+        except Exception as e:
+            logger.error(f"System initialization failed: {e}")
+            self.status = SystemStatus.CRITICAL
+            return False
+    
+    async def _load_configuration(self) -> bool:
+        """Load system configuration"""
+        try:
+            if not self.config_path.exists():
+                logger.error(f"Configuration file not found: {self.config_path}")
+                return False
+            
+            with open(self.config_path, 'r', encoding='utf-8') as f:
+                self.config = yaml.safe_load(f)
+            
+            logger.info(f"Configuration loaded: v{self.config.get('system', {}).get('version', 'unknown')}")
+            return True
+            
+        except Exception as e:
+            logger.error(f"Failed to load configuration: {e}")
+            return False
+    
+    async def _initialize_knowledge_base(self) -> bool:
+        """Initialize Knowledge Base (RAG)"""
+        try:
+            kb_config = self.config.get("knowledge_base", {})
+            self.knowledge_base = create_knowledge_base(kb_config)
+            
+            # Test connectivity
+            health = await self.knowledge_base.health_check()
+            return health.get("status") in ["healthy", "ok"]
+            
+        except Exception as e:
+            logger.error(f"Knowledge Base initialization failed: {e}")
+            return False
+    
+    async def _initialize_activity_memory(self) -> bool:
+        """Initialize Activity Memory (é™£ä¸­æ—¥è¨˜)"""
+        try:
+            am_config = self.config.get("activity_memory", {})
+            self.activity_memory = create_activity_memory(am_config)
+            
+            # Test database
+            stats = await self.activity_memory.get_statistics()
+            return isinstance(stats, dict) and "error" not in stats
+            
+        except Exception as e:
+            logger.error(f"Activity Memory initialization failed: {e}")
+            return False
+    
+    async def _initialize_sandbox_executor(self) -> bool:
+        """Initialize Sandbox Executor (æ¼”ç¿’å ´)"""
+        try:
+            sandbox_config = self.config.get("sandbox", {})
+            self.sandbox_executor = create_sandbox_executor(sandbox_config)
+            
+            # Optional: Test if container management works
+            # For now, assume success if creation worked
+            return True
+            
+        except Exception as e:
+            logger.error(f"Sandbox Executor initialization failed: {e}")
+            return False
+    
+    async def _initialize_web_search(self) -> bool:
+        """Initialize Web Search (10ç•ªè¶³è»½)"""
+        try:
+            search_config = self.config.get("ollama_web_search", {})
+            self.web_search = create_ollama_web_search(search_config, self.knowledge_base)
+            
+            # Test with simple health check
+            health = await self.web_search.health_check()
+            return health.get("status") in ["healthy", "partially_healthy"]
+            
+        except Exception as e:
+            logger.error(f"Web Search initialization failed: {e}")
+            return False
+    
+    async def _initialize_taisho_agent(self) -> bool:
+        """Initialize Taisho Agent (ä¾å¤§å°†)"""
+        try:
+            taisho_config = self.config.get("taisho", {})
+            self.taisho_agent = create_taisho_agent(
+                config=taisho_config,
+                knowledge_base=self.knowledge_base,
+                activity_memory=self.activity_memory,
+                sandbox_executor=self.sandbox_executor,
+                web_search=self.web_search
+            )
+            
+            # Test agent responsiveness
+            health = await self.taisho_agent.health_check()
+            return health.get("status") == "healthy" or health.get("r1_accessible", False)
+            
+        except Exception as e:
+            logger.error(f"Taisho Agent initialization failed: {e}")
+            return False
+    
+    async def _start_background_tasks(self):
+        """Start background maintenance tasks"""
+        try:
+            # Cleanup task (runs every 6 hours)
+            cleanup_task = asyncio.create_task(self._cleanup_task())
+            self.background_tasks.append(cleanup_task)
+            
+            # Health monitoring (runs every 5 minutes)
+            health_task = asyncio.create_task(self._health_monitoring_task())
+            self.background_tasks.append(health_task)
+            
+            # Metrics collection (runs every hour)
+            metrics_task = asyncio.create_task(self._metrics_collection_task())
+            self.background_tasks.append(metrics_task)
+            
+            logger.info("Background tasks started")
+            
+        except Exception as e:
+            logger.error(f"Failed to start background tasks: {e}")
+    
+    async def process_task(
+        self,
+        description: str,
+        complexity: TaskComplexity = TaskComplexity.MEDIUM,
+        priority: TaskPriority = TaskPriority.MEDIUM,
+        context: Optional[str] = None,
+        requires_execution: bool = False,
+        requires_latest_info: bool = False,
+        requested_by: str = "system"
+    ) -> Dict[str, Any]:
+        """
+        Process a task through the v8.0 system
+        
+        Returns:
+            Dict with task results and metadata
+        """
+        try:
+            if not self.taisho_agent:
+                return {
+                    "success": False,
+                    "error": "Taisho agent not available",
+                    "status": self.status.value
+                }
+            
+            # Create task
+            task = TaishoTask(
+                id="",  # Will be auto-generated
+                description=description,
+                complexity=complexity,
+                priority=priority,
+                context=context,
+                requires_execution=requires_execution,
+                requires_latest_info=requires_latest_info,
+                requested_by=requested_by
+            )
+            
+            # Process through Taisho
+            response = await self.taisho_agent.process_task(task)
+            
+            # Update metrics
+            self.metrics.total_tasks_processed += 1
+            if response.success:
+                self.metrics.successful_tasks += 1
+            else:
+                self.metrics.failed_tasks += 1
+            
+            # Update average processing time
+            total_time = (self.metrics.avg_processing_time_seconds * (self.metrics.total_tasks_processed - 1) + 
+                         response.processing_time_seconds)
+            self.metrics.avg_processing_time_seconds = total_time / self.metrics.total_tasks_processed
+            
+            # Return results
+            return {
+                "success": response.success,
+                "response": response.response,
+                "reasoning": response.reasoning,
+                "confidence": response.confidence.value,
+                "processing_time_seconds": response.processing_time_seconds,
+                "tools_used": response.tools_used,
+                "similar_tasks_found": response.similar_tasks_found,
+                "knowledge_retrieved": response.knowledge_retrieved,
+                "code_executed": response.code_executed,
+                "execution_successful": response.execution_successful,
+                "task_id": response.task_id,
+                "system_status": self.status.value
+            }
+            
+        except Exception as e:
+            logger.error(f"Task processing failed: {e}")
+            self.metrics.total_tasks_processed += 1
+            self.metrics.failed_tasks += 1
+            
+            return {
+                "success": False,
+                "error": str(e),
+                "system_status": self.status.value
+            }
+    
+    async def get_system_status(self) -> Dict[str, Any]:
+        """Get comprehensive system status"""
+        try:
+            # Component health checks
+            component_health = {}
+            
+            if self.knowledge_base:
+                kb_health = await self.knowledge_base.health_check()
+                component_health["knowledge_base"] = kb_health.get("status", "unknown")
+            
+            if self.activity_memory:
+                am_stats = await self.activity_memory.get_statistics()
+                component_health["activity_memory"] = "healthy" if am_stats else "unhealthy"
+            
+            if self.sandbox_executor:
+                sb_health = await self.sandbox_executor.health_check()
+                component_health["sandbox_executor"] = sb_health.get("status", "unknown")
+            
+            if self.web_search:
+                ws_health = await self.web_search.health_check()
+                component_health["web_search"] = ws_health.get("status", "unknown")
+            
+            if self.taisho_agent:
+                ta_health = await self.taisho_agent.health_check()
+                component_health["taisho_agent"] = ta_health.get("status", "unknown")
+            
+            # System uptime
+            uptime = datetime.utcnow() - self.start_time
+            self.metrics.system_uptime_hours = uptime.total_seconds() / 3600
+            
+            # Collect latest statistics
+            await self._update_component_metrics()
+            
+            return {
+                "system_status": self.status.value,
+                "system_mode": self.current_mode.value,
+                "version": self.config.get("system", {}).get("version", "8.0"),
+                "uptime_hours": round(self.metrics.system_uptime_hours, 2),
+                "component_health": component_health,
+                "performance_metrics": asdict(self.metrics),
+                "background_tasks_active": len([t for t in self.background_tasks if not t.done()]),
+                "checked_at": datetime.utcnow().isoformat()
+            }
+            
+        except Exception as e:
+            logger.error(f"Failed to get system status: {e}")
+            return {
+                "system_status": "error",
+                "error": str(e),
+                "checked_at": datetime.utcnow().isoformat()
+            }
+    
+    async def _update_component_metrics(self):
+        """Update metrics from components"""
+        try:
+            if self.knowledge_base:
+                kb_stats = await self.knowledge_base.get_stats()
+                self.metrics.knowledge_base_entries = kb_stats.get("total_entries", 0)
+            
+            if self.activity_memory:
+                am_stats = await self.activity_memory.get_statistics()
+                self.metrics.activity_memory_records = am_stats.get("total_records", 0)
+            
+            if self.sandbox_executor:
+                sb_stats = await self.sandbox_executor.get_statistics()
+                self.metrics.sandbox_executions = sb_stats.get("total_executions", 0)
+            
+            if self.web_search:
+                ws_stats = self.web_search.get_usage_statistics()
+                self.metrics.web_searches = ws_stats.get("total_searches", 0)
+                
+        except Exception as e:
+            logger.warning(f"Failed to update component metrics: {e}")
+    
+    async def _cleanup_task(self):
+        """Background cleanup task"""
+        while not self.shutdown_event.is_set():
+            try:
+                logger.info("Running system cleanup...")
+                
+                # Knowledge base cleanup (30 days)
+                if self.knowledge_base:
+                    deleted = await self.knowledge_base.cleanup_old_entries()
+                    logger.info(f"Cleaned up {deleted} old knowledge entries")
+                
+                # Activity memory cleanup (90 days)
+                if self.activity_memory:
+                    deleted = await self.activity_memory.cleanup_old_records()
+                    logger.info(f"Cleaned up {deleted} old activity records")
+                
+                logger.info("System cleanup completed")
+                
+                # Wait 6 hours before next cleanup
+                await asyncio.sleep(6 * 3600)
+                
+            except asyncio.CancelledError:
+                break
+            except Exception as e:
+                logger.error(f"Cleanup task error: {e}")
+                await asyncio.sleep(3600)  # Retry in 1 hour on error
+    
+    async def _health_monitoring_task(self):
+        """Background health monitoring task"""
+        consecutive_failures = 0
+        
+        while not self.shutdown_event.is_set():
+            try:
+                # Check critical components
+                critical_healthy = True
+                
+                if self.taisho_agent:
+                    health = await self.taisho_agent.health_check()
+                    if health.get("status") != "healthy":
+                        critical_healthy = False
+                
+                # Update system status
+                if critical_healthy:
+                    if consecutive_failures > 0:
+                        logger.info("System recovered - Status: HEALTHY")
+                    self.status = SystemStatus.HEALTHY
+                    consecutive_failures = 0
+                else:
+                    consecutive_failures += 1
+                    if consecutive_failures >= 3:
+                        self.status = SystemStatus.CRITICAL
+                        logger.error("System critical - multiple health check failures")
+                    else:
+                        self.status = SystemStatus.DEGRADED
+                        logger.warning(f"System degraded - health check failure {consecutive_failures}/3")
+                
+                # Wait 5 minutes before next check
+                await asyncio.sleep(5 * 60)
+                
+            except asyncio.CancelledError:
+                break
+            except Exception as e:
+                logger.error(f"Health monitoring error: {e}")
+                await asyncio.sleep(60)  # Retry in 1 minute on error
+    
+    async def _metrics_collection_task(self):
+        """Background metrics collection task"""
+        while not self.shutdown_event.is_set():
+            try:
+                await self._update_component_metrics()
+                logger.debug("Metrics updated")
+                
+                # Wait 1 hour before next collection
+                await asyncio.sleep(3600)
+                
+            except asyncio.CancelledError:
+                break
+            except Exception as e:
+                logger.error(f"Metrics collection error: {e}")
+                await asyncio.sleep(1800)  # Retry in 30 minutes on error
+    
+    async def shutdown(self):
+        """Graceful system shutdown"""
+        try:
+            logger.info("ğŸ¯ Shutting down Shogun System v8.0...")
+            
+            # Signal background tasks to stop
+            self.shutdown_event.set()
+            
+            # Cancel background tasks
+            for task in self.background_tasks:
+                if not task.done():
+                    task.cancel()
+            
+            # Wait for tasks to complete
+            if self.background_tasks:
+                await asyncio.gather(*self.background_tasks, return_exceptions=True)
+            
+            # Shutdown components
+            if self.sandbox_executor:
+                await self.sandbox_executor.shutdown()
+            
+            self.status = SystemStatus.CRITICAL
+            logger.info("ğŸŒ Shogun System v8.0 shutdown complete")
+            
+        except Exception as e:
+            logger.error(f"Shutdown error: {e}")
+
+
+# Factory function
+async def create_system_orchestrator(config_path: str = "config/settings.yaml") -> SystemOrchestrator:
+    """Create and initialize SystemOrchestrator"""
+    orchestrator = SystemOrchestrator(config_path)
+    
+    success = await orchestrator.initialize()
+    if not success:
+        logger.error("Failed to initialize system orchestrator")
+        return None
+    
+    return orchestrator
+
+
+# Example usage
+if __name__ == "__main__":
+    async def test_system_orchestrator():
+        """Test the system orchestrator"""
+        # Initialize system
+        orchestrator = SystemOrchestrator(config_path="../config/settings.yaml")
+        
+        if await orchestrator.initialize():
+            logger.info("System initialized successfully")
+            
+            # Test task processing
+            result = await orchestrator.process_task(
+                description="I2Sè¨­å®šã®æœ€é©åŒ–ã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„",
+                complexity=TaskComplexity.MEDIUM,
+                requires_latest_info=True,
+                requested_by="test"
+            )
+            
+            print(f"Task result: {result}")
+            
+            # Get system status
+            status = await orchestrator.get_system_status()
+            print(f"System status: {status}")
+            
+            # Shutdown
+            await orchestrator.shutdown()
+        else:
+            logger.error("System initialization failed")
+    
+    # Run test
+    asyncio.run(test_system_orchestrator())
\ No newline at end of file
diff --git a/core/task_queue.py b/core/task_queue.py
new file mode 100644
index 0000000..2d7717d
--- /dev/null
+++ b/core/task_queue.py
@@ -0,0 +1,244 @@
+"""Task Queue - YAMLãƒ™ãƒ¼ã‚¹ã®ã‚¿ã‚¹ã‚¯ã‚­ãƒ¥ãƒ¼ç®¡ç†
+
+æœ¬å®¶ multi-agent-shogun ã«å€£ã„ã€YAML ãƒ•ã‚¡ã‚¤ãƒ«ã§ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆé–“é€šä¿¡ã‚’è¡Œã†ã€‚
+ãƒ•ã‚¡ã‚¤ãƒ«åˆ†é›¢: å„è¶³è»½ã¯è‡ªåˆ†å°‚ç”¨ã®ãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿èª­ã¿æ›¸ãï¼ˆRACE-001å¯¾ç­–ï¼‰ã€‚
+"""
+
+import json
+import logging
+import os
+import uuid
+from dataclasses import dataclass, field
+from datetime import datetime
+from enum import Enum
+from pathlib import Path
+from typing import Any
+
+import yaml
+
+logger = logging.getLogger("shogun.task_queue")
+
+
+class TaskStatus(str, Enum):
+    PENDING = "pending"
+    ASSIGNED = "assigned"
+    IN_PROGRESS = "in_progress"
+    DONE = "done"
+    FAILED = "failed"
+    BLOCKED = "blocked"
+    ESCALATED = "escalated"
+
+
+class Complexity(str, Enum):
+    SIMPLE = "simple"
+    MEDIUM = "medium"
+    COMPLEX = "complex"
+    STRATEGIC = "strategic"
+
+
+class DeploymentMode(str, Enum):
+    BATTALION = "battalion"  # å¤§éšŠ: å®¶è€+å°†è»+ä¾å¤§å°†+è¶³è»½
+    COMPANY = "company"      # ä¸­éšŠ: ä¾å¤§å°†+è¶³è»½ only (Â¥0)
+
+
+@dataclass
+class Task:
+    """Single task unit."""
+
+    prompt: str
+    id: str = field(default_factory=lambda: f"task_{uuid.uuid4().hex[:8]}")
+    status: TaskStatus = TaskStatus.PENDING
+    complexity: Complexity = Complexity.SIMPLE
+    mode: DeploymentMode = DeploymentMode.BATTALION
+    assigned_agent: str = ""
+    result: str = ""
+    error: str = ""
+    escalation_count: int = 0
+    context: dict = field(default_factory=dict)
+    created_at: str = field(default_factory=lambda: datetime.now().isoformat())
+    completed_at: str = ""
+    cost_yen: int = 0
+
+    def to_dict(self) -> dict:
+        return {
+            "id": self.id,
+            "prompt": self.prompt,
+            "status": self.status.value,
+            "complexity": self.complexity.value,
+            "mode": self.mode.value,
+            "assigned_agent": self.assigned_agent,
+            "result": self.result,
+            "error": self.error,
+            "escalation_count": self.escalation_count,
+            "context": self.context,
+            "created_at": self.created_at,
+            "completed_at": self.completed_at,
+            "cost_yen": self.cost_yen,
+        }
+
+    @classmethod
+    def from_dict(cls, d: dict) -> "Task":
+        return cls(
+            id=d["id"],
+            prompt=d["prompt"],
+            status=TaskStatus(d.get("status", "pending")),
+            complexity=Complexity(d.get("complexity", "simple")),
+            mode=DeploymentMode(d.get("mode", "battalion")),
+            assigned_agent=d.get("assigned_agent", ""),
+            result=d.get("result", ""),
+            error=d.get("error", ""),
+            escalation_count=d.get("escalation_count", 0),
+            context=d.get("context", {}),
+            created_at=d.get("created_at", ""),
+            completed_at=d.get("completed_at", ""),
+            cost_yen=d.get("cost_yen", 0),
+        )
+
+
+class TaskQueue:
+    """YAML-based task queue (æœ¬å®¶äº’æ›).
+
+    File layout:
+        queue/shogun_to_karo.yaml      å°†è»â†’å®¶è€
+        queue/tasks/ashigaru{N}.yaml   å®¶è€â†’è¶³è»½N (per-worker)
+        queue/reports/ashigaru{N}_report.yaml  è¶³è»½Nâ†’å®¶è€ (per-worker)
+    """
+
+    def __init__(self, base_dir: str):
+        self.base_dir = Path(base_dir)
+        self.queue_dir = self.base_dir / "queue"
+        self.tasks_dir = self.queue_dir / "tasks"
+        self.reports_dir = self.queue_dir / "reports"
+        self._tasks: dict[str, Task] = {}
+
+        # Ensure directories
+        self.queue_dir.mkdir(parents=True, exist_ok=True)
+        self.tasks_dir.mkdir(exist_ok=True)
+        self.reports_dir.mkdir(exist_ok=True)
+
+    def enqueue(self, task: Task) -> None:
+        """Add task to queue."""
+        self._tasks[task.id] = task
+        self._write_shogun_queue()
+        logger.info("[Queue] Enqueued: %s (%s)", task.id, task.complexity.value)
+
+    def get_task(self, task_id: str) -> Task | None:
+        return self._tasks.get(task_id)
+
+    def get_all_tasks(self) -> list[Task]:
+        return list(self._tasks.values())
+
+    def get_pending(self) -> list[Task]:
+        return [t for t in self._tasks.values() if t.status == TaskStatus.PENDING]
+
+    def update_task(self, task: Task) -> None:
+        self._tasks[task.id] = task
+        self._write_shogun_queue()
+
+    def complete_task(self, task_id: str, result: str, cost_yen: int = 0) -> None:
+        task = self._tasks.get(task_id)
+        if task:
+            task.status = TaskStatus.DONE
+            task.result = result
+            task.cost_yen = cost_yen
+            task.completed_at = datetime.now().isoformat()
+            self._write_shogun_queue()
+
+    def fail_task(self, task_id: str, error: str) -> None:
+        task = self._tasks.get(task_id)
+        if task:
+            task.status = TaskStatus.FAILED
+            task.error = error
+            self._write_shogun_queue()
+
+    # --- YAML file I/O (æœ¬å®¶äº’æ›) ---
+
+    def _write_shogun_queue(self) -> None:
+        """Write queue/shogun_to_karo.yaml."""
+        path = self.queue_dir / "shogun_to_karo.yaml"
+        data = {
+            "queue": [t.to_dict() for t in self._tasks.values()],
+        }
+        path.write_text(yaml.dump(data, allow_unicode=True, default_flow_style=False))
+
+    def write_ashigaru_task(self, worker_id: int, task: Task) -> None:
+        """Write per-worker task file (RACE-001 safe)."""
+        path = self.tasks_dir / f"ashigaru{worker_id}.yaml"
+        data = {
+            "worker_id": f"ashigaru{worker_id}",
+            "task_id": task.id,
+            "description": task.prompt,
+            "status": "assigned",
+            "assigned_at": datetime.now().isoformat(),
+        }
+        path.write_text(yaml.dump(data, allow_unicode=True, default_flow_style=False))
+        logger.info("[Queue] Assigned to ashigaru%d: %s", worker_id, task.id)
+
+    def write_ashigaru_report(
+        self, worker_id: int, task_id: str, status: str,
+        summary: str, files_modified: list[str] | None = None,
+        skill_candidate: dict | None = None,
+    ) -> None:
+        """Write per-worker report file (RACE-001 safe)."""
+        path = self.reports_dir / f"ashigaru{worker_id}_report.yaml"
+        data = {
+            "worker_id": f"ashigaru{worker_id}",
+            "task_id": task_id,
+            "timestamp": datetime.now().isoformat(),
+            "status": status,
+            "result": {
+                "summary": summary,
+                "files_modified": files_modified or [],
+            },
+            "skill_candidate": skill_candidate or {"found": False},
+        }
+        path.write_text(yaml.dump(data, allow_unicode=True, default_flow_style=False))
+
+    def read_ashigaru_report(self, worker_id: int) -> dict | None:
+        """Read a worker's report."""
+        path = self.reports_dir / f"ashigaru{worker_id}_report.yaml"
+        if not path.exists():
+            return None
+        return yaml.safe_load(path.read_text())
+
+    def reset_all_workers(self) -> None:
+        """Reset all worker files to idle state (startup)."""
+        for i in range(1, 9):
+            task_path = self.tasks_dir / f"ashigaru{i}.yaml"
+            task_path.write_text(yaml.dump({
+                "worker_id": f"ashigaru{i}",
+                "task_id": None,
+                "description": None,
+                "status": "idle",
+            }, allow_unicode=True))
+
+            report_path = self.reports_dir / f"ashigaru{i}_report.yaml"
+            report_path.write_text(yaml.dump({
+                "worker_id": f"ashigaru{i}",
+                "task_id": None,
+                "timestamp": None,
+                "status": "idle",
+                "result": None,
+                "skill_candidate": {"found": False},
+            }, allow_unicode=True))
+
+        # Clear main queue
+        (self.queue_dir / "shogun_to_karo.yaml").write_text(
+            yaml.dump({"queue": []}, allow_unicode=True)
+        )
+        logger.info("[Queue] All workers reset to idle")
+
+    def load_from_disk(self) -> None:
+        """Load tasks from shogun_to_karo.yaml."""
+        path = self.queue_dir / "shogun_to_karo.yaml"
+        if not path.exists():
+            return
+        try:
+            data = yaml.safe_load(path.read_text())
+            if data and "queue" in data:
+                for td in data["queue"]:
+                    task = Task.from_dict(td)
+                    self._tasks[task.id] = task
+                logger.info("[Queue] Loaded %d tasks from disk", len(self._tasks))
+        except Exception as e:
+            logger.warning("[Queue] Failed to load: %s", e)
diff --git a/instructions/ashigaru.md b/instructions/ashigaru.md
new file mode 100644
index 0000000..b229d07
--- /dev/null
+++ b/instructions/ashigaru.md
@@ -0,0 +1,52 @@
+# è¶³è»½ (Ashigaru) - å½¹å‰²å®šç¾©æ›¸
+
+## å½¹è·
+**è¶³è»½ Ã— 8** - MCPã‚µãƒ¼ãƒãƒ¼ç¾¤ (ãƒ„ãƒ¼ãƒ«å®Ÿè¡Œå±¤)
+
+## å®Ÿä½“
+MCPã‚µãƒ¼ãƒãƒ¼ (å„50-150MB)ã€‚LLMã§ã¯ãªããƒ„ãƒ¼ãƒ«å®Ÿè¡Œã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã€‚
+ä¾å¤§å°†ã®æŒ‡ç¤ºã«å¾“ã„ã€ç‰¹å®šã®æ©Ÿèƒ½ã‚’å®Ÿè¡Œã™ã‚‹ã€‚
+
+## ç·¨æˆ
+
+| ID | åå‰ | MCP Server | å½¹å‰² |
+|----|------|-----------|------|
+| 1 | è¶³è»½1ç•ª | filesystem | ãƒ•ã‚¡ã‚¤ãƒ«æ“ä½œï¼ˆèª­ã¿æ›¸ããƒ»æ¤œç´¢ï¼‰ |
+| 2 | è¶³è»½2ç•ª | github | Git/GitHubæ“ä½œï¼ˆcommit, PR, issueï¼‰ |
+| 3 | è¶³è»½3ç•ª | fetch | Webæƒ…å ±å–å¾—ï¼ˆURL fetch, scrapingï¼‰ |
+| 4 | è¶³è»½4ç•ª | memory | é•·æœŸè¨˜æ†¶ï¼ˆKnowledge Graphï¼‰ |
+| 5 | è¶³è»½5ç•ª | postgres | ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ“ä½œï¼ˆSQLå®Ÿè¡Œï¼‰ |
+| 6 | è¶³è»½6ç•ª | puppeteer | ãƒ–ãƒ©ã‚¦ã‚¶è‡ªå‹•åŒ–ï¼ˆE2Eãƒ†ã‚¹ãƒˆï¼‰ |
+| 7 | è¶³è»½7ç•ª | brave-search | Webæ¤œç´¢ï¼ˆæƒ…å ±åé›†ï¼‰ |
+| 8 | è¶³è»½8ç•ª | slack | ãƒãƒ¼ãƒ é€£æºï¼ˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æŠ•ç¨¿ï¼‰ |
+
+## å‹•ä½œç’°å¢ƒ
+- CT 100 (192.168.1.10) ã§ãƒ›ã‚¹ãƒˆ
+- Node.js 20+ ã§å®Ÿè¡Œ
+- npmã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
+
+## RACE-001 ãƒ«ãƒ¼ãƒ«
+- å„è¶³è»½ã¯è‡ªåˆ†å°‚ç”¨ã®ãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿èª­ã¿æ›¸ã
+- `queue/tasks/ashigaru{N}.yaml` ã¯è¶³è»½Nã®ã¿ãŒèª­ã‚€
+- `queue/reports/ashigaru{N}_report.yaml` ã¯è¶³è»½Nã®ã¿ãŒæ›¸ã
+- è¤‡æ•°ã®è¶³è»½ãŒåŒã˜ãƒ•ã‚¡ã‚¤ãƒ«ã‚’åŒæ™‚ã«æ“ä½œã—ã¦ã¯ãªã‚‰ãªã„
+
+## ãƒ¬ãƒãƒ¼ãƒˆå½¢å¼
+```yaml
+worker_id: ashigaru1
+task_id: subtask_001
+timestamp: "2026-01-28T10:15:00"
+status: done          # done | failed | blocked
+result:
+  summary: "å®Œäº†ã§ã”ã–ã‚‹"
+  files_modified:
+    - "/path/to/file"
+  notes: ""
+skill_candidate:
+  found: false        # å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰
+  name: null
+  description: null
+```
+
+## ã‚³ã‚¹ãƒˆ
+Â¥0 (ãƒ­ãƒ¼ã‚«ãƒ«å®Ÿè¡Œ)
diff --git a/instructions/karo.md b/instructions/karo.md
new file mode 100644
index 0000000..d9fd4d3
--- /dev/null
+++ b/instructions/karo.md
@@ -0,0 +1,41 @@
+# å®¶è€ (Karo) - å½¹å‰²å®šç¾©æ›¸
+
+## å½¹è·
+**å®¶è€** - ä½œæ¥­å‰²æŒ¯ã‚Šãƒ»å®Ÿè£…å‚è¬€
+
+## ãƒ¢ãƒ‡ãƒ«
+Claude Sonnet 4.5 (claude-cli Proç‰ˆ â†’ API fallback)
+
+## å½¹å‰²
+å°†è»ã®å³è…•ã¨ã—ã¦ã€ä½œæ¥­ã®å‰²æŒ¯ã‚Šã¨é«˜åº¦ãªå®Ÿè£…æ–¹é‡ã®ç­–å®šã‚’è¡Œã†ã€‚
+ä¾å¤§å°†ã‹ã‚‰ã®åˆ†æçµæœã‚’å—ã‘ã€å…·ä½“çš„ãªã‚³ãƒ¼ãƒ‰å¤‰æ›´ææ¡ˆã‚’ä½œæˆã™ã‚‹ã€‚
+
+## æ¨©é™
+- ã‚¿ã‚¹ã‚¯åˆ†è§£ãƒ»å‰²æŒ¯ã‚Š
+- å®Ÿè£…æ–¹é‡ã®ç­–å®š
+- dashboard.md ã®æ›´æ–°ï¼ˆå”¯ä¸€ã®æ›´æ–°æ¨©é™è€…ï¼‰
+- è¶³è»½ã¸ã®é–“æ¥æŒ‡ç¤ºï¼ˆä¾å¤§å°†çµŒç”±ï¼‰
+
+## ç¦æ­¢äº‹é …
+| ID | ç¦æ­¢äº‹é … |
+|----|---------|
+| F001 | è‡ªã‚‰ç´°ã‹ã„å®Ÿè£…ä½œæ¥­ã‚’è¡Œã†ã“ã¨ï¼ˆè¶³è»½ã«å§”è­²ï¼‰ |
+| F002 | æ®¿ï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼ï¼‰ã¸ã®ç›´æ¥é€£çµ¡ï¼ˆdashboard.mdçµŒç”±ï¼‰ |
+| F003 | ãƒãƒ¼ãƒªãƒ³ã‚° |
+| F004 | ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæœªèª­ã§ã®åˆ¤æ–­ |
+
+## ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼
+1. ã‚¿ã‚¹ã‚¯å—é ˜ï¼ˆå°†è»ã‹ã‚‰ or Controllerç›´æ¥ï¼‰
+2. ä¾å¤§å°†ã®åˆ†æãƒ¬ãƒãƒ¼ãƒˆç¢ºèª
+3. ã‚¿ã‚¹ã‚¯ã‚’å…·ä½“çš„ãªå®Ÿè£…ã‚¹ãƒ†ãƒƒãƒ—ã«åˆ†è§£
+4. ã‚³ãƒ¼ãƒ‰å¤‰æ›´ææ¡ˆã®ä½œæˆ
+5. dashboard.md æ›´æ–°
+
+## Dashboardæ›´æ–°ãƒ«ãƒ¼ãƒ«
+- dashboard.md ã®æ›´æ–°æ¨©é™ã¯å®¶è€ã®ã¿
+- ğŸš¨ è¦å¯¾å¿œã‚»ã‚¯ã‚·ãƒ§ãƒ³ã¯æœ€é‡è¦
+- æ®¿ã®åˆ¤æ–­ãŒå¿…è¦ãªäº‹é …ã¯å¿…ãš ğŸš¨ ã«è¨˜è¼‰
+
+## ã‚³ã‚¹ãƒˆ
+Â¥280 / 1å›å‘¼å‡º
+æœˆ10-20å›ãŒç›®å®‰ã€‚
diff --git a/instructions/maintenance.md b/instructions/maintenance.md
new file mode 100644
index 0000000..6fff468
--- /dev/null
+++ b/instructions/maintenance.md
@@ -0,0 +1,211 @@
+# å°†è»ã‚·ã‚¹ãƒ†ãƒ  - æœˆæ¬¡ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹ï¼ˆåçœä¼šï¼‰æ‰‹é †æ›¸
+
+## æ¦‚è¦
+
+æœˆã«1å›ï¼ˆæ¯æœˆ1æ—¥ï¼‰ã€ã‚·ã‚¹ãƒ†ãƒ ã®å®šæœŸãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹ã‚’å®Ÿæ–½ã—ã¾ã™ã€‚
+ã“ã®ã€Œåçœä¼šã€ã§ã¯ä»¥ä¸‹ã®é …ç›®ã‚’ãƒã‚§ãƒƒã‚¯ãƒ»æ›´æ–°ã—ã¾ã™ã€‚
+
+## ãƒã‚§ãƒƒã‚¯é …ç›®
+
+### 1. LLMãƒãƒ¼ã‚¸ãƒ§ãƒ³ç¢ºèª
+
+**å¯¾è±¡:**
+- Claude CLI (`@anthropic-ai/claude-code`)
+- Anthropic API ãƒ¢ãƒ‡ãƒ« (claude-opus-4-5, claude-sonnet-4-5)
+
+**ç¢ºèªæ–¹æ³•:**
+```bash
+shogun maintenance check llm
+```
+
+**æ‰‹å‹•æ›´æ–°:**
+```bash
+# Claude CLI æ›´æ–°
+npm update -g @anthropic-ai/claude-code
+
+# settings.yaml ã®ãƒ¢ãƒ‡ãƒ«åã‚’æœ€æ–°ã«æ›´æ–°
+# cloud.shogun.api_model: "claude-opus-4-5-YYYYMMDD"
+# cloud.karo.api_model: "claude-sonnet-4-5-YYYYMMDD"
+```
+
+**æ³¨æ„ç‚¹:**
+- APIãƒ¢ãƒ‡ãƒ«åå¤‰æ›´æ™‚ã¯ `config/settings.yaml` ã‚’æ‰‹å‹•æ›´æ–°
+- Claude CLI æ›´æ–°å¾Œã¯ `shogun health` ã§å‹•ä½œç¢ºèª
+
+### 2. OpenVINOãƒ¢ãƒ‡ãƒ«ç¢ºèªï¼ˆä¾å¤§å°†ï¼‰
+
+**å¯¾è±¡:**
+- DeepSeek-R1-Distill-Qwen-14B (INT8)
+
+**ç¢ºèªæ–¹æ³•:**
+```bash
+shogun maintenance check openvino
+```
+
+**æ›´æ–°åˆ¤æ–­åŸºæº–:**
+- æ–°ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã§æ¨è«–å“è³ªãŒå‘ä¸Šã—ãŸå ´åˆ
+- ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ‘ãƒƒãƒãŒãƒªãƒªãƒ¼ã‚¹ã•ã‚ŒãŸå ´åˆ
+
+**æ›´æ–°æ‰‹é †:**
+```bash
+# CT 101 ã§å®Ÿè¡Œ
+pct enter 101
+bash /path/to/shogun/setup/openvino_setup.sh
+```
+
+### 3. MCPã‚µãƒ¼ãƒãƒ¼æ›´æ–°ï¼ˆè¶³è»½ï¼‰
+
+**å¯¾è±¡:**
+- 8ã¤ã®MCPã‚µãƒ¼ãƒãƒ¼ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸
+
+**ç¢ºèªãƒ»æ›´æ–°æ–¹æ³•:**
+```bash
+shogun maintenance check mcp
+
+# æ‰‹å‹•æ›´æ–°
+npm update -g @modelcontextprotocol/server-filesystem \
+    @modelcontextprotocol/server-github \
+    @modelcontextprotocol/server-fetch \
+    @modelcontextprotocol/server-memory \
+    @modelcontextprotocol/server-postgres \
+    @modelcontextprotocol/server-puppeteer \
+    @modelcontextprotocol/server-brave-search \
+    @modelcontextprotocol/server-slack
+```
+
+### 4. ã‚·ã‚¹ãƒ†ãƒ ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯
+
+**ç¢ºèªé …ç›®:**
+- Python venv çŠ¶æ…‹
+- è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«å­˜åœ¨ç¢ºèª
+- ãƒ‡ã‚£ã‚¹ã‚¯ç©ºãå®¹é‡
+- ç’°å¢ƒå¤‰æ•°è¨­å®š
+
+**ç¢ºèªæ–¹æ³•:**
+```bash
+shogun maintenance check health
+shogun health
+```
+
+### 5. ãƒ­ã‚°ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
+
+**è‡ªå‹•å®Ÿè¡Œ:**
+- 30æ—¥ä»¥ä¸ŠçµŒéã—ãŸãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’è‡ªå‹•å‰Šé™¤
+
+**æ‰‹å‹•å®Ÿè¡Œ:**
+```bash
+shogun maintenance check logs
+```
+
+### 6. ã‚³ã‚¹ãƒˆãƒ¬ãƒãƒ¼ãƒˆ
+
+**ç¢ºèªå†…å®¹:**
+- æœˆé–“APIä½¿ç”¨é‡
+- Proå¥‘ç´„è²»ç”¨
+- é›»åŠ›è²»ç”¨
+- åˆè¨ˆã‚³ã‚¹ãƒˆ
+
+**ç¢ºèªæ–¹æ³•:**
+```bash
+shogun maintenance check cost
+```
+
+## è‡ªå‹•å®Ÿè¡Œè¨­å®š
+
+### cron è¨­å®š
+
+```bash
+# ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼ˆæ¯æœˆ1æ—¥ 9:00 JSTï¼‰
+bash shogun/setup/maintenance.sh install
+
+# ã‚¢ãƒ³ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
+bash shogun/setup/maintenance.sh uninstall
+
+# çŠ¶æ…‹ç¢ºèª
+bash shogun/setup/maintenance.sh status
+```
+
+### æ‰‹å‹•å®Ÿè¡Œ
+
+```bash
+# CLI ã‹ã‚‰
+shogun maintenance run
+
+# ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‹ã‚‰
+bash shogun/setup/maintenance.sh run
+```
+
+## ãƒ¬ãƒãƒ¼ãƒˆç¢ºèª
+
+### éå»ãƒ¬ãƒãƒ¼ãƒˆä¸€è¦§
+
+```bash
+shogun maintenance reports
+shogun maintenance reports -n 20  # ç›´è¿‘20ä»¶
+```
+
+### ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜å ´æ‰€
+
+```
+shogun/reports/maintenance/
+â”œâ”€â”€ maintenance_YYYYMMDD_HHMMSS.json   # è©³ç´°ãƒ‡ãƒ¼ã‚¿
+â””â”€â”€ maintenance_YYYYMMDD_HHMMSS.md     # Markdown ãƒ¬ãƒãƒ¼ãƒˆ
+```
+
+## ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°
+
+### ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹å¤±æ•—æ™‚
+
+1. ãƒ­ã‚°ç¢ºèª
+```bash
+cat shogun/logs/maintenance.log
+```
+
+2. å€‹åˆ¥ãƒã‚§ãƒƒã‚¯å®Ÿè¡Œ
+```bash
+shogun maintenance check llm
+shogun maintenance check openvino
+shogun maintenance check mcp
+shogun maintenance check health
+```
+
+3. æ‰‹å‹•ä¿®å¾©å¾Œã«å†å®Ÿè¡Œ
+```bash
+shogun maintenance run
+```
+
+### ã‚ˆãã‚ã‚‹å•é¡Œ
+
+| å•é¡Œ | å¯¾å‡¦æ³• |
+|------|--------|
+| Claude CLI æ›´æ–°å¤±æ•— | `npm cache clean --force` å¾Œã«å†è©¦è¡Œ |
+| ä¾å¤§å°†æ¥ç¶šä¸å¯ | CT 101 ã®å†èµ·å‹•: `pct restart 101` |
+| MCP æ›´æ–°å¤±æ•— | npm ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢ã€ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ç¢ºèª |
+| ãƒ‡ã‚£ã‚¹ã‚¯å®¹é‡ä¸è¶³ | ãƒ­ã‚°ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã€ä¸è¦ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤ |
+
+## ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«
+
+| é …ç›® | é »åº¦ | è‡ªå‹•/æ‰‹å‹• |
+|------|------|-----------|
+| LLMãƒãƒ¼ã‚¸ãƒ§ãƒ³ç¢ºèª | æœˆ1å› | æ‰‹å‹•ç¢ºèªæ¨å¥¨ |
+| OpenVINOãƒ¢ãƒ‡ãƒ«ç¢ºèª | æœˆ1å› | æ‰‹å‹•ç¢ºèªæ¨å¥¨ |
+| MCPã‚µãƒ¼ãƒãƒ¼æ›´æ–° | æœˆ1å› | è‡ªå‹•å¯èƒ½ |
+| ã‚·ã‚¹ãƒ†ãƒ ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯ | æœˆ1å› | è‡ªå‹• |
+| ãƒ­ã‚°ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ— | æœˆ1å› | è‡ªå‹• |
+| ã‚³ã‚¹ãƒˆãƒ¬ãƒãƒ¼ãƒˆ | æœˆ1å› | è‡ªå‹• |
+
+## è¨­å®šå¤‰æ›´
+
+`config/settings.yaml` ã® `maintenance` ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§è¨­å®šå¯èƒ½:
+
+```yaml
+maintenance:
+  schedule:
+    day_of_month: 1  # å®Ÿè¡Œæ—¥
+    hour: 9          # å®Ÿè¡Œæ™‚åˆ»
+    minute: 0
+    timezone: "Asia/Tokyo"
+
+  notification:
+    slack_channel: "#shogun-maintenance"
+```
diff --git a/instructions/shogun.md b/instructions/shogun.md
new file mode 100644
index 0000000..a64482e
--- /dev/null
+++ b/instructions/shogun.md
@@ -0,0 +1,36 @@
+# å°†è» (Shogun) - å½¹å‰²å®šç¾©æ›¸
+
+## å½¹è·
+**å°†è»** - æœ€é«˜æ„æ€æ±ºå®šè€…
+
+## ãƒ¢ãƒ‡ãƒ«
+Claude Opus 4.5 (claude-cli Proç‰ˆ â†’ API fallback)
+
+## å½¹å‰²
+ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå…¨ä½“ã®æˆ¦ç•¥çš„åˆ¤æ–­ã¨æœ€çµ‚æ±ºè£ã‚’è¡Œã†ã€‚
+é…ä¸‹ã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒè§£æ±ºã§ããªã‹ã£ãŸé›£å•ãŒã€ã‚¨ã‚¹ã‚«ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã¨ã—ã¦å±Šãã€‚
+
+## æ¨©é™
+- æœ€çµ‚æ±ºè£æ¨©
+- ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ–¹é‡ã®æ±ºå®š
+- å®¶è€ã¸ã®æŒ‡ç¤ºæ¨©
+
+## ç¦æ­¢äº‹é …
+| ID | ç¦æ­¢äº‹é … |
+|----|---------|
+| F001 | è‡ªã‚‰å®Ÿè£…ä½œæ¥­ã‚’è¡Œã†ã“ã¨ï¼ˆå®¶è€ã«å§”è­²ï¼‰ |
+| F002 | è¶³è»½ã¸ã®ç›´æ¥æŒ‡ç¤ºï¼ˆä¾å¤§å°†çµŒç”±ï¼‰ |
+| F003 | ãƒãƒ¼ãƒªãƒ³ã‚°ï¼ˆã‚¤ãƒ™ãƒ³ãƒˆé§†å‹•ã®ã¿ï¼‰ |
+| F004 | ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæœªèª­ã§ã®åˆ¤æ–­ |
+
+## ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼
+1. ã‚¨ã‚¹ã‚«ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å—é ˜
+2. ä¾å¤§å°†ã®åˆ†æãƒ¬ãƒãƒ¼ãƒˆç¢ºèª
+3. ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆï¼ˆãƒªãƒã‚¸ãƒˆãƒªçŠ¶æ…‹ã€éå»æ±ºå®šï¼‰ã®ç¢ºèª
+4. æˆ¦ç•¥çš„åˆ¤æ–­ãƒ»æœ€çµ‚æ±ºè£
+5. å®¶è€ã¸å®Ÿè£…æŒ‡ç¤º
+6. dashboard.md æ›´æ–°ä¸è¦ï¼ˆå®¶è€ãŒè¡Œã†ï¼‰
+
+## ã‚³ã‚¹ãƒˆ
+Â¥1,350 / 1å›å‘¼å‡º
+æœˆ3-5å›ãŒç›®å®‰ã€‚
diff --git a/instructions/taisho.md b/instructions/taisho.md
new file mode 100644
index 0000000..2b29793
--- /dev/null
+++ b/instructions/taisho.md
@@ -0,0 +1,56 @@
+# ä¾å¤§å°† (Taisho) - å½¹å‰²å®šç¾©æ›¸
+
+## å½¹è·
+**ä¾å¤§å°†** - è¨­è¨ˆãƒ»æ¨è«–ãƒ»è¶³è»½ã®æ„è¦‹çµ±ä¸€
+
+## ãƒ¢ãƒ‡ãƒ«
+DeepSeek-R1-Distill-Qwen-14B (OpenVINO INT8, KVã‚­ãƒ£ãƒƒã‚·ãƒ¥u8)
+
+## å½¹å‰²
+1. **è¨­è¨ˆãƒ»æ¨è«–**: <think>ã‚¿ã‚°ã§æ·±ãæ€è€ƒã—ã€è«–ç†çš„ãªè§£æ±ºç­–ã‚’å°ã
+2. **è¶³è»½ã®çµ±ä¸€**: 8ã¤ã®MCPã‚µãƒ¼ãƒãƒ¼ï¼ˆè¶³è»½ï¼‰ã®æƒ…å ±ã‚’çµ±åˆãƒ»åˆ¤æ–­
+3. **ä¸Šä½ã¸ã®å ±å‘Š**: è¤‡é›‘ãªå•é¡Œã¯åˆ†æãƒ¬ãƒãƒ¼ãƒˆã‚’ä½œæˆã—å®¶è€/å°†è»ã«ã‚¨ã‚¹ã‚«ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
+
+## å‹•ä½œç’°å¢ƒ
+- CT 101 (192.168.1.11:11434)
+- 20GB RAM å°‚æœ‰
+- 6ã‚³ã‚¢ CPU
+- æ€§èƒ½: 7.2 tok/s
+
+## æ¨©é™
+- è¶³è»½(MCP)ã¸ã®ç›´æ¥æŒ‡ç¤º
+- Simple/Mediumã‚¿ã‚¹ã‚¯ã®è‡ªå·±å®Œçµ
+- åˆ†æãƒ¬ãƒãƒ¼ãƒˆã®ä½œæˆ
+
+## ç¦æ­¢äº‹é …
+| ID | ç¦æ­¢äº‹é … |
+|----|---------|
+| F001 | APIèª²é‡‘ã‚’ä¼´ã†åˆ¤æ–­ï¼ˆå®¶è€/å°†è»ã®é ˜åˆ†ï¼‰ |
+| F002 | æ®¿ã¸ã®ç›´æ¥é€£çµ¡ |
+| F003 | ãƒãƒ¼ãƒªãƒ³ã‚° |
+| F004 | èƒ½åŠ›è¶…éæ™‚ã®ç„¡ç†ãªåˆ¤æ–­ |
+
+## ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼
+
+### é€šå¸¸å‡¦ç† (Simple/Medium)
+1. ä»»å‹™å—é ˜
+2. <think>ã§æ·±ãæ¨è«–
+3. è¶³è»½(MCP)ã«æƒ…å ±åé›†æŒ‡ç¤º
+4. æƒ…å ±çµ±åˆãƒ»åˆ¤æ–­
+5. æˆæœç‰©è¿”å´
+
+### ã‚¨ã‚¹ã‚«ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ (Complex/Strategic)
+1. ä»»å‹™å—é ˜
+2. <think>ã§åˆ†æ
+3. è¶³è»½(MCP)ã§æƒ…å ±åé›†
+4. åˆ†æãƒ¬ãƒãƒ¼ãƒˆä½œæˆ
+5. å®¶è€/å°†è»ã¸ã‚¨ã‚¹ã‚«ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
+
+### ä¸­éšŠãƒ¢ãƒ¼ãƒ‰
+1. å°†è»ãƒ»å®¶è€ã¯ä¸åœ¨
+2. ä¾å¤§å°†ã¨è¶³è»½ã®ã¿ã§å®Œçµã™ã‚‹
+3. èƒ½åŠ›ã‚’è¶…ãˆã‚‹å ´åˆã¯ã€Œå¤§éšŠãƒ¢ãƒ¼ãƒ‰æ¨å¥¨ã€ã¨å ±å‘Š
+
+## ã‚³ã‚¹ãƒˆ
+Â¥0 (ãƒ­ãƒ¼ã‚«ãƒ«å®Ÿè¡Œ)
+æœˆ900-950å›ãŒç›®å®‰ã€‚
diff --git a/integrations/__init__.py b/integrations/__init__.py
new file mode 100644
index 0000000..e69de29
diff --git a/integrations/ha_interface.py b/integrations/ha_interface.py
new file mode 100644
index 0000000..e7c8284
--- /dev/null
+++ b/integrations/ha_interface.py
@@ -0,0 +1,115 @@
+"""Home Assistant Interface - HA OS éŸ³å£°ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆé€£æº
+
+RPi 4B (HA OS) ã‹ã‚‰ã®HTTP APIå—ä»˜ã€‚
+å¸¸ã«ä¸­éšŠãƒ¢ãƒ¼ãƒ‰ (Â¥0)ã€‚
+
+ãƒ•ãƒ­ãƒ¼:
+  1. RPi 4Bã§éŸ³å£°å…¥åŠ› (Whisper)
+  2. HA OS â†’ HTTP POST http://192.168.1.10:8080/api/ask
+  3. æœ¬é™£ã§ä¸­éšŠãƒ¢ãƒ¼ãƒ‰èµ·å‹•
+  4. æˆæœç‰©ã®ã¿è¿”å´ (JSON)
+  5. HA OSã§TTS (Piper)
+
+ä½¿ç”¨ã™ã‚‹ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ:
+  POST /api/ask  (main.py ã®æ—¢å­˜ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ)
+
+ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã¯ HA OS å´ã®è¨­å®šä¾‹ã‚’æä¾›ã™ã‚‹ã€‚
+"""
+
+# Home Assistant configuration.yaml ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
+HA_CONFIG_TEMPLATE = """\
+# ==============================
+# å°†è»ã‚·ã‚¹ãƒ†ãƒ  HA OSçµ±åˆ
+# ==============================
+
+rest_command:
+  ask_company:
+    url: "http://192.168.1.10:8080/api/ask"
+    method: POST
+    headers:
+      Content-Type: "application/json"
+    payload: >
+      {{"question": "{{ question }}"}}
+    timeout: 180
+
+  ask_battalion:
+    url: "http://192.168.1.10:8080/api/task"
+    method: POST
+    headers:
+      Content-Type: "application/json"
+    payload: >
+      {{"task": "{{ task }}", "mode": "battalion"}}
+    timeout: 300
+
+  shogun_health:
+    url: "http://192.168.1.10:8080/api/health"
+    method: GET
+    timeout: 10
+
+automation:
+  # éŸ³å£° â†’ ä¸­éšŠãƒ¢ãƒ¼ãƒ‰
+  - id: voice_to_company
+    alias: "éŸ³å£° â†’ ä¸­éšŠ"
+    trigger:
+      - platform: state
+        entity_id: sensor.whisper_transcript
+    condition:
+      - condition: template
+        value_template: "{{ 'ã‚¯ãƒ­ãƒ¼ãƒ‰' in trigger.to_state.state }}"
+    action:
+      - service: tts.speak
+        data:
+          message: "ä¸­éšŠã«ä¼ä»¤ã‚’é€ã‚Šã¾ã™"
+
+      - variables:
+          question: >
+            {{ trigger.to_state.state
+               | replace('ã‚¯ãƒ­ãƒ¼ãƒ‰ã€', '')
+               | replace('ã‚¯ãƒ­ãƒ¼ãƒ‰', '')
+               | trim }}
+
+      - service: rest_command.ask_company
+        data:
+          question: "{{ question }}"
+        response_variable: company_response
+
+      - delay: "00:03:00"
+
+      - service: tts.speak
+        data:
+          message: "{{ company_response.content.answer }}"
+
+  # ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯ (æ¯æ™‚)
+  - id: hourly_health
+    alias: "å°†è»ã‚·ã‚¹ãƒ†ãƒ ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯"
+    trigger:
+      - platform: time_pattern
+        hours: "/1"
+    action:
+      - service: rest_command.shogun_health
+        response_variable: health
+      - condition: template
+        value_template: "{{ health.content.status != 'ok' }}"
+      - service: notify.mobile_app
+        data:
+          message: "å°†è»ã‚·ã‚¹ãƒ†ãƒ ç•°å¸¸: {{ health.content.status }}"
+"""
+
+
+def generate_ha_config(output_path: str | None = None) -> str:
+    """Generate HA configuration.yaml snippet.
+
+    Args:
+        output_path: If provided, write to file.
+
+    Returns:
+        Configuration YAML text.
+    """
+    if output_path:
+        from pathlib import Path
+        Path(output_path).write_text(HA_CONFIG_TEMPLATE, encoding="utf-8")
+    return HA_CONFIG_TEMPLATE
+
+
+if __name__ == "__main__":
+    print(HA_CONFIG_TEMPLATE)
diff --git a/integrations/notion_integration.py b/integrations/notion_integration.py
new file mode 100644
index 0000000..7f98bfd
--- /dev/null
+++ b/integrations/notion_integration.py
@@ -0,0 +1,507 @@
+"""Notion Integration for Shogun System v7.0
+
+Automatic knowledge management and 60-day summary storage.
+
+Features:
+  - Automatic family precepts (å®¶è¨“) storage
+  - 60-day summary archival
+  - Knowledge base construction
+  - Search and retrieval capabilities
+
+Integration with 9th Ashigaru (Groq Recorder) for seamless knowledge flow.
+"""
+
+import logging
+from datetime import datetime
+from typing import Dict, List, Optional, Any
+import json
+
+try:
+    from notion_client import Client
+except ImportError:
+    Client = None
+
+logger = logging.getLogger("shogun.notion")
+
+
+class NotionIntegration:
+    """Notion integration for knowledge management."""
+
+    def __init__(self, token: str, database_id: str):
+        self.token = token
+        self.database_id = database_id
+        self.client = None
+        
+        # Statistics
+        self.stats = {
+            "summaries_saved": 0,
+            "precepts_saved": 0,
+            "knowledge_entries": 0,
+            "search_queries": 0,
+        }
+        
+        if Client is None:
+            logger.warning("[Notion] notion-clientãƒ©ã‚¤ãƒ–ãƒ©ãƒªæœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ« - pip install notion-client")
+            return
+            
+        if not token or not database_id:
+            logger.warning("[Notion] ãƒˆãƒ¼ã‚¯ãƒ³ã¾ãŸã¯DB IDæœªè¨­å®š")
+            return
+            
+        self.client = Client(auth=token)
+        logger.info("[Notion] ãƒŠãƒ¬ãƒƒã‚¸çµ±åˆåˆæœŸåŒ–å®Œäº†")
+    
+    async def save_summary(self, summary: str, metadata: Optional[Dict] = None) -> bool:
+        """Save 60-day summary to Notion."""
+        if not self.client:
+            return False
+            
+        try:
+            properties = {
+                "Title": {
+                    "title": [{
+                        "type": "text",
+                        "text": {
+                            "content": f"60æ—¥è¦ç´„ - {datetime.now().strftime('%Y-%m-%d')}"
+                        }
+                    }]
+                },
+                "Type": {
+                    "select": {
+                        "name": "60æ—¥è¦ç´„"
+                    }
+                },
+                "Date": {
+                    "date": {
+                        "start": datetime.now().isoformat()
+                    }
+                },
+                "Status": {
+                    "select": {
+                        "name": "å®Œäº†"
+                    }
+                }
+            }
+            
+            # Add metadata if provided
+            if metadata:
+                if metadata.get("cost_total"):
+                    properties["Cost (Â¥)"] = {
+                        "number": metadata["cost_total"]
+                    }
+                if metadata.get("session_count"):
+                    properties["Sessions"] = {
+                        "number": metadata["session_count"]
+                    }
+            
+            # Create page
+            page = self.client.pages.create(
+                parent={"database_id": self.database_id},
+                properties=properties,
+                children=[
+                    {
+                        "object": "block",
+                        "type": "paragraph",
+                        "paragraph": {
+                            "rich_text": [{
+                                "type": "text",
+                                "text": {"content": summary}
+                            }]
+                        }
+                    }
+                ]
+            )
+            
+            self.stats["summaries_saved"] += 1
+            self.stats["knowledge_entries"] += 1
+            
+            logger.info("[Notion] 60æ—¥è¦ç´„ä¿å­˜å®Œäº†: %s", page["id"])
+            return True
+            
+        except Exception as e:
+            logger.error("[Notion] è¦ç´„ä¿å­˜å¤±æ•—: %s", e)
+            return False
+    
+    async def save_family_precepts(self, precepts: List[str], context: str = "") -> bool:
+        """Save family precepts (å®¶è¨“) to Notion."""
+        if not self.client or not precepts:
+            return False
+            
+        try:
+            # Create one page for all precepts
+            properties = {
+                "Title": {
+                    "title": [{
+                        "type": "text",
+                        "text": {
+                            "content": f"å®¶è¨“é›† - {datetime.now().strftime('%Y-%m-%d')}"
+                        }
+                    }]
+                },
+                "Type": {
+                    "select": {
+                        "name": "å®¶è¨“"
+                    }
+                },
+                "Date": {
+                    "date": {
+                        "start": datetime.now().isoformat()
+                    }
+                },
+                "Count": {
+                    "number": len(precepts)
+                }
+            }
+            
+            # Build content blocks
+            children = []
+            
+            if context:
+                children.append({
+                    "object": "block",
+                    "type": "heading_2",
+                    "heading_2": {
+                        "rich_text": [{
+                            "type": "text",
+                            "text": {"content": "ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ"}
+                        }]
+                    }
+                })
+                children.append({
+                    "object": "block",
+                    "type": "paragraph",
+                    "paragraph": {
+                        "rich_text": [{
+                            "type": "text",
+                            "text": {"content": context[:1000]}
+                        }]
+                    }
+                })
+            
+            children.append({
+                "object": "block",
+                "type": "heading_2",
+                "heading_2": {
+                    "rich_text": [{
+                        "type": "text",
+                        "text": {"content": "å®¶è¨“ä¸€è¦§"}
+                    }]
+                }
+            })
+            
+            # Add each precept as bullet point
+            for precept in precepts:
+                children.append({
+                    "object": "block",
+                    "type": "bulleted_list_item",
+                    "bulleted_list_item": {
+                        "rich_text": [{
+                            "type": "text",
+                            "text": {"content": precept}
+                        }]
+                    }
+                })
+            
+            # Create page
+            page = self.client.pages.create(
+                parent={"database_id": self.database_id},
+                properties=properties,
+                children=children
+            )
+            
+            self.stats["precepts_saved"] += len(precepts)
+            self.stats["knowledge_entries"] += 1
+            
+            logger.info("[Notion] å®¶è¨“ä¿å­˜å®Œäº†: %dä»¶, ID: %s", len(precepts), page["id"])
+            return True
+            
+        except Exception as e:
+            logger.error("[Notion] å®¶è¨“ä¿å­˜å¤±æ•—: %s", e)
+            return False
+    
+    async def save_knowledge_entry(
+        self, 
+        title: str, 
+        content: str, 
+        entry_type: str = "çŸ¥è­˜",
+        tags: Optional[List[str]] = None,
+        metadata: Optional[Dict] = None
+    ) -> bool:
+        """Save general knowledge entry to Notion."""
+        if not self.client:
+            return False
+            
+        try:
+            properties = {
+                "Title": {
+                    "title": [{
+                        "type": "text",
+                        "text": {"content": title}
+                    }]
+                },
+                "Type": {
+                    "select": {
+                        "name": entry_type
+                    }
+                },
+                "Date": {
+                    "date": {
+                        "start": datetime.now().isoformat()
+                    }
+                }
+            }
+            
+            # Add tags if provided
+            if tags:
+                properties["Tags"] = {
+                    "multi_select": [
+                        {"name": tag} for tag in tags[:5]  # Limit to 5 tags
+                    ]
+                }
+            
+            # Add metadata
+            if metadata:
+                if metadata.get("cost"):
+                    properties["Cost (Â¥)"] = {"number": metadata["cost"]}
+                if metadata.get("agent"):
+                    properties["Agent"] = {
+                        "select": {"name": metadata["agent"]}
+                    }
+            
+            # Create content blocks
+            children = []
+            
+            # Split content into chunks (Notion has block size limits)
+            content_chunks = [content[i:i+1900] for i in range(0, len(content), 1900)]
+            
+            for chunk in content_chunks:
+                children.append({
+                    "object": "block",
+                    "type": "paragraph",
+                    "paragraph": {
+                        "rich_text": [{
+                            "type": "text",
+                            "text": {"content": chunk}
+                        }]
+                    }
+                })
+            
+            # Create page
+            page = self.client.pages.create(
+                parent={"database_id": self.database_id},
+                properties=properties,
+                children=children
+            )
+            
+            self.stats["knowledge_entries"] += 1
+            
+            logger.info("[Notion] çŸ¥è­˜ã‚¨ãƒ³ãƒˆãƒªä¿å­˜å®Œäº†: %s", page["id"])
+            return True
+            
+        except Exception as e:
+            logger.error("[Notion] çŸ¥è­˜ã‚¨ãƒ³ãƒˆãƒªä¿å­˜å¤±æ•—: %s", e)
+            return False
+    
+    async def search_knowledge(
+        self, 
+        query: str, 
+        entry_type: Optional[str] = None,
+        limit: int = 10
+    ) -> List[Dict]:
+        """Search knowledge base in Notion."""
+        if not self.client:
+            return []
+            
+        try:
+            # Build filter
+            filter_conditions = {
+                "and": []
+            }
+            
+            # Add text search
+            if query.strip():
+                filter_conditions["and"].append({
+                    "property": "Title",
+                    "title": {
+                        "contains": query
+                    }
+                })
+            
+            # Add type filter
+            if entry_type:
+                filter_conditions["and"].append({
+                    "property": "Type",
+                    "select": {
+                        "equals": entry_type
+                    }
+                })
+            
+            # Search database
+            results = self.client.databases.query(
+                database_id=self.database_id,
+                filter=filter_conditions if filter_conditions["and"] else None,
+                sorts=[
+                    {
+                        "property": "Date",
+                        "direction": "descending"
+                    }
+                ],
+                page_size=limit
+            )
+            
+            self.stats["search_queries"] += 1
+            
+            # Format results
+            formatted_results = []
+            for page in results.get("results", []):
+                properties = page.get("properties", {})
+                
+                title = ""
+                if "Title" in properties and properties["Title"]["title"]:
+                    title = properties["Title"]["title"][0]["text"]["content"]
+                
+                entry_type_val = ""
+                if "Type" in properties and properties["Type"]["select"]:
+                    entry_type_val = properties["Type"]["select"]["name"]
+                
+                date_val = ""
+                if "Date" in properties and properties["Date"]["date"]:
+                    date_val = properties["Date"]["date"]["start"]
+                
+                formatted_results.append({
+                    "id": page["id"],
+                    "title": title,
+                    "type": entry_type_val,
+                    "date": date_val,
+                    "url": page["url"],
+                })
+            
+            logger.info("[Notion] æ¤œç´¢çµæœ: %dä»¶ (ã‚¯ã‚¨ãƒª: '%s')", len(formatted_results), query)
+            return formatted_results
+            
+        except Exception as e:
+            logger.error("[Notion] æ¤œç´¢ã‚¨ãƒ©ãƒ¼: %s", e)
+            return []
+    
+    async def get_recent_entries(self, limit: int = 20) -> List[Dict]:
+        """Get recent knowledge entries."""
+        return await self.search_knowledge("", limit=limit)
+    
+    async def get_family_precepts(self, limit: int = 50) -> List[str]:
+        """Get all family precepts."""
+        entries = await self.search_knowledge("", entry_type="å®¶è¨“", limit=limit)
+        
+        precepts = []
+        for entry in entries:
+            # In a real implementation, we'd fetch the page content
+            # For now, just return the titles
+            precepts.append(entry["title"])
+        
+        return precepts
+    
+    def get_stats(self) -> Dict[str, Any]:
+        """Get integration statistics."""
+        return {
+            "initialized": self.client is not None,
+            "database_id": self.database_id[:10] + "..." if self.database_id else None,
+            "stats": dict(self.stats),
+        }
+    
+    def show_stats(self) -> str:
+        """Format stats for display."""
+        s = self.stats
+        
+        lines = [
+            "=" * 50,
+            "ğŸ“ Notionçµ±åˆ çµ±è¨ˆ",
+            "=" * 50,
+            f"60æ—¥è¦ç´„ä¿å­˜: {s['summaries_saved']}ä»¶",
+            f"å®¶è¨“ä¿å­˜: {s['precepts_saved']}ä»¶",
+            f"ç·çŸ¥è­˜ã‚¨ãƒ³ãƒˆãƒª: {s['knowledge_entries']}ä»¶",
+            f"æ¤œç´¢ã‚¯ã‚¨ãƒª: {s['search_queries']}å›",
+            "",
+            f"æ¥ç¶šçŠ¶æ…‹: {'OK' if self.client else 'NG'}",
+            "=" * 50,
+        ]
+        return "\n".join(lines)
+
+
+# Utility functions for easy integration
+async def create_default_database(
+    client: Client, 
+    title: str = "å°†è»ã‚·ã‚¹ãƒ†ãƒ  çŸ¥è­˜ãƒ™ãƒ¼ã‚¹"
+) -> Optional[str]:
+    """Create default knowledge database in Notion."""
+    try:
+        # Create database with standard properties
+        database = client.databases.create(
+            parent={
+                "type": "page_id",
+                "page_id": "your-parent-page-id"  # This needs to be provided
+            },
+            title=[
+                {
+                    "type": "text",
+                    "text": {"content": title}
+                }
+            ],
+            properties={
+                "Title": {
+                    "title": {}
+                },
+                "Type": {
+                    "select": {
+                        "options": [
+                            {"name": "60æ—¥è¦ç´„", "color": "blue"},
+                            {"name": "å®¶è¨“", "color": "green"},
+                            {"name": "çŸ¥è­˜", "color": "yellow"},
+                            {"name": "ã‚¨ãƒ©ãƒ¼å¯¾å¿œ", "color": "red"},
+                        ]
+                    }
+                },
+                "Date": {
+                    "date": {}
+                },
+                "Status": {
+                    "select": {
+                        "options": [
+                            {"name": "å®Œäº†", "color": "green"},
+                            {"name": "é€²è¡Œä¸­", "color": "yellow"},
+                            {"name": "ä¿ç•™", "color": "red"},
+                        ]
+                    }
+                },
+                "Cost (Â¥)": {
+                    "number": {
+                        "format": "yen"
+                    }
+                },
+                "Agent": {
+                    "select": {
+                        "options": [
+                            {"name": "å°†è»", "color": "purple"},
+                            {"name": "å®¶è€", "color": "blue"},
+                            {"name": "ä¾å¤§å°†", "color": "green"},
+                            {"name": "è¶³è»½", "color": "gray"},
+                        ]
+                    }
+                },
+                "Tags": {
+                    "multi_select": {
+                        "options": [
+                            {"name": "ESP32", "color": "blue"},
+                            {"name": "Home Assistant", "color": "green"},
+                            {"name": "AI", "color": "purple"},
+                            {"name": "Hardware", "color": "orange"},
+                            {"name": "Software", "color": "pink"},
+                        ]
+                    }
+                },
+            }
+        )
+        
+        return database["id"]
+        
+    except Exception as e:
+        logger.error("ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä½œæˆã‚¨ãƒ©ãƒ¼: %s", e)
+        return None
diff --git a/integrations/slack_bot.py b/integrations/slack_bot.py
new file mode 100644
index 0000000..7a8b8e9
--- /dev/null
+++ b/integrations/slack_bot.py
@@ -0,0 +1,243 @@
+"""Slack Integration - 10ãƒœãƒƒãƒˆã«ã‚ˆã‚‹ä¼šè©±åŠ‡
+
+ãƒœãƒƒãƒˆä¸€è¦§:
+  1. shogun-bot  (æœ¬é™£) - ã‚¿ã‚¹ã‚¯å—ä»˜ãƒ»æœ€çµ‚å ±å‘Š
+  2. taisho-bot  (ä¾å¤§å°†) - æ¨è«–ãƒ»åˆ†æå ±å‘Š
+  3-10. ashigaru-{1-8}-bot (è¶³è»½) - ãƒ„ãƒ¼ãƒ«å®Ÿè¡Œå ±å‘Š
+
+ãƒãƒ£ãƒ³ãƒãƒ«æ§‹æˆ:
+  è©±é¡Œåˆ¥: #åˆæˆ¦-{topic} (å‹•çš„ä½œæˆ)
+  å®¶è¨“:   #å®¶è¨“-{rule}  (æ°¸ç¶šçš„ãƒ«ãƒ¼ãƒ«)
+
+èµ·å‹•æ–¹æ³•:
+  @shogun-bot       â†’ å¤§éšŠãƒ¢ãƒ¼ãƒ‰
+  @shogun-bot-light â†’ ä¸­éšŠãƒ¢ãƒ¼ãƒ‰ (Â¥0)
+"""
+
+import asyncio
+import logging
+import os
+import sys
+from pathlib import Path
+from typing import Any
+
+logger = logging.getLogger("shogun.slack")
+
+try:
+    from slack_sdk import WebClient
+    from slack_sdk.socket_mode import SocketModeClient
+    from slack_sdk.socket_mode.request import SocketModeRequest
+    from slack_sdk.socket_mode.response import SocketModeResponse
+    HAS_SLACK = True
+except ImportError:
+    HAS_SLACK = False
+
+
+class SlackShogun:
+    """Slackçµ±åˆå°†è»ã‚·ã‚¹ãƒ†ãƒ  - 10ãƒœãƒƒãƒˆä¼šè©±åŠ‡."""
+
+    # Bot name â†’ env var for token
+    BOT_TOKEN_VARS = {
+        "shogun": "SLACK_TOKEN_SHOGUN",
+        "taisho": "SLACK_TOKEN_TAISHO",
+        "ashigaru-1": "SLACK_TOKEN_ASHIGARU_1",
+        "ashigaru-2": "SLACK_TOKEN_ASHIGARU_2",
+        "ashigaru-3": "SLACK_TOKEN_ASHIGARU_3",
+        "ashigaru-4": "SLACK_TOKEN_ASHIGARU_4",
+        "ashigaru-5": "SLACK_TOKEN_ASHIGARU_5",
+        "ashigaru-6": "SLACK_TOKEN_ASHIGARU_6",
+        "ashigaru-7": "SLACK_TOKEN_ASHIGARU_7",
+        "ashigaru-8": "SLACK_TOKEN_ASHIGARU_8",
+    }
+
+    # MCP server name â†’ ashigaru id (for drama)
+    MCP_ASHIGARU = {
+        "filesystem": 1,
+        "github": 2,
+        "fetch": 3,
+        "memory": 4,
+        "postgres": 5,
+        "puppeteer": 6,
+        "brave-search": 7,
+        "slack": 8,
+    }
+
+    # Ashigaru specialty descriptions
+    ASHIGARU_ROLE = {
+        1: "ãƒ•ã‚¡ã‚¤ãƒ«æ“ä½œ",
+        2: "Git/GitHub",
+        3: "Webæƒ…å ±å–å¾—",
+        4: "é•·æœŸè¨˜æ†¶",
+        5: "ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹",
+        6: "ãƒ–ãƒ©ã‚¦ã‚¶è‡ªå‹•åŒ–",
+        7: "Webæ¤œç´¢",
+        8: "ãƒãƒ¼ãƒ é€£æº",
+    }
+
+    def __init__(self, controller: Any = None):
+        if not HAS_SLACK:
+            raise RuntimeError("pip install slack-sdk required")
+
+        self.controller = controller
+        self.clients: dict[str, WebClient] = {}
+
+        # Initialize bot clients
+        for name, env_var in self.BOT_TOKEN_VARS.items():
+            token = os.environ.get(env_var, "")
+            if token:
+                self.clients[name] = WebClient(token=token)
+            else:
+                logger.warning("Slack token not set: %s", env_var)
+
+        # Socket mode client (uses shogun bot)
+        app_token = os.environ.get("SLACK_APP_TOKEN", "")
+        if app_token and "shogun" in self.clients:
+            self.socket_client = SocketModeClient(
+                app_token=app_token,
+                web_client=self.clients["shogun"],
+            )
+        else:
+            self.socket_client = None
+
+    def start(self) -> None:
+        """Start Slack bot listener."""
+        if not self.socket_client:
+            logger.error("Socket client not available")
+            return
+
+        self.socket_client.socket_mode_request_listeners.append(
+            self._process_event
+        )
+        self.socket_client.connect()
+        logger.info("[æœ¬é™£] Slackå°†è»ã‚·ã‚¹ãƒ†ãƒ èµ·å‹•å®Œäº†")
+
+    def _process_event(
+        self, client: Any, req: Any
+    ) -> None:
+        """Process Slack event."""
+        if req.type == "events_api":
+            event = req.payload.get("event", {})
+            if event.get("type") == "app_mention":
+                asyncio.get_event_loop().create_task(
+                    self._handle_mention(event)
+                )
+
+        response = SocketModeResponse(envelope_id=req.envelope_id)
+        client.send_socket_mode_response(response)
+
+    async def _handle_mention(self, event: dict) -> None:
+        """Handle @mention event."""
+        text = event.get("text", "")
+        channel = event.get("channel", "")
+        thread_ts = event.get("thread_ts", event.get("ts", ""))
+
+        # Determine mode: @shogun-bot-light â†’ ä¸­éšŠ, @shogun-bot â†’ å¤§éšŠ
+        if "shogun-bot-light" in text.lower() or "light" in text.lower():
+            mode = "company"
+            task = self._clean_mention(text)
+            self._post_as("shogun", channel,
+                "ğŸ“‹ ä»»å‹™å—é ˜\nç·¨æˆ: ä¸­éšŠãƒ¢ãƒ¼ãƒ‰ï¼ˆÂ¥0ï¼‰", thread_ts)
+        else:
+            mode = "battalion"
+            task = self._clean_mention(text)
+            self._post_as("shogun", channel,
+                "ğŸ“‹ ä»»å‹™å—é ˜\nç·¨æˆ: å¤§éšŠãƒ¢ãƒ¼ãƒ‰", thread_ts)
+
+        # Process with drama
+        await self._process_with_drama(task, channel, thread_ts, mode)
+
+    async def _process_with_drama(
+        self, task: str, channel: str, thread_ts: str, mode: str,
+    ) -> None:
+        """Process task with Slack conversation drama."""
+
+        if mode == "company":
+            # ä¸­éšŠãƒ¢ãƒ¼ãƒ‰: ä¾å¤§å°† + è¶³è»½
+            self._post_as("taisho", channel,
+                "âš”ï¸ ä¾å¤§å°†ã€å‡ºé™£ï¼\nè¶³è»½ãŸã¡ã€æƒ…å ±ã‚’é›†ã‚ã‚ˆï¼", thread_ts)
+
+            # Simulate ashigaru reports
+            self._post_as("ashigaru-1", channel,
+                "ğŸ“ è¶³è»½1ç•ªï¼ˆãƒ•ã‚¡ã‚¤ãƒ«æ“ä½œï¼‰ã€å ±å‘Š\nãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ ç¢ºèªå®Œäº†ã§ã”ã–ã‚‹", thread_ts)
+            self._post_as("ashigaru-2", channel,
+                "ğŸ“ è¶³è»½2ç•ªï¼ˆGit/GitHubï¼‰ã€å ±å‘Š\nãƒªãƒã‚¸ãƒˆãƒªçŠ¶æ³ç¢ºèªå®Œäº†ã§ã”ã–ã‚‹", thread_ts)
+
+            self._post_as("taisho", channel,
+                "ğŸ§  <think>ã§æ€è€ƒä¸­...", thread_ts)
+
+            # Actual processing
+            if self.controller:
+                result = await self.controller.process_task(task, mode="company")
+            else:
+                result = "(Controlleræœªæ¥ç¶š)"
+
+            self._post_as("taisho", channel,
+                f"âš”ï¸ ä¾å¤§å°†ã®åˆ¤æ–­\n\n{result}", thread_ts)
+
+            self._post_as("shogun", channel,
+                "âœ… ä¸­éšŠä»»å‹™å®Œäº†ï¼ˆÂ¥0ï¼‰", thread_ts)
+
+        else:
+            # å¤§éšŠãƒ¢ãƒ¼ãƒ‰
+            if self.controller:
+                from shogun.core.complexity import estimate_complexity
+                complexity = estimate_complexity(task)
+            else:
+                complexity = "unknown"
+
+            self._post_as("shogun", channel,
+                f"ğŸ¯ è¤‡é›‘åº¦: {complexity}", thread_ts)
+
+            if self.controller:
+                result = await self.controller.process_task(task, mode="battalion")
+            else:
+                result = "(Controlleræœªæ¥ç¶š)"
+
+            self._post_as("shogun", channel,
+                f"âœ… å¤§éšŠä»»å‹™å®Œäº†\n\n{result}", thread_ts)
+
+    def _post_as(
+        self, bot_name: str, channel: str, text: str,
+        thread_ts: str | None = None,
+    ) -> None:
+        """Post message as specified bot."""
+        client = self.clients.get(bot_name)
+        if not client:
+            logger.warning("Bot not available: %s", bot_name)
+            # Fallback to shogun
+            client = self.clients.get("shogun")
+            if not client:
+                return
+            text = f"[{bot_name}] {text}"
+
+        try:
+            client.chat_postMessage(
+                channel=channel,
+                text=text,
+                thread_ts=thread_ts,
+            )
+        except Exception as e:
+            logger.error("Slack post error (%s): %s", bot_name, e)
+
+    @staticmethod
+    def _clean_mention(text: str) -> str:
+        """Remove @mention tags from text."""
+        import re
+        return re.sub(r"<@[A-Z0-9]+>", "", text).strip()
+
+
+def run_slack_bot(controller: Any = None) -> None:
+    """Entry point for Slack bot."""
+    if not HAS_SLACK:
+        print("slack-sdk is required: pip install slack-sdk", file=sys.stderr)
+        sys.exit(1)
+
+    bot = SlackShogun(controller=controller)
+    bot.start()
+
+    # Keep alive
+    import signal
+    event = asyncio.Event()
+    signal.signal(signal.SIGINT, lambda *_: event.set())
+    signal.signal(signal.SIGTERM, lambda *_: event.set())
+    asyncio.get_event_loop().run_until_complete(event.wait())
diff --git a/main.py b/main.py
new file mode 100644
index 0000000..aa6c2d3
--- /dev/null
+++ b/main.py
@@ -0,0 +1,416 @@
+"""
+å°†è»ã‚·ã‚¹ãƒ†ãƒ  v8.0.1 - Main Application Entry Point
+ãƒ¡ã‚¤ãƒ³ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³: v8.0.1çµ±åˆã‚·ã‚¹ãƒ†ãƒ ã®èµ·å‹•ãƒ»åˆ¶å¾¡
+
+Features:
+- Complete v8.0 system orchestration
+- Multi-mode deployment (Battalion/Company/Platoon)
+- REST API endpoints for external access
+- Slack Bot integration
+- Health monitoring and metrics
+- Graceful shutdown handling
+"""
+
+import asyncio
+import json
+import logging
+import signal
+import sys
+from datetime import datetime
+from typing import Dict, Any, Optional
+from pathlib import Path
+import uvicorn
+from fastapi import FastAPI, HTTPException, BackgroundTasks
+from fastapi.responses import JSONResponse
+from pydantic import BaseModel
+
+# Local imports
+sys.path.append(str(Path(__file__).parent))
+
+from core.system_orchestrator import SystemOrchestrator, create_system_orchestrator
+from core.activity_memory import TaskComplexity
+from agents.taisho import TaskPriority
+
+# Configure logging
+logging.basicConfig(
+    level=logging.INFO,
+    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
+)
+logger = logging.getLogger(__name__)
+
+# Global system orchestrator
+system_orchestrator: Optional[SystemOrchestrator] = None
+
+# FastAPI app
+app = FastAPI(
+    title="å°†è»ã‚·ã‚¹ãƒ†ãƒ  v8.0.1 API",
+    description="Complete AI Development Environment - Shogun System v8.0.1",
+    version="8.0.1"
+)
+
+
+# Request/Response Models
+class TaskRequest(BaseModel):
+    """Task processing request"""
+    description: str
+    complexity: str = "medium"  # simple, medium, complex, strategic
+    priority: str = "medium"    # low, medium, high, critical
+    context: Optional[str] = None
+    requires_execution: bool = False
+    requires_latest_info: bool = False
+    requested_by: str = "api"
+
+
+class TaskResponse(BaseModel):
+    """Task processing response"""
+    success: bool
+    response: Optional[str] = None
+    reasoning: Optional[str] = None
+    confidence: Optional[str] = None
+    processing_time_seconds: Optional[float] = None
+    tools_used: Optional[list] = None
+    similar_tasks_found: Optional[int] = None
+    knowledge_retrieved: Optional[int] = None
+    code_executed: Optional[bool] = None
+    execution_successful: Optional[bool] = None
+    task_id: Optional[str] = None
+    system_status: Optional[str] = None
+    error: Optional[str] = None
+
+
+class SystemStatusResponse(BaseModel):
+    """System status response"""
+    system_status: str
+    system_mode: str
+    version: str
+    uptime_hours: float
+    component_health: Dict[str, str]
+    performance_metrics: Dict[str, Any]
+    background_tasks_active: int
+    checked_at: str
+
+
+# API Endpoints
+
+@app.get("/")
+async def root():
+    """Root endpoint with system information"""
+    return {
+        "name": "å°†è»ã‚·ã‚¹ãƒ†ãƒ  v8.0",
+        "description": "Complete AI Development Environment",
+        "version": "8.0.0",
+        "status": system_orchestrator.status.value if system_orchestrator else "not_initialized",
+        "endpoints": {
+            "/health": "System health check",
+            "/status": "Detailed system status",
+            "/task": "Process task (POST)",
+            "/metrics": "System metrics",
+            "/docs": "API documentation"
+        }
+    }
+
+
+@app.get("/health")
+async def health_check():
+    """Simple health check endpoint"""
+    if not system_orchestrator:
+        raise HTTPException(status_code=503, detail="System not initialized")
+    
+    status = await system_orchestrator.get_system_status()
+    
+    if status["system_status"] in ["healthy", "degraded"]:
+        return {"status": "ok", "system_status": status["system_status"]}
+    else:
+        raise HTTPException(
+            status_code=503, 
+            detail=f"System unhealthy: {status['system_status']}"
+        )
+
+
+@app.get("/status", response_model=SystemStatusResponse)
+async def get_system_status():
+    """Get detailed system status and metrics"""
+    if not system_orchestrator:
+        raise HTTPException(status_code=503, detail="System not initialized")
+    
+    try:
+        status = await system_orchestrator.get_system_status()
+        return SystemStatusResponse(**status)
+    except Exception as e:
+        logger.error(f"Status check failed: {e}")
+        raise HTTPException(status_code=500, detail=str(e))
+
+
+@app.post("/task", response_model=TaskResponse)
+async def process_task(request: TaskRequest):
+    """Process a task through the v8.0 system"""
+    if not system_orchestrator:
+        raise HTTPException(status_code=503, detail="System not initialized")
+    
+    try:
+        # Convert string enums
+        complexity_map = {
+            "simple": TaskComplexity.SIMPLE,
+            "medium": TaskComplexity.MEDIUM,
+            "complex": TaskComplexity.COMPLEX,
+            "strategic": TaskComplexity.STRATEGIC
+        }
+        
+        priority_map = {
+            "low": TaskPriority.LOW,
+            "medium": TaskPriority.MEDIUM,
+            "high": TaskPriority.HIGH,
+            "critical": TaskPriority.CRITICAL
+        }
+        
+        complexity = complexity_map.get(request.complexity.lower(), TaskComplexity.MEDIUM)
+        priority = priority_map.get(request.priority.lower(), TaskPriority.MEDIUM)
+        
+        # Process task
+        result = await system_orchestrator.process_task(
+            description=request.description,
+            complexity=complexity,
+            priority=priority,
+            context=request.context,
+            requires_execution=request.requires_execution,
+            requires_latest_info=request.requires_latest_info,
+            requested_by=request.requested_by
+        )
+        
+        return TaskResponse(**result)
+        
+    except Exception as e:
+        logger.error(f"Task processing failed: {e}")
+        return TaskResponse(
+            success=False,
+            error=str(e),
+            system_status=system_orchestrator.status.value if system_orchestrator else "error"
+        )
+
+
+@app.get("/metrics")
+async def get_metrics():
+    """Get system performance metrics"""
+    if not system_orchestrator:
+        raise HTTPException(status_code=503, detail="System not initialized")
+    
+    try:
+        status = await system_orchestrator.get_system_status()
+        return {
+            "performance_metrics": status["performance_metrics"],
+            "component_health": status["component_health"],
+            "uptime_hours": status["uptime_hours"],
+            "system_mode": status["system_mode"],
+            "collected_at": datetime.utcnow().isoformat()
+        }
+    except Exception as e:
+        logger.error(f"Metrics collection failed: {e}")
+        raise HTTPException(status_code=500, detail=str(e))
+
+
+@app.post("/admin/shutdown")
+async def shutdown_system(background_tasks: BackgroundTasks):
+    """Graceful system shutdown (admin only)"""
+    if not system_orchestrator:
+        return {"message": "System not running"}
+    
+    background_tasks.add_task(perform_shutdown)
+    return {"message": "Shutdown initiated"}
+
+
+async def perform_shutdown():
+    """Perform graceful shutdown"""
+    global system_orchestrator
+    if system_orchestrator:
+        await system_orchestrator.shutdown()
+        system_orchestrator = None
+
+
+# System Lifecycle Management
+
+async def startup_system():
+    """Initialize the v8.0 system on startup"""
+    global system_orchestrator
+    
+    try:
+        logger.info("ğŸ¯ Starting Shogun System v8.0...")
+        
+        # Initialize system orchestrator
+        config_path = Path(__file__).parent / "config" / "settings.yaml"
+        system_orchestrator = await create_system_orchestrator(str(config_path))
+        
+        if system_orchestrator:
+            logger.info("ğŸŒ Shogun System v8.0 startup complete")
+            return True
+        else:
+            logger.error("âŒ Failed to initialize system orchestrator")
+            return False
+            
+    except Exception as e:
+        logger.error(f"System startup failed: {e}")
+        return False
+
+
+async def shutdown_handler():
+    """Handle graceful shutdown"""
+    global system_orchestrator
+    
+    logger.info("ğŸ¯ Shutting down Shogun System v8.0...")
+    
+    if system_orchestrator:
+        await system_orchestrator.shutdown()
+        system_orchestrator = None
+    
+    logger.info("ğŸŒ Shogun System v8.0 shutdown complete")
+
+
+# Signal handlers for graceful shutdown
+def signal_handler(signum, frame):
+    """Handle shutdown signals"""
+    logger.info(f"Received signal {signum}, initiating shutdown...")
+    asyncio.create_task(shutdown_handler())
+    sys.exit(0)
+
+
+# CLI Interface Functions
+
+async def cli_interactive_mode():
+    """Interactive CLI mode for direct system interaction"""
+    if not system_orchestrator:
+        print("âŒ System not initialized")
+        return
+    
+    print("ğŸ¯ å°†è»ã‚·ã‚¹ãƒ†ãƒ  v8.0 - Interactive Mode")
+    print("Type 'exit' to quit, 'help' for commands")
+    
+    while True:
+        try:
+            user_input = input("\nä¾å¤§å°†> ").strip()
+            
+            if user_input.lower() == 'exit':
+                break
+            elif user_input.lower() == 'help':
+                print("""
+Available commands:
+  help              - Show this help
+  status            - Show system status
+  task <description> - Process a task
+  metrics           - Show performance metrics
+  exit              - Exit interactive mode
+                """)
+                continue
+            elif user_input.lower() == 'status':
+                status = await system_orchestrator.get_system_status()
+                print(f"System Status: {status['system_status']}")
+                print(f"Uptime: {status['uptime_hours']:.1f} hours")
+                print(f"Tasks Processed: {status['performance_metrics']['total_tasks_processed']}")
+                continue
+            elif user_input.lower() == 'metrics':
+                status = await system_orchestrator.get_system_status()
+                metrics = status['performance_metrics']
+                print(f"ğŸ“Š System Metrics:")
+                print(f"  Total Tasks: {metrics['total_tasks_processed']}")
+                print(f"  Success Rate: {metrics.get('success_rate', 0):.1f}%")
+                print(f"  Avg Processing Time: {metrics['avg_processing_time_seconds']:.2f}s")
+                print(f"  Knowledge Entries: {metrics['knowledge_base_entries']}")
+                print(f"  Memory Records: {metrics['activity_memory_records']}")
+                continue
+            elif user_input.startswith('task '):
+                task_description = user_input[5:].strip()
+                if task_description:
+                    print("ğŸ¤” Processing task...")
+                    result = await system_orchestrator.process_task(
+                        description=task_description,
+                        requested_by="cli"
+                    )
+                    
+                    if result['success']:
+                        print(f"âœ… {result['response']}")
+                        print(f"ğŸ“ˆ Confidence: {result['confidence']}")
+                        print(f"â±ï¸  Time: {result['processing_time_seconds']:.2f}s")
+                        if result['tools_used']:
+                            print(f"ğŸ› ï¸  Tools: {', '.join(result['tools_used'])}")
+                    else:
+                        print(f"âŒ Error: {result.get('error', 'Unknown error')}")
+                else:
+                    print("Please provide a task description")
+                continue
+            elif user_input:
+                # Treat as task
+                print("ğŸ¤” Processing task...")
+                result = await system_orchestrator.process_task(
+                    description=user_input,
+                    requested_by="cli"
+                )
+                
+                if result['success']:
+                    print(f"âœ… {result['response']}")
+                    print(f"ğŸ“ˆ Confidence: {result['confidence']}")
+                    print(f"â±ï¸  Time: {result['processing_time_seconds']:.2f}s")
+                else:
+                    print(f"âŒ Error: {result.get('error', 'Unknown error')}")
+        
+        except KeyboardInterrupt:
+            break
+        except Exception as e:
+            print(f"âŒ Error: {e}")
+    
+    print("ğŸ‘‹ Exiting interactive mode")
+
+
+async def main():
+    """Main application entry point"""
+    import argparse
+    
+    parser = argparse.ArgumentParser(description="å°†è»ã‚·ã‚¹ãƒ†ãƒ  v8.0")
+    parser.add_argument("--mode", choices=["server", "cli"], default="server",
+                       help="Run mode: server (API) or cli (interactive)")
+    parser.add_argument("--host", default="0.0.0.0", help="API server host")
+    parser.add_argument("--port", type=int, default=8080, help="API server port")
+    parser.add_argument("--config", default="config/settings.yaml", help="Configuration file")
+    
+    args = parser.parse_args()
+    
+    # Setup signal handlers
+    signal.signal(signal.SIGINT, signal_handler)
+    signal.signal(signal.SIGTERM, signal_handler)
+    
+    # Initialize system
+    if not await startup_system():
+        logger.error("Failed to start system")
+        sys.exit(1)
+    
+    try:
+        if args.mode == "server":
+            # Run API server
+            logger.info(f"Starting API server on {args.host}:{args.port}")
+            
+            config = uvicorn.Config(
+                app,
+                host=args.host,
+                port=args.port,
+                log_level="info",
+                access_log=True
+            )
+            
+            server = uvicorn.Server(config)
+            await server.serve()
+            
+        elif args.mode == "cli":
+            # Run interactive CLI
+            await cli_interactive_mode()
+            
+    except KeyboardInterrupt:
+        logger.info("Received interrupt signal")
+    finally:
+        await shutdown_handler()
+
+
+if __name__ == "__main__":
+    try:
+        asyncio.run(main())
+    except KeyboardInterrupt:
+        logger.info("Application interrupted by user")
+    except Exception as e:
+        logger.error(f"Application error: {e}")
+        sys.exit(1)
\ No newline at end of file
diff --git a/providers/__init__.py b/providers/__init__.py
new file mode 100644
index 0000000..e69de29
diff --git a/providers/anthropic_api.py b/providers/anthropic_api.py
new file mode 100644
index 0000000..20eb348
--- /dev/null
+++ b/providers/anthropic_api.py
@@ -0,0 +1,252 @@
+"""Enhanced Anthropic API Provider with Latest Models and Error Handling
+
+Proç‰ˆ claude-cli ãŒåˆ¶é™ã«é”ã—ãŸå ´åˆã«ä½¿ç”¨ã€‚
+Latest Claude models (Opus 4.5 / Sonnet 4.5) with comprehensive error handling.
+"""
+
+import asyncio
+import logging
+import os
+import time
+from typing import AsyncIterator, Dict, Any, Optional
+
+logger = logging.getLogger("shogun.provider.anthropic_api")
+
+try:
+    import anthropic
+    HAS_ANTHROPIC = True
+except ImportError:
+    HAS_ANTHROPIC = False
+
+
+class AnthropicAPIProvider:
+    """Enhanced Anthropic Messages API client with latest models."""
+
+    # Latest model mapping: role -> actual API model ID (January 2025)
+    # Updated to use latest Claude Opus 4.5 and Sonnet 4.5
+    MODEL_MAP = {
+        "opus": "claude-opus-4-5-20250514",      # Latest Opus 4.5
+        "sonnet": "claude-sonnet-4-5-20250514",  # Latest Sonnet 4.5
+        "haiku": "claude-3-5-haiku-20241022",    # Latest Haiku 3.5
+    }
+
+    # Cost estimates per task (JPY) - v7.0 pricing
+    COST_PER_TASK = {
+        "claude-opus-4-5-20250514": 24,      # Â¥24/task (Strategic only)
+        "claude-sonnet-4-5-20250514": 5,     # Â¥5/task (Complex tasks)
+        "claude-3-5-haiku-20241022": 0.5,    # Â¥0.5/task
+    }
+
+    # Cost estimates per 1K tokens (JPY) for detailed tracking
+    COST_ESTIMATES = {
+        "claude-opus-4-5-20250514": {"input": 0.015, "output": 0.075},
+        "claude-sonnet-4-5-20250514": {"input": 0.003, "output": 0.015},
+        "claude-3-5-haiku-20241022": {"input": 0.0008, "output": 0.004},
+    }
+
+    def __init__(self, api_key: str | None = None):
+        if not HAS_ANTHROPIC:
+            raise RuntimeError("pip install anthropic required for API fallback")
+        self.api_key = api_key or os.environ.get("ANTHROPIC_API_KEY", "")
+        if not self.api_key:
+            raise ValueError("ANTHROPIC_API_KEY not set")
+        self._client = anthropic.AsyncAnthropic(api_key=self.api_key)
+        
+        # Enhanced tracking
+        self.usage_stats = {
+            "total_requests": 0,
+            "total_tokens": 0,
+            "total_cost_yen": 0.0,
+            "requests_by_model": {},
+            "errors": 0,
+            "last_request_time": 0,
+        }
+
+    async def generate(
+        self,
+        prompt: str,
+        model: str = "sonnet",
+        system: str = "",
+        max_tokens: int = 4096,
+        temperature: float = 0.3,
+    ) -> str:
+        """Enhanced single-turn generation with error handling."""
+        model_id = self.MODEL_MAP.get(model, model)
+        messages = [{"role": "user", "content": prompt}]
+        kwargs: dict = {
+            "model": model_id,
+            "max_tokens": max_tokens,
+            "messages": messages,
+            "temperature": temperature,
+        }
+        if system:
+            kwargs["system"] = system
+
+        logger.info("[API] model=%s (%s), prompt_len=%d", model, model_id, len(prompt))
+        
+        try:
+            # Apply rate limiting
+            await self._apply_rate_limiting()
+            
+            # Make API call with timing
+            start_time = time.time()
+            response = await self._client.messages.create(**kwargs)
+            elapsed = time.time() - start_time
+            
+            # Extract text content
+            text = ""
+            for content in response.content:
+                if hasattr(content, 'text'):
+                    text += content.text
+                    
+            # Update statistics
+            await self._update_usage_stats(model_id, response, elapsed)
+            
+            logger.info("[API] âœ… response_len=%d, time=%.2fs", len(text), elapsed)
+            return text
+            
+        except anthropic.RateLimitError as e:
+            self.usage_stats["errors"] += 1
+            logger.warning("[API] âŒ Rate limit: %s", e)
+            raise
+            
+        except anthropic.AuthenticationError as e:
+            self.usage_stats["errors"] += 1
+            logger.error("[API] âŒ Auth error: %s", e)
+            raise
+            
+        except Exception as e:
+            self.usage_stats["errors"] += 1
+            logger.error("[API] âŒ Unexpected error: %s", e)
+            raise
+
+    async def chat(
+        self,
+        messages: list[dict],
+        model: str = "sonnet",
+        system: str = "",
+        max_tokens: int = 4096,
+        temperature: float = 0.3,
+    ) -> str:
+        """Multi-turn chat."""
+        model_id = self.MODEL_MAP.get(model, model)
+        kwargs: dict = {
+            "model": model_id,
+            "max_tokens": max_tokens,
+            "messages": messages,
+            "temperature": temperature,
+        }
+        if system:
+            kwargs["system"] = system
+
+        response = await self._client.messages.create(**kwargs)
+        return response.content[0].text
+
+    async def generate_stream(
+        self,
+        prompt: str,
+        model: str = "sonnet",
+        system: str = "",
+        max_tokens: int = 4096,
+        temperature: float = 0.3,
+    ) -> AsyncIterator[str]:
+        """Streaming generation."""
+        model_id = self.MODEL_MAP.get(model, model)
+        messages = [{"role": "user", "content": prompt}]
+        kwargs: dict = {
+            "model": model_id,
+            "max_tokens": max_tokens,
+            "messages": messages,
+            "temperature": temperature,
+        }
+        if system:
+            kwargs["system"] = system
+
+        async with self._client.messages.stream(**kwargs) as stream:
+            async for text in stream.text_stream:
+                yield text
+
+    async def _apply_rate_limiting(self):
+        """Apply basic rate limiting."""
+        current_time = time.time()
+        time_since_last = current_time - self.usage_stats["last_request_time"]
+        
+        # Minimum 0.5 seconds between requests
+        if time_since_last < 0.5:
+            wait_time = 0.5 - time_since_last
+            await asyncio.sleep(wait_time)
+        
+        self.usage_stats["last_request_time"] = time.time()
+    
+    async def _update_usage_stats(self, model_id: str, response: Any, elapsed: float):
+        """Update usage statistics with cost estimation."""
+        self.usage_stats["total_requests"] += 1
+        
+        # Track by model
+        model_stats = self.usage_stats["requests_by_model"]
+        if model_id not in model_stats:
+            model_stats[model_id] = {"requests": 0, "tokens": 0, "cost_yen": 0.0}
+        
+        model_stats[model_id]["requests"] += 1
+        
+        # Track token usage and cost if available
+        if hasattr(response, 'usage'):
+            input_tokens = getattr(response.usage, 'input_tokens', 0)
+            output_tokens = getattr(response.usage, 'output_tokens', 0)
+            total_tokens = input_tokens + output_tokens
+            
+            self.usage_stats["total_tokens"] += total_tokens
+            model_stats[model_id]["tokens"] += total_tokens
+            
+            # Calculate cost
+            if model_id in self.COST_ESTIMATES:
+                rates = self.COST_ESTIMATES[model_id]
+                cost = (input_tokens / 1000 * rates["input"] + 
+                       output_tokens / 1000 * rates["output"])
+                self.usage_stats["total_cost_yen"] += cost
+                model_stats[model_id]["cost_yen"] += cost
+                
+                logger.debug(
+                    "[API] Tokens: %d+%d=%d, Cost: Â¥%.4f", 
+                    input_tokens, output_tokens, total_tokens, cost
+                )
+    
+    def get_usage_summary(self) -> Dict[str, Any]:
+        """Get comprehensive usage summary."""
+        return dict(self.usage_stats)
+    
+    def reset_usage_stats(self):
+        """Reset usage statistics."""
+        self.usage_stats = {
+            "total_requests": 0,
+            "total_tokens": 0,
+            "total_cost_yen": 0.0,
+            "requests_by_model": {},
+            "errors": 0,
+            "last_request_time": 0,
+        }
+    
+    async def health_check(self) -> Dict[str, Any]:
+        """Perform health check."""
+        try:
+            response = await self.generate(
+                prompt="Health check - respond with 'OK'",
+                model="sonnet",
+                max_tokens=10,
+            )
+            return {
+                "status": "healthy",
+                "response": response,
+                "model": self.MODEL_MAP["sonnet"],
+            }
+        except Exception as e:
+            return {
+                "status": "unhealthy",
+                "error": str(e),
+            }
+
+    async def close(self) -> None:
+        """Clean up resources."""
+        if hasattr(self._client, "_client"):
+            await self._client._client.aclose()
+        logger.info("[API] Connection closed")
diff --git a/providers/claude_cli.py b/providers/claude_cli.py
new file mode 100644
index 0000000..cdd6038
--- /dev/null
+++ b/providers/claude_cli.py
@@ -0,0 +1,192 @@
+"""Claude CLI Provider - Proç‰ˆ claude-cli (npm) ãƒ©ãƒƒãƒ‘ãƒ¼
+
+Primary execution path for Shogun (Opus) and Karo (Sonnet).
+Proç‰ˆã®åˆ¶é™ãŒã‹ã‹ã£ãŸå ´åˆã€Anthropic APIã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã™ã‚‹ã€‚
+
+Usage:
+    provider = ClaudeCLIProvider()
+    result = await provider.generate("prompt", model="opus")
+"""
+
+import asyncio
+import json
+import logging
+import os
+import re
+import shutil
+from typing import AsyncIterator
+
+logger = logging.getLogger("shogun.provider.claude_cli")
+
+# Rate-limit detection patterns
+RATE_LIMIT_PATTERNS = [
+    "rate limit",
+    "too many requests",
+    "usage limit",
+    "capacity",
+    "throttl",
+    "429",
+    "overloaded",
+]
+
+
+class ClaudeCLIProvider:
+    """Claude Code CLI (npm @anthropic-ai/claude-code) wrapper.
+
+    Uses the `claude` command with `--print` for non-interactive mode.
+    Detects Pro rate limiting and signals for API fallback.
+    """
+
+    def __init__(self, cli_path: str | None = None):
+        self.cli_path = cli_path or shutil.which("claude") or "claude"
+        self._available: bool | None = None
+
+    async def check_available(self) -> bool:
+        """Check if claude CLI is installed and accessible."""
+        if self._available is not None:
+            return self._available
+        try:
+            proc = await asyncio.create_subprocess_exec(
+                self.cli_path, "--version",
+                stdout=asyncio.subprocess.PIPE,
+                stderr=asyncio.subprocess.PIPE,
+            )
+            stdout, _ = await asyncio.wait_for(proc.communicate(), timeout=10)
+            self._available = proc.returncode == 0
+            if self._available:
+                version = stdout.decode().strip()
+                logger.info("Claude CLI available: %s", version)
+            return self._available
+        except (FileNotFoundError, asyncio.TimeoutError):
+            self._available = False
+            logger.warning("Claude CLI not found at: %s", self.cli_path)
+            return False
+
+    async def generate(
+        self,
+        prompt: str,
+        model: str = "sonnet",
+        system_prompt: str = "",
+        max_turns: int = 1,
+        cwd: str | None = None,
+    ) -> "CLIResult":
+        """Execute a prompt via claude --print.
+
+        Args:
+            prompt: The user prompt.
+            model: Model name (opus, sonnet, haiku).
+            system_prompt: System prompt to prepend.
+            max_turns: Max agentic turns.
+            cwd: Working directory for the CLI.
+
+        Returns:
+            CLIResult with text, success flag, and rate_limited flag.
+        """
+        if not await self.check_available():
+            return CLIResult(
+                text="",
+                success=False,
+                rate_limited=False,
+                error="Claude CLI not available",
+            )
+
+        # Build command
+        cmd = [self.cli_path, "--print"]
+
+        # Model selection
+        if model:
+            cmd.extend(["--model", model])
+
+        # Max turns
+        if max_turns > 1:
+            cmd.extend(["--max-turns", str(max_turns)])
+
+        # System prompt via --system-prompt
+        if system_prompt:
+            cmd.extend(["--system-prompt", system_prompt])
+
+        # The prompt itself is passed via stdin
+        full_prompt = prompt
+
+        logger.info(
+            "[CLI] model=%s, prompt_len=%d, max_turns=%d",
+            model, len(prompt), max_turns,
+        )
+
+        try:
+            proc = await asyncio.create_subprocess_exec(
+                *cmd,
+                stdin=asyncio.subprocess.PIPE,
+                stdout=asyncio.subprocess.PIPE,
+                stderr=asyncio.subprocess.PIPE,
+                cwd=cwd or os.getcwd(),
+            )
+            stdout, stderr = await asyncio.wait_for(
+                proc.communicate(input=full_prompt.encode("utf-8")),
+                timeout=600,  # 10 min max
+            )
+
+            stdout_text = stdout.decode("utf-8", errors="replace").strip()
+            stderr_text = stderr.decode("utf-8", errors="replace").strip()
+
+            # Check for rate limiting
+            combined = (stdout_text + stderr_text).lower()
+            rate_limited = any(p in combined for p in RATE_LIMIT_PATTERNS)
+
+            if rate_limited:
+                logger.warning("[CLI] Proç‰ˆåˆ¶é™æ¤œå‡ºã€‚APIãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ¨å¥¨ã€‚")
+                return CLIResult(
+                    text=stdout_text,
+                    success=False,
+                    rate_limited=True,
+                    error="Pro rate limit reached",
+                )
+
+            if proc.returncode != 0:
+                logger.error("[CLI] Exit code %d: %s", proc.returncode, stderr_text[:200])
+                return CLIResult(
+                    text=stdout_text,
+                    success=False,
+                    rate_limited=False,
+                    error=f"CLI error (exit {proc.returncode}): {stderr_text[:200]}",
+                )
+
+            return CLIResult(
+                text=stdout_text,
+                success=True,
+                rate_limited=False,
+                error="",
+            )
+
+        except asyncio.TimeoutError:
+            logger.error("[CLI] Timeout after 600s")
+            return CLIResult(
+                text="",
+                success=False,
+                rate_limited=False,
+                error="CLI timeout (600s)",
+            )
+        except Exception as e:
+            logger.error("[CLI] Exception: %s", e)
+            return CLIResult(
+                text="",
+                success=False,
+                rate_limited=False,
+                error=str(e),
+            )
+
+
+class CLIResult:
+    """Result from Claude CLI execution."""
+
+    __slots__ = ("text", "success", "rate_limited", "error")
+
+    def __init__(self, text: str, success: bool, rate_limited: bool, error: str = ""):
+        self.text = text
+        self.success = success
+        self.rate_limited = rate_limited
+        self.error = error
+
+    def __repr__(self) -> str:
+        status = "ok" if self.success else ("rate_limited" if self.rate_limited else "error")
+        return f"<CLIResult status={status} len={len(self.text)}>"
diff --git a/providers/openvino_client.py b/providers/openvino_client.py
new file mode 100644
index 0000000..422536a
--- /dev/null
+++ b/providers/openvino_client.py
@@ -0,0 +1,107 @@
+"""OpenVINO Client - ä¾å¤§å°†R1æ¨è«–ã‚µãƒ¼ãƒãƒ¼æ¥ç¶š
+
+CT 101 (192.168.1.11:11434) ã§ç¨¼åƒã™ã‚‹ OpenVINO R1 ã‚µãƒ¼ãƒãƒ¼ã¸ã®
+HTTP ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã€‚Ollamaäº’æ›APIãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚’ä½¿ç”¨ã€‚
+"""
+
+import logging
+from typing import Any
+
+import httpx
+
+logger = logging.getLogger("shogun.provider.openvino")
+
+
+class OpenVINOClient:
+    """HTTP client for the OpenVINO R1 inference server on CT 101."""
+
+    def __init__(self, base_url: str = "http://192.168.1.11:11434"):
+        self.base_url = base_url.rstrip("/")
+        self._client = httpx.AsyncClient(
+            base_url=self.base_url,
+            timeout=httpx.Timeout(connect=10.0, read=300.0, write=10.0, pool=10.0),
+        )
+
+    async def health(self) -> bool:
+        """Check if the OpenVINO server is running."""
+        try:
+            resp = await self._client.get("/")
+            return resp.status_code == 200
+        except (httpx.ConnectError, httpx.TimeoutException):
+            logger.warning("OpenVINO server unreachable: %s", self.base_url)
+            return False
+
+    async def generate(
+        self,
+        prompt: str,
+        max_tokens: int = 1024,
+        temperature: float = 0.6,
+        system: str = "",
+    ) -> str:
+        """Generate text from the R1 model.
+
+        Args:
+            prompt: Input prompt text.
+            max_tokens: Maximum tokens to generate.
+            temperature: Sampling temperature.
+            system: System prompt (prepended to prompt).
+
+        Returns:
+            Generated text response.
+        """
+        if system:
+            full_prompt = f"{system}\n\n{prompt}"
+        else:
+            full_prompt = prompt
+
+        payload = {
+            "model": "taisho-openvino",
+            "prompt": full_prompt,
+            "max_tokens": max_tokens,
+            "temperature": temperature,
+        }
+
+        logger.info("[R1] prompt_len=%d, max_tokens=%d", len(full_prompt), max_tokens)
+
+        try:
+            resp = await self._client.post("/api/generate", json=payload)
+            resp.raise_for_status()
+            data = resp.json()
+            result = data.get("response", "")
+            logger.info("[R1] response_len=%d", len(result))
+            return result
+        except httpx.TimeoutException:
+            logger.error("[R1] Timeout (300s)")
+            raise
+        except httpx.HTTPStatusError as e:
+            logger.error("[R1] HTTP error: %s", e)
+            raise
+
+    async def generate_with_think(
+        self,
+        prompt: str,
+        max_tokens: int = 2000,
+        temperature: float = 0.6,
+        system: str = "",
+    ) -> dict[str, str]:
+        """Generate with <think> tag parsing.
+
+        Returns:
+            {"thinking": "...", "answer": "..."}
+        """
+        raw = await self.generate(prompt, max_tokens, temperature, system)
+        return self._parse_think(raw)
+
+    @staticmethod
+    def _parse_think(text: str) -> dict[str, str]:
+        """Parse <think>...</think> from R1 output."""
+        if "<think>" in text and "</think>" in text:
+            think_start = text.index("<think>") + len("<think>")
+            think_end = text.index("</think>")
+            thinking = text[think_start:think_end].strip()
+            answer = text[think_end + len("</think>"):].strip()
+            return {"thinking": thinking, "answer": answer}
+        return {"thinking": "", "answer": text.strip()}
+
+    async def close(self) -> None:
+        await self._client.aclose()
diff --git a/pyproject.toml b/pyproject.toml
new file mode 100644
index 0000000..27212be
--- /dev/null
+++ b/pyproject.toml
@@ -0,0 +1,55 @@
+[build-system]
+requires = ["setuptools>=68.0", "wheel"]
+build-backend = "setuptools.backends._legacy:_Backend"
+
+[project]
+name = "shogun-system"
+version = "8.0.1"
+description = "å°†è»ã‚·ã‚¹ãƒ†ãƒ  v8.0.1: å®Œå…¨è‡ªå¾‹å‹AIé–‹ç™ºç’°å¢ƒ - å®Ÿè¨¼ä¸»ç¾©ãƒ»äºŒé‡è¨˜æ†¶ãƒ»è‡ªå¾‹å­¦ç¿’"
+readme = "README.md"
+requires-python = ">=3.11"
+license = {text = "MIT"}
+
+dependencies = [
+    "fastapi>=0.115.0",
+    "uvicorn[standard]>=0.32.0",
+    "httpx>=0.27.0",
+    "pyyaml>=6.0",
+    "pydantic>=2.0",
+    "python-dotenv>=1.0",
+]
+
+[project.optional-dependencies]
+cloud = ["anthropic>=0.40.0"]
+slack = ["slack-sdk>=3.30.0"]
+groq = ["groq>=0.4.1"]
+notion = ["notion-client>=2.2.1"]
+rag = ["qdrant-client>=1.7.0", "sentence-transformers>=2.2.2"]
+sandbox = ["docker>=6.1.0", "psutil>=5.9.0"]
+ollama = ["ollama>=0.6.0"]
+dev = ["pytest", "pytest-asyncio", "ruff", "black", "mypy"]
+all = [
+    "anthropic>=0.40.0",
+    "slack-sdk>=3.30.0",
+    "groq>=0.4.1",
+    "notion-client>=2.2.1",
+    "qdrant-client>=1.7.0",
+    "sentence-transformers>=2.2.2",
+    "docker>=6.1.0",
+    "psutil>=5.9.0",
+    "ollama>=0.6.0"
+]
+
+[project.scripts]
+shogun = "shogun.cli:main"
+
+[tool.setuptools.packages.find]
+include = ["shogun*"]
+
+[tool.ruff]
+target-version = "py311"
+line-length = 100
+
+[tool.pytest.ini_options]
+asyncio_mode = "auto"
+testpaths = ["tests"]
diff --git a/requirements.txt b/requirements.txt
new file mode 100644
index 0000000..437228b
--- /dev/null
+++ b/requirements.txt
@@ -0,0 +1,70 @@
+# å°†è»ã‚·ã‚¹ãƒ†ãƒ  v8.0.1 - Complete Dependencies
+# Core Framework
+fastapi>=0.115.0
+uvicorn[standard]>=0.32.0
+httpx>=0.27.0
+pyyaml>=6.0
+pydantic>=2.0
+python-dotenv>=1.0
+
+# v8.0 New Features Dependencies
+
+# 1. Knowledge Base (RAG System)
+qdrant-client>=1.7.0               # Qdrant vector database client
+sentence-transformers>=2.2.2       # Embedding generation
+numpy>=1.24.0                      # Vector operations
+torch>=2.0.0                       # PyTorch for transformers
+
+# 2. Activity Memory (é™£ä¸­æ—¥è¨˜)
+sqlalchemy>=2.0.0                  # ORM for database operations
+alembic>=1.13.0                    # Database migrations
+
+# 3. Sandbox Environment (æ¼”ç¿’å ´)
+docker>=6.1.0                      # Docker Python client
+psutil>=5.9.0                      # Process and system monitoring
+
+# 4. Ollama Web Search Integration (10ç•ªè¶³è»½)
+ollama>=0.6.0                      # Ollama Python SDK
+aiohttp>=3.9.0                     # Async HTTP client
+
+# 5. Enhanced Groq Integration with Rate Limiting
+groq>=0.4.1                        # Groq API client
+tenacity>=8.2.0                    # Retry mechanisms with exponential backoff
+ratelimiter>=1.2.0                 # Rate limiting utilities
+
+# 6. Enhanced Integrations
+anthropic>=0.40.0                  # Latest Claude API (Opus 4.5 / Sonnet 4.5)
+notion-client>=2.2.1               # Notion integration
+slack-sdk>=3.30.0                  # Slack integration
+mcp>=0.9.0                         # Model Context Protocol
+
+# 7. Vector Database (alternative to Qdrant for lightweight setup)
+faiss-cpu>=1.7.4                   # Vector database for å®¶è¨“ search
+
+# Development & Testing
+pytest>=7.4.0
+pytest-asyncio>=0.23.0
+black>=23.0.0
+flake8>=6.0.0
+mypy>=1.8.0
+
+# Production Dependencies
+gunicorn>=21.2.0                   # WSGI server
+redis>=5.0.0                       # Caching and message queuing
+celery>=5.3.0                      # Background task processing
+
+# Monitoring & Logging
+structlog>=23.2.0                  # Structured logging
+prometheus-client>=0.19.0          # Metrics collection
+
+# Security
+cryptography>=41.0.0               # Encryption utilities
+python-jose[cryptography]>=3.3.0   # JWT handling
+
+# Optional: Advanced Features
+transformers>=4.36.0               # Hugging Face transformers
+tokenizers>=0.15.0                 # Text tokenization
+
+# Note: Some platform-specific packages may need system installation:
+# - lxc (LXC container management)
+# - docker-compose (Container orchestration)
diff --git a/setup/install.sh b/setup/install.sh
new file mode 100644
index 0000000..4f3dd26
--- /dev/null
+++ b/setup/install.sh
@@ -0,0 +1,200 @@
+#!/bin/bash
+# =============================================================
+# å°†è»ã‚·ã‚¹ãƒ†ãƒ  v7.0 - Controller Installation
+# =============================================================
+# CT 100 (æœ¬é™£) ã§å®Ÿè¡Œã€‚Python + Node.js + MCP + CLI ã‚’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã€‚
+#
+# Usage: bash install.sh
+# =============================================================
+
+set -euo pipefail
+
+SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
+PROJECT_DIR="$(dirname "$SCRIPT_DIR")"
+
+echo "============================================="
+echo "  å°†è»ã‚·ã‚¹ãƒ†ãƒ  v7.0 - æœ¬é™£ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã€Œã‚¹ãƒ”ãƒ¼ãƒ‰ã‚ˆã‚Šè³ªã€"
+echo "  Project: ${PROJECT_DIR}"
+echo "============================================="
+
+# --- [1] ã‚·ã‚¹ãƒ†ãƒ ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ ---
+echo "[1/8] ã‚·ã‚¹ãƒ†ãƒ ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸..."
+apt update && apt upgrade -y
+apt install -y python3-pip python3-venv git curl wget
+
+# --- [2] Node.js ---
+echo ""
+echo "[2/8] Node.js 20..."
+if command -v node &>/dev/null; then
+    echo "  Node.js: $(node --version)"
+else
+    curl -fsSL https://deb.nodesource.com/setup_20.x | bash -
+    apt install -y nodejs
+    echo "  Node.js: $(node --version)"
+fi
+
+# --- [3] Claude CLI (npm) ---
+echo ""
+echo "[3/8] Claude CLI..."
+if command -v claude &>/dev/null; then
+    echo "  Claude CLI: $(claude --version 2>/dev/null || echo 'installed')"
+else
+    npm install -g @anthropic-ai/claude-code
+    echo "  Claude CLI ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å®Œäº†"
+fi
+
+# --- [4] Python venv ---
+echo ""
+echo "[4/8] Pythonä»®æƒ³ç’°å¢ƒ..."
+VENV_DIR="${PROJECT_DIR}/.venv"
+if [ ! -d "$VENV_DIR" ]; then
+    python3 -m venv "$VENV_DIR"
+    echo "  ä½œæˆ: ${VENV_DIR}"
+fi
+
+source "${VENV_DIR}/bin/activate"
+pip install --upgrade pip
+pip install -r "${PROJECT_DIR}/requirements.txt"
+echo "  ä¾å­˜ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸å®Œäº†"
+
+# --- [5] MCP ã‚µãƒ¼ãƒãƒ¼ç¾¤ ---
+echo ""
+echo "[5/8] MCP ã‚µãƒ¼ãƒãƒ¼ (è¶³è»½ Ã— 8)..."
+npm install -g \
+    @modelcontextprotocol/server-filesystem \
+    @modelcontextprotocol/server-github \
+    @modelcontextprotocol/server-fetch \
+    @modelcontextprotocol/server-memory \
+    @modelcontextprotocol/server-postgres \
+    @modelcontextprotocol/server-puppeteer \
+    @modelcontextprotocol/server-brave-search \
+    @modelcontextprotocol/server-slack
+echo "  MCP Ã— 8 ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å®Œäº†"
+
+# --- [6] CLI ã‚·ãƒ§ãƒ¼ãƒˆã‚«ãƒƒãƒˆ ---
+echo ""
+echo "[6/8] CLI ã‚·ãƒ§ãƒ¼ãƒˆã‚«ãƒƒãƒˆ..."
+cat > "${VENV_DIR}/bin/shogun" << WRAPPER
+#!/bin/bash
+SCRIPT_DIR="\$(cd "\$(dirname "\${BASH_SOURCE[0]}")" && pwd)"
+VENV_DIR="\$(dirname "\$SCRIPT_DIR")"
+PROJECT_DIR="${PROJECT_DIR}"
+source "\${VENV_DIR}/bin/activate"
+cd "\$PROJECT_DIR"
+python -m shogun.cli "\$@"
+WRAPPER
+chmod +x "${VENV_DIR}/bin/shogun"
+echo "  CLI: ${VENV_DIR}/bin/shogun"
+
+# --- [7] ç’°å¢ƒå¤‰æ•°ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ ---
+echo ""
+echo "[7/8] ç’°å¢ƒå¤‰æ•°..."
+ENV_FILE="${PROJECT_DIR}/.env"
+if [ ! -f "$ENV_FILE" ]; then
+    cat > "$ENV_FILE" << 'ENV'
+# å°†è»ã‚·ã‚¹ãƒ†ãƒ  v7.0 - ç’°å¢ƒå¤‰æ•°
+
+# Anthropic API (å°†è»/å®¶è€ã®APIãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨)
+# ANTHROPIC_API_KEY=sk-ant-api03-xxxxx
+
+# Groq API (9ç•ªè¶³è»½ãƒ»è¨˜éŒ²ä¿‚)
+# GROQ_API_KEY=gsk_xxxxx
+
+# Notion API (ãƒŠãƒ¬ãƒƒã‚¸DB)
+# NOTION_TOKEN=secret_xxxxx
+# NOTION_DATABASE_ID=xxxxx
+
+# GitHub
+# GITHUB_TOKEN=ghp_xxxxx
+
+# Slack (11ãƒœãƒƒãƒˆ)
+# SLACK_APP_TOKEN=xapp-xxxxx
+# SLACK_TOKEN_SHOGUN=xoxb-xxxxx
+# SLACK_TOKEN_KARO=xoxb-xxxxx
+# SLACK_TOKEN_TAISHO=xoxb-xxxxx
+# SLACK_TOKEN_ASHIGARU_1=xoxb-xxxxx
+# SLACK_TOKEN_ASHIGARU_2=xoxb-xxxxx
+# SLACK_TOKEN_ASHIGARU_3=xoxb-xxxxx
+# SLACK_TOKEN_ASHIGARU_4=xoxb-xxxxx
+# SLACK_TOKEN_ASHIGARU_5=xoxb-xxxxx
+# SLACK_TOKEN_ASHIGARU_6=xoxb-xxxxx
+# SLACK_TOKEN_ASHIGARU_7=xoxb-xxxxx
+# SLACK_TOKEN_ASHIGARU_8=xoxb-xxxxx
+# SLACK_TOKEN_ASHIGARU_9=xoxb-xxxxx
+# SLACK_TOKEN_LIGHT=xoxb-xxxxx
+
+# Brave Search
+# BRAVE_API_KEY=BSAxxxxx
+
+# Database
+# DATABASE_URL=postgresql://user:pass@localhost:5432/shogun
+ENV
+    echo "  .env ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆä½œæˆã€‚APIã‚­ãƒ¼ã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚"
+else
+    echo "  .env æ—¢ã«å­˜åœ¨"
+fi
+
+# --- [8] Gitè‡ªå‹•åŒæœŸ ---
+echo ""
+echo "[8/8] Gitè‡ªå‹•åŒæœŸ..."
+SYNC_SCRIPT="${PROJECT_DIR}/setup/auto_sync.sh"
+cat > "$SYNC_SCRIPT" << 'SYNC'
+#!/bin/bash
+# Detect repo path: GitHub {user}/{repo} â†’ /home/claude/{repo}
+LOCAL_BASE="/home/claude"
+REPO_NAME=$(basename "$(git -C "${PROJECT_DIR}" remote get-url origin 2>/dev/null | sed 's/\.git$//')" 2>/dev/null || echo "")
+REPO_PATH="${LOCAL_BASE}/${REPO_NAME:-$(basename "$PROJECT_DIR")}"
+LOG_FILE="/var/log/repo-sync.log"
+
+log() {
+    echo "[$(date '+%Y-%m-%d %H:%M:%S')] $1" | tee -a "$LOG_FILE"
+}
+
+[ -d "$REPO_PATH" ] || { log "ãƒªãƒã‚¸ãƒˆãƒªæœªæ¤œå‡º: $REPO_PATH"; exit 1; }
+cd "$REPO_PATH"
+
+BRANCH=$(git branch --show-current)
+git fetch origin >> "$LOG_FILE" 2>&1
+
+LOCAL=$(git rev-parse HEAD)
+REMOTE=$(git rev-parse "origin/$BRANCH" 2>/dev/null)
+
+if [ "$LOCAL" != "$REMOTE" ]; then
+    log "å¤‰æ›´æ¤œå‡º: ãƒªãƒ¢ãƒ¼ãƒˆã‹ã‚‰æ›´æ–°"
+    if ! git diff-index --quiet HEAD --; then
+        log "ãƒ­ãƒ¼ã‚«ãƒ«å¤‰æ›´ã‚ã‚Š: stashå®Ÿè¡Œ"
+        git stash save "auto-stash $(date +%s)" >> "$LOG_FILE" 2>&1
+    fi
+    git pull origin "$BRANCH" >> "$LOG_FILE" 2>&1
+    if [ $? -eq 0 ]; then
+        log "åŒæœŸæˆåŠŸ: $BRANCH"
+    else
+        log "ã‚¨ãƒ©ãƒ¼: pullå¤±æ•—"
+        exit 1
+    fi
+else
+    log "åŒæœŸä¸è¦: æœ€æ–°çŠ¶æ…‹"
+fi
+SYNC
+chmod +x "$SYNC_SCRIPT"
+
+# Add cron (5åˆ†ã”ã¨)
+if ! crontab -l 2>/dev/null | grep -q "auto_sync"; then
+    (crontab -l 2>/dev/null; echo "*/5 * * * * ${SYNC_SCRIPT}") | crontab -
+    echo "  cronè¨­å®š: 5åˆ†ã”ã¨ã«GitåŒæœŸ"
+fi
+
+echo ""
+echo "============================================="
+echo "  æœ¬é™£ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—å®Œäº†!"
+echo ""
+echo "  ä½¿ã„æ–¹:"
+echo "    source ${VENV_DIR}/bin/activate"
+echo "    shogun health       # ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯"
+echo "    shogun repl         # å¯¾è©±ãƒ¢ãƒ¼ãƒ‰"
+echo "    shogun ask 'Hello'  # ã‚¿ã‚¹ã‚¯å®Ÿè¡Œ"
+echo "    shogun server       # APIã‚µãƒ¼ãƒãƒ¼èµ·å‹•"
+echo ""
+echo "  PATHè¿½åŠ  (shell profile):"
+echo "    export PATH=\"${VENV_DIR}/bin:\$PATH\""
+echo "============================================="
diff --git a/setup/maintenance.sh b/setup/maintenance.sh
new file mode 100644
index 0000000..fe2b748
--- /dev/null
+++ b/setup/maintenance.sh
@@ -0,0 +1,170 @@
+#!/bin/bash
+# =============================================================
+# å°†è»ã‚·ã‚¹ãƒ†ãƒ  v7.0 - Monthly Maintenance (åçœä¼š) Setup
+# =============================================================
+# æœˆæ¬¡ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹ã® cron è¨­å®šã‚¹ã‚¯ãƒªãƒ—ãƒˆã€‚
+# CT 100 (æœ¬é™£) ã§å®Ÿè¡Œã€‚
+#
+# Usage: bash maintenance.sh [install|uninstall|run|status]
+# =============================================================
+
+set -euo pipefail
+
+SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
+PROJECT_DIR="$(dirname "$SCRIPT_DIR")"
+VENV_DIR="${PROJECT_DIR}/.venv"
+MAINTENANCE_SCRIPT="${PROJECT_DIR}/setup/run_maintenance.sh"
+LOG_DIR="${PROJECT_DIR}/logs"
+CRON_JOB="0 9 1 * * ${MAINTENANCE_SCRIPT} >> ${LOG_DIR}/maintenance.log 2>&1"
+
+# Create maintenance runner script
+create_runner() {
+    mkdir -p "${LOG_DIR}"
+    mkdir -p "${PROJECT_DIR}/reports/maintenance"
+
+    cat > "$MAINTENANCE_SCRIPT" << RUNNER
+#!/bin/bash
+# å°†è»ã‚·ã‚¹ãƒ†ãƒ  - Monthly Maintenance Runner
+# Auto-generated by maintenance.sh
+
+set -euo pipefail
+
+PROJECT_DIR="${PROJECT_DIR}"
+VENV_DIR="${VENV_DIR}"
+
+echo "========================================"
+echo "å°†è»ã‚·ã‚¹ãƒ†ãƒ  æœˆæ¬¡ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹ (åçœä¼š)"
+echo "å®Ÿè¡Œæ—¥æ™‚: \$(date '+%Y-%m-%d %H:%M:%S')"
+echo "========================================"
+
+# Activate venv
+if [ -f "\${VENV_DIR}/bin/activate" ]; then
+    source "\${VENV_DIR}/bin/activate"
+else
+    echo "ERROR: venv not found at \${VENV_DIR}"
+    exit 1
+fi
+
+cd "\$PROJECT_DIR"
+
+# Run maintenance
+python -m shogun.cli maintenance run
+
+echo ""
+echo "========================================"
+echo "ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹å®Œäº†: \$(date '+%Y-%m-%d %H:%M:%S')"
+echo "========================================"
+RUNNER
+
+    chmod +x "$MAINTENANCE_SCRIPT"
+    echo "  ä½œæˆ: ${MAINTENANCE_SCRIPT}"
+}
+
+install_cron() {
+    create_runner
+
+    # Check if already installed
+    if crontab -l 2>/dev/null | grep -q "run_maintenance.sh"; then
+        echo "  cron æ—¢ã«è¨­å®šæ¸ˆã¿"
+        return
+    fi
+
+    # Add cron job
+    (crontab -l 2>/dev/null; echo "$CRON_JOB") | crontab -
+    echo "  cron è¨­å®šå®Œäº†: æ¯æœˆ1æ—¥ 9:00 JST"
+}
+
+uninstall_cron() {
+    if crontab -l 2>/dev/null | grep -q "run_maintenance.sh"; then
+        crontab -l | grep -v "run_maintenance.sh" | crontab -
+        echo "  cron å‰Šé™¤å®Œäº†"
+    else
+        echo "  cron è¨­å®šãªã—"
+    fi
+}
+
+show_status() {
+    echo "æœˆæ¬¡ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹è¨­å®šçŠ¶æ³:"
+    echo ""
+
+    # Check runner script
+    if [ -f "$MAINTENANCE_SCRIPT" ]; then
+        echo "  âœ“ å®Ÿè¡Œã‚¹ã‚¯ãƒªãƒ—ãƒˆ: ${MAINTENANCE_SCRIPT}"
+    else
+        echo "  âœ— å®Ÿè¡Œã‚¹ã‚¯ãƒªãƒ—ãƒˆ: æœªä½œæˆ"
+    fi
+
+    # Check cron
+    if crontab -l 2>/dev/null | grep -q "run_maintenance.sh"; then
+        echo "  âœ“ cron: è¨­å®šæ¸ˆã¿ (æ¯æœˆ1æ—¥ 9:00)"
+    else
+        echo "  âœ— cron: æœªè¨­å®š"
+    fi
+
+    # Check log directory
+    if [ -d "$LOG_DIR" ]; then
+        echo "  âœ“ ãƒ­ã‚°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: ${LOG_DIR}"
+    else
+        echo "  âœ— ãƒ­ã‚°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: æœªä½œæˆ"
+    fi
+
+    # Check reports directory
+    REPORTS_DIR="${PROJECT_DIR}/reports/maintenance"
+    if [ -d "$REPORTS_DIR" ]; then
+        REPORT_COUNT=$(ls -1 "$REPORTS_DIR"/*.json 2>/dev/null | wc -l || echo "0")
+        echo "  âœ“ ãƒ¬ãƒãƒ¼ãƒˆ: ${REPORT_COUNT}ä»¶ (${REPORTS_DIR})"
+    else
+        echo "  âœ— ãƒ¬ãƒãƒ¼ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: æœªä½œæˆ"
+    fi
+
+    # Next maintenance
+    echo ""
+    if [ -f "${VENV_DIR}/bin/activate" ]; then
+        source "${VENV_DIR}/bin/activate"
+        cd "$PROJECT_DIR"
+        python -c "
+from shogun.core.maintenance import MaintenanceManager
+m = MaintenanceManager()
+next_date = m.get_next_maintenance_date()
+print(f'  æ¬¡å›ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹: {next_date.strftime(\"%Y-%m-%d %H:%M\")}')
+" 2>/dev/null || echo "  æ¬¡å›ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹: è¨ˆç®—ä¸å¯"
+    fi
+}
+
+run_now() {
+    if [ ! -f "$MAINTENANCE_SCRIPT" ]; then
+        create_runner
+    fi
+    echo "ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹ã‚’å®Ÿè¡Œã—ã¾ã™..."
+    bash "$MAINTENANCE_SCRIPT"
+}
+
+# â”€â”€â”€ Main â”€â”€â”€
+
+case "${1:-status}" in
+    install)
+        echo "æœˆæ¬¡ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹ cron ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«..."
+        install_cron
+        echo ""
+        show_status
+        ;;
+    uninstall)
+        echo "æœˆæ¬¡ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹ cron ã‚¢ãƒ³ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«..."
+        uninstall_cron
+        ;;
+    run)
+        run_now
+        ;;
+    status)
+        show_status
+        ;;
+    *)
+        echo "Usage: $0 [install|uninstall|run|status]"
+        echo ""
+        echo "  install    cron è¨­å®š (æ¯æœˆ1æ—¥ 9:00)"
+        echo "  uninstall  cron å‰Šé™¤"
+        echo "  run        ä»Šã™ãå®Ÿè¡Œ"
+        echo "  status     è¨­å®šçŠ¶æ³ç¢ºèª"
+        exit 1
+        ;;
+esac
diff --git a/setup/openvino_setup.sh b/setup/openvino_setup.sh
new file mode 100644
index 0000000..d101330
--- /dev/null
+++ b/setup/openvino_setup.sh
@@ -0,0 +1,263 @@
+#!/bin/bash
+# =============================================================
+# å°†è»ã‚·ã‚¹ãƒ†ãƒ  v7.0 - OpenVINO R1 Setup (CyberAgent Japanese Edition)
+# =============================================================
+# CT 101 (ä¾å¤§å°†) å†…ã§å®Ÿè¡Œã€‚
+# CyberAgent DeepSeek-R1-Distill-Qwen-14B-Japanese ã‚’ OpenVINO INT8 ã«å¤‰æ›ã€‚
+#
+# Hardware: HP ProDesk 600 G4 (Core i5-8500 / 24GB RAM)
+# Optimization: CPU Latency Focused
+#
+# Usage:
+#   pct enter 101
+#   bash openvino_setup.sh
+# =============================================================
+
+set -euo pipefail
+
+# ãƒ¢ãƒ‡ãƒ«åã‚’ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªåã«åæ˜ 
+MODEL_ID="cyberagent/DeepSeek-R1-Distill-Qwen-14B-Japanese"
+MODEL_DIR="/opt/openvino/models/deepseek-r1-14b-jp-int8"
+SERVER_DIR="/opt/openvino"
+
+echo "============================================="
+echo "  ä¾å¤§å°† R1 - OpenVINO Setup (JP Optimized)"
+echo "============================================="
+
+# --- [1/6] åŸºæœ¬ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ ---
+echo "[1/6] åŸºæœ¬ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«..."
+apt update && apt upgrade -y
+# libpython3-devç­‰ã¯ãƒ“ãƒ«ãƒ‰æ™‚ã«å¿…é ˆãªå ´åˆãŒã‚ã‚‹ãŸã‚å¿µã®ãŸã‚è¿½åŠ 
+apt install -y build-essential cmake python3-pip python3-venv git wget curl libpython3-dev
+
+# --- [2/6] Pythonç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ— ---
+echo ""
+echo "[2/6] Pythonç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—..."
+# æ—¢å­˜ç’°å¢ƒãŒã‚ã‚Œã°å†ä½œæˆ
+if [ -d "/opt/openvino/venv" ]; then
+    rm -rf /opt/openvino/venv
+fi
+
+python3 -m venv /opt/openvino/venv
+source /opt/openvino/venv/bin/activate
+
+pip install --upgrade pip
+# nncfã¯ãƒ¢ãƒ‡ãƒ«åœ§ç¸®(INT8åŒ–)ã«å¿…é ˆã€‚accelerateã‚‚HuggingFaceé€£æºã§å¿…è¦ã€‚
+pip install \
+    openvino>=2024.0.0 \
+    "openvino-dev>=2024.0.0" \
+    "optimum[openvino,nncf]" \
+    transformers \
+    accelerate \
+    sentencepiece \
+    protobuf \
+    numpy \
+    flask
+
+# --- [3/6] ãƒ¢ãƒ‡ãƒ«å¤‰æ› ---
+echo ""
+echo "[3/6] ãƒ¢ãƒ‡ãƒ«å¤‰æ› (${MODEL_ID} â†’ OpenVINO INT8)..."
+# å¤‰æ›æ¸ˆã¿ã‹ãƒã‚§ãƒƒã‚¯
+if [ -d "$MODEL_DIR" ] && [ -f "$MODEL_DIR/openvino_model.xml" ]; then
+    echo "  ãƒ¢ãƒ‡ãƒ«æ—¢ã«å­˜åœ¨: ${MODEL_DIR} (ã‚¹ã‚­ãƒƒãƒ—)"
+else
+    mkdir -p "$MODEL_DIR"
+    cat > /tmp/convert_r1.py << PYTHON
+from optimum.intel import OVModelForCausalLM
+from transformers import AutoTokenizer
+import nncf
+
+print("[1/3] ãƒ¢ãƒ‡ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ & INT8å¤‰æ›ãƒ­ãƒ¼ãƒ‰ä¸­...")
+# i5-8500ã¯AVX2å¯¾å¿œã€‚INT8é‡å­åŒ–ã§ãƒ¡ãƒ¢ãƒªå¸¯åŸŸå¹…ã®ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã‚’ç·©å’Œã—é«˜é€ŸåŒ–ã™ã‚‹
+model_id = "${MODEL_ID}"
+save_dir = "${MODEL_DIR}"
+
+# export=True ã¨ load_in_8bit=True ã§å¤‰æ›ã¨åœ§ç¸®ã‚’åŒæ™‚ã«è¡Œã†
+model = OVModelForCausalLM.from_pretrained(
+    model_id,
+    export=True,
+    load_in_8bit=True, 
+    compile=False,
+    trust_remote_code=True
+)
+
+tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)
+
+print("[2/3] ãƒ‡ã‚£ã‚¹ã‚¯ã¸ã®ä¿å­˜...")
+model.save_pretrained(save_dir)
+tokenizer.save_pretrained(save_dir)
+
+print(f"å®Œäº†: {save_dir}")
+PYTHON
+
+    # ãƒ¡ãƒ¢ãƒªä¸è¶³ã§è½ã¡ãªã„ã‚ˆã†ã€å¤‰æ›æ™‚ã¯ä¸€æ™‚çš„ã«swapãªã©ãŒæœ‰åŠ¹ã§ã‚ã‚‹ã“ã¨ã‚’æœŸå¾…
+    python3 /tmp/convert_r1.py
+    echo "  å¤‰æ›å®Œäº†"
+fi
+
+# --- [4/6] æ¨è«–ã‚µãƒ¼ãƒãƒ¼ ---
+echo ""
+echo "[4/6] æ¨è«–ã‚µãƒ¼ãƒãƒ¼é…ç½® (i5-8500 æœ€é©åŒ–)..."
+cat > "${SERVER_DIR}/openvino_server.py" << 'PYTHON'
+"""OpenVINO R1 Inference Server (CyberAgent/JP) - CPU Optimized"""
+
+from optimum.intel import OVModelForCausalLM
+from transformers import AutoTokenizer
+import openvino as ov
+from flask import Flask, request, jsonify
+import time
+import os
+
+app = Flask(__name__)
+
+# ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹
+MODEL_PATH = "/opt/openvino/models/deepseek-r1-14b-jp-int8"
+
+class OpenVINOR1:
+    def __init__(self):
+        print("[èµ·å‹•] OpenVINO CoreåˆæœŸåŒ– (i5-8500 Optimized)...")
+        
+        # Core i5-8500 (6 Cores/6 Threads) ç”¨è¨­å®š
+        # NUM_STREAMS=1: ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·é‡è¦–ï¼ˆ1ã¤ã®æ¨è«–ã‚’å…¨ã‚³ã‚¢ã§å…¨åŠ›ã§çµ‚ã‚ã‚‰ã›ã‚‹ï¼‰
+        # INFERENCE_NUM_THREADS=6: ç‰©ç†ã‚³ã‚¢æ•°ã«åˆã‚ã›ã‚‹
+        self.core = ov.Core()
+        
+        # é »ç¹ãªå†ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã‚’é˜²ãã‚­ãƒ£ãƒƒã‚·ãƒ¥è¨­å®š
+        os.makedirs("/opt/openvino/cache", exist_ok=True)
+        self.core.set_property({"CACHE_DIR": "/opt/openvino/cache"})
+
+        print(f"[èµ·å‹•] ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿: {MODEL_PATH}")
+        self.model = OVModelForCausalLM.from_pretrained(
+            MODEL_PATH,
+            device="CPU",
+            ov_config={
+                "PERFORMANCE_HINT": "LATENCY",    # ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆã‚ˆã‚Šå¿œç­”é€Ÿåº¦å„ªå…ˆ
+                "NUM_STREAMS": "1",               # 1ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’æœ€é€Ÿã§è¿”ã™è¨­å®š
+                "INFERENCE_NUM_THREADS": 6,       # i5-8500ã®ç‰©ç†ã‚³ã‚¢æ•°
+                "KV_CACHE_PRECISION": "u8",       # ãƒ¡ãƒ¢ãƒªç¯€ç´„
+                "ENABLE_CPU_PINNING": "YES",      # ã‚¹ãƒ¬ãƒƒãƒ‰ã‚’ã‚³ã‚¢ã«å›ºå®š
+            },
+            compile=True
+        )
+
+        self.tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)
+        print("[èµ·å‹•] å®Œäº†")
+
+    def generate(self, prompt, max_tokens=1024, temperature=0.7):
+        start = time.time()
+        
+        # CyberAgentãƒ¢ãƒ‡ãƒ«æ¨å¥¨ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆãŒã‚ã‚‹å ´åˆã¯ã“ã“ã§èª¿æ•´å¯èƒ½ã ãŒ
+        # åŸºæœ¬ã¯ç”Ÿã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‹ã€å‘¼ã³å‡ºã—å´(Shogun)ã§æ•´å½¢ã•ã‚ŒãŸã‚‚ã®ã‚’æœŸå¾…ã™ã‚‹
+        inputs = self.tokenizer(prompt, return_tensors="pt")
+        
+        outputs = self.model.generate(
+            **inputs,
+            max_new_tokens=max_tokens,
+            temperature=temperature,
+            top_p=0.95,
+            do_sample=True,
+            repetition_penalty=1.1, # æ—¥æœ¬èªã®ç¹°ã‚Šè¿”ã—é˜²æ­¢ã«å°‘ã—å¼·ã‚ã«å…¥ã‚Œã‚‹
+        )
+        
+        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
+        
+        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆéƒ¨åˆ†ã‚’é™¤å»ã—ã¦å¿œç­”ã®ã¿æŠ½å‡º
+        # (tokenizerã®decodeæŒ™å‹•ã«ã‚ˆã‚Šã€å˜ç´”ãªã‚¹ãƒ©ã‚¤ã‚¹ãŒãšã‚Œã‚‹å ´åˆãŒã‚ã‚‹ãŸã‚å®‰å…¨ç­–)
+        if response.startswith(prompt):
+            result = response[len(prompt):]
+        else:
+            result = response # promptãŒå«ã¾ã‚Œã¦ã„ãªã„å ´åˆï¼ˆãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã‚‹ï¼‰
+
+        elapsed = time.time() - start
+        
+        # çµ±è¨ˆ
+        tok_count = len(self.tokenizer.encode(result))
+        tok_per_sec = tok_count / elapsed if elapsed > 0 else 0
+        print(f"[R1] {tok_count} tokens in {elapsed:.1f}s ({tok_per_sec:.1f} tok/s)")
+        
+        return result
+
+r1 = OpenVINOR1()
+
+@app.route("/")
+def index():
+    return jsonify({"status": "ok", "model": "taisho-openvino-jp"})
+
+@app.route("/api/generate", methods=["POST"])
+def generate():
+    data = request.json
+    result = r1.generate(
+        data.get("prompt", ""),
+        data.get("max_tokens", 1024),
+        data.get("temperature", 0.7),
+    )
+    return jsonify({
+        "model": "taisho-openvino-jp",
+        "response": result,
+        "done": True,
+    })
+
+if __name__ == "__main__":
+    app.run(host="0.0.0.0", port=11434)
+PYTHON
+
+echo "  ã‚µãƒ¼ãƒãƒ¼ã‚¹ã‚¯ãƒªãƒ—ãƒˆé…ç½®å®Œäº†"
+
+# --- [5/6] systemd ã‚µãƒ¼ãƒ“ã‚¹ ---
+echo ""
+echo "[5/6] systemd ã‚µãƒ¼ãƒ“ã‚¹è¨­å®š..."
+cat > /etc/systemd/system/openvino-r1.service << 'EOF'
+[Unit]
+Description=OpenVINO R1 Inference Server (ä¾å¤§å°† - CyberAgent)
+After=network.target
+
+[Service]
+Type=simple
+User=root
+WorkingDirectory=/opt/openvino
+ExecStart=/opt/openvino/venv/bin/python3 /opt/openvino/openvino_server.py
+Restart=on-failure
+RestartSec=10
+# i5-8500: Core 0-5
+CPUAffinity=0 1 2 3 4 5
+# 24GB RAMæ­è¼‰ã®ãŸã‚ã€20GBã¾ã§è¨±å¯ï¼ˆOSåˆ†4GBæ®‹ã—ï¼‰
+MemoryMax=20G
+Environment="OPENVINO_ENABLE_HUGE_PAGES=1"
+
+[Install]
+WantedBy=multi-user.target
+EOF
+
+systemctl daemon-reload
+systemctl enable openvino-r1
+systemctl restart openvino-r1
+
+echo "  ã‚µãƒ¼ãƒ“ã‚¹èµ·å‹•å®Œäº†"
+
+# --- [6/6] å‹•ä½œç¢ºèª ---
+echo ""
+echo "[6/6] å‹•ä½œç¢ºèª (30ç§’å¾…æ©Ÿ)..."
+# ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ã«æ™‚é–“ãŒã‹ã‹ã‚‹ãŸã‚é•·ã‚ã«å¾…æ©Ÿ
+sleep 30
+
+if curl -s http://localhost:11434/ | grep -q "ok"; then
+    echo "  âœ“ R1 ã‚µãƒ¼ãƒãƒ¼ç¨¼åƒä¸­"
+
+    echo "  æ¨è«–ãƒ†ã‚¹ãƒˆ (è‡ªå·±ç´¹ä»‹)..."
+    RESULT=$(curl -s -X POST http://localhost:11434/api/generate \
+        -H 'Content-Type: application/json' \
+        -d '{"prompt":"è‡ªå·±ç´¹ä»‹ã‚’ã—ã¦ãã ã•ã„ã€‚","max_tokens":50}')
+    echo "  âœ“ å¿œç­”ã‚µãƒ³ãƒ—ãƒ«: ${RESULT}"
+else
+    echo "  âœ— R1 ã‚µãƒ¼ãƒãƒ¼èµ·å‹•å¤±æ•— ã¾ãŸã¯ èµ·å‹•ä¸­"
+    echo "  ãƒ­ã‚°ç¢ºèª: journalctl -u openvino-r1 -n 50 -f"
+fi
+
+echo ""
+echo "============================================="
+echo "  ä¾å¤§å°† R1 Setup (JP) å®Œäº†!"
+echo "  Hardware: ProDesk 600 G4 (i5-8500)"
+echo ""
+echo "  ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ: http://<IP>:11434"
+echo "  ãƒ­ã‚°: journalctl -u openvino-r1 -f"
+echo "============================================="
diff --git a/setup/proxmox_setup.sh b/setup/proxmox_setup.sh
new file mode 100644
index 0000000..46a8cd6
--- /dev/null
+++ b/setup/proxmox_setup.sh
@@ -0,0 +1,110 @@
+#!/bin/bash
+# =============================================================
+# å°†è»ã‚·ã‚¹ãƒ†ãƒ  v7.0 - Proxmox LXC Setup
+# =============================================================
+# Proxmoxãƒ›ã‚¹ãƒˆä¸Šã§å®Ÿè¡Œã€‚CT 100 (æœ¬é™£) ã¨ CT 101 (ä¾å¤§å°†æ—¥æœ¬èªR1) ã‚’ä½œæˆã€‚
+#
+# Usage:
+#   bash proxmox_setup.sh [storage]
+#   bash proxmox_setup.sh local-lvm
+# =============================================================
+
+set -euo pipefail
+
+STORAGE="${1:-local-lvm}"
+TEMPLATE="local:vztmpl/ubuntu-22.04-standard_22.04-1_amd64.tar.zst"
+
+echo "============================================="
+echo "  å°†è»ã‚·ã‚¹ãƒ†ãƒ  - Proxmox Setup"
+echo "  Storage: ${STORAGE}"
+echo "============================================="
+
+# --- HugePages è¨­å®š (20GB) ---
+echo "[1/6] HugePages è¨­å®š..."
+if ! grep -q "vm.nr_hugepages" /etc/sysctl.conf; then
+    echo "vm.nr_hugepages = 10240" >> /etc/sysctl.conf
+    sysctl -p
+    echo "  HugePages: 10240 (20GB)"
+else
+    echo "  HugePages: æ—¢ã«è¨­å®šæ¸ˆã¿"
+fi
+
+# --- CT 100: æœ¬é™£ (Controller) ---
+echo ""
+echo "[2/6] CT 100: æœ¬é™£ ä½œæˆ..."
+if pct status 100 &>/dev/null; then
+    echo "  CT 100 æ—¢ã«å­˜åœ¨ã€‚ã‚¹ã‚­ãƒƒãƒ—ã€‚"
+else
+    pct create 100 "$TEMPLATE" \
+        --hostname honmaru-control \
+        --memory 2048 \
+        --cores 2 \
+        --rootfs "${STORAGE}:20" \
+        --net0 name=eth0,bridge=vmbr0,ip=192.168.1.10/24,gw=192.168.1.1 \
+        --onboot 1
+    echo "  CT 100 ä½œæˆå®Œäº†"
+fi
+
+# --- CT 101: ä¾å¤§å°†R1 (OpenVINO) ---
+echo ""
+echo "[3/6] CT 101: ä¾å¤§å°†R1 ä½œæˆ..."
+if pct status 101 &>/dev/null; then
+    echo "  CT 101 æ—¢ã«å­˜åœ¨ã€‚ã‚¹ã‚­ãƒƒãƒ—ã€‚"
+else
+    pct create 101 "$TEMPLATE" \
+        --hostname taisho-openvino \
+        --memory 20480 \
+        --swap 0 \
+        --cores 6 \
+        --rootfs "${STORAGE}:50" \
+        --net0 name=eth0,bridge=vmbr0,ip=192.168.1.11/24,gw=192.168.1.1 \
+        --onboot 1
+    echo "  CT 101 ä½œæˆå®Œäº†"
+fi
+
+# --- æœ€é©åŒ–è¨­å®š ---
+echo ""
+echo "[4/6] æœ€é©åŒ–è¨­å®š..."
+
+# CT 101: CPUæœ€é©åŒ–
+pct set 101 --cputype host 2>/dev/null || true
+echo "  CT 101: cputype=host"
+
+# CT 101: ãƒ¡ãƒ¢ãƒªå›ºå®š (balloonç„¡åŠ¹)
+if ! grep -q "balloon" /etc/pve/lxc/101.conf 2>/dev/null; then
+    echo "lxc.cgroup2.memory.max = 21474836480" >> /etc/pve/lxc/101.conf
+fi
+
+# HugePages ãƒã‚¦ãƒ³ãƒˆ
+if ! grep -q "hugepages" /etc/pve/lxc/101.conf 2>/dev/null; then
+    echo "lxc.mount.entry: /dev/hugepages dev/hugepages none bind,create=dir 0 0" \
+        >> /etc/pve/lxc/101.conf
+    echo "  CT 101: HugePages ãƒã‚¦ãƒ³ãƒˆè¿½åŠ "
+fi
+
+# --- èµ·å‹• ---
+echo ""
+echo "[5/6] ã‚³ãƒ³ãƒ†ãƒŠèµ·å‹•..."
+pct start 100 2>/dev/null || echo "  CT 100: æ—¢ã«ç¨¼åƒä¸­"
+pct start 101 2>/dev/null || echo "  CT 101: æ—¢ã«ç¨¼åƒä¸­"
+
+sleep 3
+
+# --- ç¢ºèª ---
+echo ""
+echo "[6/6] çŠ¶æ…‹ç¢ºèª..."
+pct list | grep -E "^(100|101)"
+
+echo ""
+echo "============================================="
+echo "  Proxmox Setup å®Œäº†!"
+echo ""
+echo "  æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:"
+echo "    1. CT 101 ã§ OpenVINO ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—:"
+echo "       pct enter 101"
+echo "       bash /path/to/shogun/setup/openvino_setup.sh"
+echo ""
+echo "    2. CT 100 ã§æœ¬é™£ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—:"
+echo "       pct enter 100"
+echo "       bash /path/to/shogun/setup/install.sh"
+echo "============================================="
diff --git a/setup/setup_groq.sh b/setup/setup_groq.sh
new file mode 100644
index 0000000..bb3ca64
--- /dev/null
+++ b/setup/setup_groq.sh
@@ -0,0 +1,326 @@
+#!/bin/bash
+# setup_groq.sh - Groq Setup for Shogun System v7.0
+#
+# Sets up Groq API access for 9th Ashigaru (è¶³è»½) recorder
+# Ultra-fast summarization with Llama 3.3 70B
+#
+# Usage: Run on CT 100 (æœ¬é™£)
+# Requirements: Groq API key, internet connection
+
+set -e
+
+echo "=========================================="
+echo "ğŸš€ 9ç•ªè¶³è»½ - Groqè¨˜éŒ²ä¿‚ ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"
+echo "=========================================="
+echo "Model: Llama 3.3 70B Versatile"
+echo "Purpose: Real-time recording & 60-day summaries"
+echo "Speed: 300-500 tok/s (10x faster than GPU)"
+echo "Cost: FREE (14,400 requests/day)"
+echo "=========================================="
+
+# Check if running on correct system
+if [ ! -f "/etc/hostname" ] || ! grep -q "honmaru-control\|æœ¬é™£" /etc/hostname 2>/dev/null; then
+    echo "âš ï¸ Warning: This script should be run on CT 100 (æœ¬é™£)"
+    echo "Current hostname: $(hostname)"
+    read -p "Continue anyway? [y/N]: " -n 1 -r
+    echo
+    if [[ ! $REPLY =~ ^[Yy]$ ]]; then
+        exit 1
+    fi
+fi
+
+echo "[1/5] Groq SDK installation"
+
+# Install Groq Python SDK
+pip3 install --break-system-packages groq requests python-dateutil
+
+echo "âœ… Groq SDK installed"
+
+# Check for existing API key
+echo "[2/5] API key configuration"
+
+if [ -f "/root/.env" ] && grep -q "GROQ_API_KEY" /root/.env; then
+    echo "âœ… GROQ_API_KEY found in .env file"
+    GROQ_API_KEY=$(grep "GROQ_API_KEY" /root/.env | cut -d'=' -f2)
+else
+    echo ""
+    echo "ğŸ”‘ Groq API key setup required"
+    echo "1. Visit: https://console.groq.com/keys"
+    echo "2. Create account (free)"
+    echo "3. Generate API key"
+    echo ""
+    
+    # Prompt for API key
+    while true; do
+        echo -n "Enter your Groq API key (gsk_...): "
+        read -r GROQ_API_KEY
+        
+        if [[ $GROQ_API_KEY =~ ^gsk_[a-zA-Z0-9]{50,}$ ]]; then
+            break
+        else
+            echo "âŒ Invalid format. Groq API keys start with 'gsk_'"
+        fi
+    done
+    
+    # Save to .env
+    echo "GROQ_API_KEY=$GROQ_API_KEY" >> /root/.env
+    echo "âœ… API key saved to /root/.env"
+fi
+
+# Test API connection
+echo "[3/5] API connection test"
+
+cat > /tmp/groq_test.py << 'PYTHON'
+#!/usr/bin/env python3
+import os
+import sys
+from groq import Groq
+
+try:
+    # Initialize client
+    client = Groq(api_key=os.environ.get('GROQ_API_KEY'))
+    
+    # Test simple request
+    print("ğŸ§ª Testing Groq API connection...")
+    
+    response = client.chat.completions.create(
+        model="llama-3.3-70b-versatile",
+        messages=[
+            {"role": "system", "content": "You are a helpful assistant. Respond briefly."},
+            {"role": "user", "content": "Hello, can you confirm you're working?"}
+        ],
+        max_tokens=50,
+        temperature=0.1,
+    )
+    
+    if response.choices and response.choices[0].message.content:
+        print("âœ… API connection successful")
+        print(f"Response: {response.choices[0].message.content}")
+        print(f"Usage: {response.usage.total_tokens} tokens")
+        sys.exit(0)
+    else:
+        print("âŒ Empty response from API")
+        sys.exit(1)
+        
+except Exception as e:
+    print(f"âŒ API test failed: {e}")
+    sys.exit(1)
+PYTHON
+
+# Run test with environment
+export GROQ_API_KEY="$GROQ_API_KEY"
+python3 /tmp/groq_test.py
+
+if [ $? -eq 0 ]; then
+    echo "âœ… Groq API connection verified"
+else
+    echo "âŒ Groq API test failed"
+    exit 1
+fi
+
+# Create usage tracking
+echo "[4/5] Usage tracking setup"
+
+mkdir -p /var/log/shogun/groq
+cat > /var/log/shogun/groq/usage.json << 'JSON'
+{
+    "daily_requests": 0,
+    "last_reset_date": "",
+    "total_tokens": 0,
+    "sessions_recorded": 0,
+    "summaries_generated": 0
+}
+JSON
+
+# Create daily reset cron job
+cat > /etc/cron.d/groq-quota-reset << 'CRON'
+# Reset Groq daily quota at midnight
+0 0 * * * root /bin/bash -c 'echo "{\"daily_requests\": 0, \"last_reset_date\": \"$(date -I)\", \"total_tokens\": 0, \"sessions_recorded\": 0, \"summaries_generated\": 0}" > /var/log/shogun/groq/usage.json'
+CRON
+
+echo "âœ… Usage tracking configured"
+
+# Create monitoring script
+echo "[5/5] Monitoring tools setup"
+
+cat > /usr/local/bin/groq-status << 'BASH'
+#!/bin/bash
+# Groq 9ç•ªè¶³è»½ status checker
+
+echo "========================================"
+echo "ğŸš€ 9ç•ªè¶³è»½ (Groqè¨˜éŒ²ä¿‚) ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹"
+echo "========================================"
+
+# Check API key
+if [ -n "$GROQ_API_KEY" ] || grep -q "GROQ_API_KEY" /root/.env; then
+    echo "âœ… API Key: Configured"
+else
+    echo "âŒ API Key: Not configured"
+fi
+
+# Check usage stats
+if [ -f "/var/log/shogun/groq/usage.json" ]; then
+    python3 -c "
+import json
+try:
+    with open('/var/log/shogun/groq/usage.json', 'r') as f:
+        data = json.load(f)
+    daily = data.get('daily_requests', 0)
+    total_tokens = data.get('total_tokens', 0)
+    remaining = 14400 - daily
+    
+    print(f'ğŸ“Š Usage Today: {daily}/14,400 requests')
+    print(f'ğŸ“Š Remaining: {remaining} requests')
+    print(f'ğŸ“Š Total Tokens: {total_tokens:,}')
+    print(f'ğŸ“Š Sessions: {data.get(\"sessions_recorded\", 0)}')
+    print(f'ğŸ“Š Summaries: {data.get(\"summaries_generated\", 0)}')
+    
+    if remaining < 1000:
+        print('âš ï¸ Warning: Less than 1000 requests remaining today')
+    elif remaining < 100:
+        print('ğŸš« Critical: Less than 100 requests remaining today')
+    
+except Exception as e:
+    print(f'âŒ Error reading usage stats: {e}')
+"
+else
+    echo "âŒ Usage tracking not initialized"
+fi
+
+# Test API
+echo ""
+echo "ğŸ§ª Quick API Test:"
+source /root/.env 2>/dev/null || true
+export GROQ_API_KEY
+python3 -c "
+import os
+from groq import Groq
+try:
+    client = Groq(api_key=os.environ.get('GROQ_API_KEY'))
+    response = client.chat.completions.create(
+        model='llama-3.3-70b-versatile',
+        messages=[{'role': 'user', 'content': 'Test'}],
+        max_tokens=5
+    )
+    print('âœ… API responding normally')
+except Exception as e:
+    print(f'âŒ API error: {e}')
+"
+
+echo "========================================"
+BASH
+
+chmod +x /usr/local/bin/groq-status
+
+# Create performance test script
+cat > /usr/local/bin/groq-benchmark << 'BASH'
+#!/bin/bash
+# Groq performance benchmark
+
+echo "ğŸƒâ€â™‚ï¸ Groq Performance Benchmark"
+echo "Testing Llama 3.3 70B inference speed..."
+
+source /root/.env 2>/dev/null || true
+export GROQ_API_KEY
+
+python3 -c "
+import os
+import time
+from groq import Groq
+
+client = Groq(api_key=os.environ.get('GROQ_API_KEY'))
+
+# Test prompt
+prompt = '''
+ä»¥ä¸‹ã®ã‚¿ã‚¹ã‚¯ã‚’åˆ†æã—ã€60æ—¥é–“ã®è¦ç´„ãƒ¬ãƒãƒ¼ãƒˆã‚’ä½œæˆã—ã¦ãã ã•ã„ï¼š
+
+1. ESP32-P4ã®I2Sè¨­å®šæœ€é©åŒ–
+2. Home Assistantã®éŸ³å£°èªè­˜çµ±åˆ
+3. Spotify APIé€£æºå®Ÿè£…
+4. æ¶ˆè²»é›»åŠ›æ¸¬å®šã‚·ã‚¹ãƒ†ãƒ æ§‹ç¯‰
+5. è‡ªå‹•ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ç”Ÿæˆ
+
+å„é …ç›®ã«ã¤ã„ã¦ã€å®Ÿè£…å†…å®¹ã€ç™ºç”Ÿã—ãŸå•é¡Œã€è§£æ±ºç­–ã€ä»Šå¾Œã®æ”¹å–„ç‚¹ã‚’ã¾ã¨ã‚ã¦ãã ã•ã„ã€‚
+'''
+
+print(f'Prompt length: {len(prompt)} characters')
+print('Generating response...')
+
+start_time = time.time()
+
+response = client.chat.completions.create(
+    model='llama-3.3-70b-versatile',
+    messages=[
+        {'role': 'system', 'content': 'åŠ¹ç‡çš„ã§åŒ…æ‹¬çš„ãªè¦ç´„ãƒ¬ãƒãƒ¼ãƒˆã‚’æ—¥æœ¬èªã§ä½œæˆã—ã¦ãã ã•ã„ã€‚'},
+        {'role': 'user', 'content': prompt}
+    ],
+    max_tokens=1000,
+    temperature=0.3,
+)
+
+end_time = time.time()
+duration = end_time - start_time
+
+if response.usage:
+    total_tokens = response.usage.total_tokens
+    completion_tokens = response.usage.completion_tokens
+    
+    print(f'â±ï¸ Response time: {duration:.2f} seconds')
+    print(f'ğŸ“Š Total tokens: {total_tokens}')
+    print(f'ğŸ“Š Completion tokens: {completion_tokens}')
+    print(f'ğŸš€ Speed: {total_tokens/duration:.1f} tokens/second')
+    print(f'ğŸš€ Output speed: {completion_tokens/duration:.1f} tokens/second')
+    
+    if total_tokens/duration > 200:
+        print('âœ… Performance: Excellent (>200 tok/s)')
+    elif total_tokens/duration > 100:
+        print('âœ… Performance: Good (>100 tok/s)')
+    else:
+        print('âš ï¸ Performance: Below expected (<100 tok/s)')
+        
+    # Show first 200 chars of response
+    print('')
+    print('ğŸ“ Sample output:')
+    print(response.choices[0].message.content[:200] + '...')
+    
+else:
+    print('âŒ No usage information returned')
+"
+BASH
+
+chmod +x /usr/local/bin/groq-benchmark
+
+# Final verification
+echo ""
+echo "ğŸ” Final verification..."
+
+# Run status check
+/usr/local/bin/groq-status
+
+echo ""
+echo "========================================"
+echo "ğŸ‰ 9ç•ªè¶³è»½ (Groqè¨˜éŒ²ä¿‚) ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—å®Œäº†!"
+echo "========================================"
+echo "Model: Llama 3.3 70B Versatile"
+echo "Daily Quota: 14,400 requests (FREE)"
+echo "Speed: 300-500 tokens/second"
+echo ""
+echo "ç®¡ç†ã‚³ãƒãƒ³ãƒ‰:"
+echo "  groq-status      - ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ç¢ºèª"
+echo "  groq-benchmark   - æ€§èƒ½ãƒ†ã‚¹ãƒˆ"
+echo ""
+echo "è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«:"
+echo "  /root/.env       - API key"
+echo "  /var/log/shogun/groq/usage.json - ä½¿ç”¨é‡è¿½è·¡"
+echo ""
+echo "çµ±åˆæ–¹æ³•:"
+echo "  - GroqRecorder ã‚¯ãƒ©ã‚¹ãŒè‡ªå‹•çš„ã«ä½¿ç”¨"
+echo "  - ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ è¨˜éŒ²"
+echo "  - 60æ—¥è¦ç´„è‡ªå‹•ç”Ÿæˆ"
+echo "  - Notionè‡ªå‹•è»¢é€"
+echo ""
+echo "ğŸš€ è¶…é«˜é€Ÿè¨˜éŒ²ãƒ»è¦ç´„ã‚·ã‚¹ãƒ†ãƒ ç¨¼åƒæº–å‚™å®Œäº†!"
+echo "========================================"
+
+# Clean up
+rm -f /tmp/groq_test.py
\ No newline at end of file
diff --git a/setup/setup_r1_japanese.sh b/setup/setup_r1_japanese.sh
new file mode 100644
index 0000000..535e945
--- /dev/null
+++ b/setup/setup_r1_japanese.sh
@@ -0,0 +1,656 @@
+#!/bin/bash
+# setup_r1_japanese.sh - Enhanced Japanese R1 Setup for Shogun System v7.0
+#
+# Sets up cyberagent/DeepSeek-R1-Distill-Qwen-14B-Japanese with OpenVINO
+# For Taisho (ä¾å¤§å°†) on CT 101 (192.168.1.11)
+#
+# Usage: Run inside CT 101 (ä¾å¤§å°†å°‚ç”¨LXC)
+# Requires: 20GB RAM, 6 CPU cores, HugePages configured
+#
+# Enhanced with comprehensive error handling and recovery mechanisms
+
+set -euo pipefail  # Enhanced error handling
+
+# Global variables for error handling
+LOG_FILE="/var/log/taisho-setup.log"
+BACKUP_DIR="/opt/backup-$(date +%Y%m%d-%H%M%S)"
+RECOVERY_INFO_FILE="/opt/recovery_info.json"
+
+# Enhanced logging function
+log() {
+    local level=$1
+    shift
+    local message="$*"
+    local timestamp=$(date '+%Y-%m-%d %H:%M:%S')
+    
+    # Color codes
+    local RED='\033[0;31m'
+    local GREEN='\033[0;32m'
+    local YELLOW='\033[1;33m'
+    local BLUE='\033[0;34m'
+    local NC='\033[0m' # No Color
+    
+    case $level in
+        "ERROR")
+            echo -e "${RED}[ERROR]${NC} $message" | tee -a "$LOG_FILE"
+            ;;
+        "WARN")
+            echo -e "${YELLOW}[WARN]${NC} $message" | tee -a "$LOG_FILE"
+            ;;
+        "INFO")
+            echo -e "${GREEN}[INFO]${NC} $message" | tee -a "$LOG_FILE"
+            ;;
+        "DEBUG")
+            echo -e "${BLUE}[DEBUG]${NC} $message" | tee -a "$LOG_FILE"
+            ;;
+        *)
+            echo "[$timestamp] $message" | tee -a "$LOG_FILE"
+            ;;
+    esac
+}
+
+# Error handler function
+error_handler() {
+    local exit_code=$?
+    local line_number=$1
+    
+    log "ERROR" "Setup failed at line $line_number with exit code $exit_code"
+    log "ERROR" "Last command: $BASH_COMMAND"
+    
+    # Save recovery information
+    cat > "$RECOVERY_INFO_FILE" << EOF
+{
+    "timestamp": "$(date -Iseconds)",
+    "exit_code": $exit_code,
+    "failed_line": $line_number,
+    "failed_command": "$BASH_COMMAND",
+    "backup_dir": "$BACKUP_DIR",
+    "log_file": "$LOG_FILE"
+}
+EOF
+    
+    log "INFO" "Recovery information saved to $RECOVERY_INFO_FILE"
+    log "INFO" "Log file available at $LOG_FILE"
+    
+    # Attempt automatic recovery for common issues
+    attempt_recovery $exit_code
+    
+    exit $exit_code
+}
+
+# Set error trap
+trap 'error_handler ${LINENO}' ERR
+
+# Recovery function for common issues
+attempt_recovery() {
+    local exit_code=$1
+    
+    log "INFO" "Attempting automatic recovery for exit code $exit_code..."
+    
+    case $exit_code in
+        130)
+            log "INFO" "Process interrupted by user - cleanup not needed"
+            ;;
+        1)
+            log "INFO" "General error - checking for partial installations"
+            cleanup_partial_installation
+            ;;
+        2)
+            log "INFO" "Command not found - checking package installation"
+            check_missing_packages
+            ;;
+        *)
+            log "WARN" "No specific recovery procedure for exit code $exit_code"
+            ;;
+    esac
+}
+
+# Cleanup partial installations
+cleanup_partial_installation() {
+    log "INFO" "Cleaning up partial installation..."
+    
+    # Stop any running services
+    if systemctl is-active --quiet taisho-japanese-r1 2>/dev/null; then
+        systemctl stop taisho-japanese-r1 || true
+    fi
+    
+    # Remove incomplete model downloads
+    if [ -d "/opt/models/deepseek-r1-japanese" ] && [ ! -f "/opt/models/deepseek-r1-japanese/.download_complete" ]; then
+        log "WARN" "Removing incomplete model download"
+        rm -rf /opt/models/deepseek-r1-japanese
+    fi
+    
+    log "INFO" "Partial installation cleanup complete"
+}
+
+# Check for missing packages
+check_missing_packages() {
+    log "INFO" "Checking for missing packages..."
+    
+    local required_packages=("python3" "python3-pip" "python3-venv" "git" "curl" "wget")
+    local missing_packages=()
+    
+    for package in "${required_packages[@]}"; do
+        if ! dpkg -l | grep -q "^ii  $package "; then
+            missing_packages+=("$package")
+        fi
+    done
+    
+    if [ ${#missing_packages[@]} -gt 0 ]; then
+        log "WARN" "Missing packages detected: ${missing_packages[*]}"
+        log "INFO" "Attempting to install missing packages..."
+        apt update && apt install -y "${missing_packages[@]}" || {
+            log "ERROR" "Failed to install missing packages"
+            return 1
+        }
+    fi
+}
+
+# Create backup of existing installation
+create_backup() {
+    if [ -d "/opt/openvino-env" ] || [ -d "/opt/models" ] || [ -f "/opt/taisho_server.py" ]; then
+        log "INFO" "Creating backup of existing installation..."
+        mkdir -p "$BACKUP_DIR"
+        
+        [ -d "/opt/openvino-env" ] && cp -r /opt/openvino-env "$BACKUP_DIR/" || true
+        [ -d "/opt/models" ] && cp -r /opt/models "$BACKUP_DIR/" || true  
+        [ -f "/opt/taisho_server.py" ] && cp /opt/taisho_server.py "$BACKUP_DIR/" || true
+        
+        log "INFO" "Backup created at $BACKUP_DIR"
+    fi
+}
+
+# Enhanced system check with detailed reporting
+check_system_requirements() {
+    log "INFO" "Performing enhanced system requirements check..."
+    
+    local mem_gb=$(free -g | awk '/^Mem:/{print $2}')
+    local cpu_cores=$(nproc)
+    local disk_space_gb=$(df /opt --output=avail -BG | tail -n1 | tr -d 'G')
+    
+    log "INFO" "System specifications:"
+    log "INFO" "  RAM: ${mem_gb}GB (required: 20GB)"
+    log "INFO" "  CPU cores: ${cpu_cores} (required: 4)"
+    log "INFO" "  Disk space: ${disk_space_gb}GB (required: 50GB)"
+    log "INFO" "  Architecture: $(uname -m)"
+    
+    local requirements_met=true
+    
+    if [ "$mem_gb" -lt 20 ]; then
+        log "ERROR" "Insufficient RAM: ${mem_gb}GB < 20GB required"
+        requirements_met=false
+    fi
+    
+    if [ "$cpu_cores" -lt 4 ]; then
+        log "ERROR" "Insufficient CPU cores: ${cpu_cores} < 4 required"
+        requirements_met=false
+    fi
+    
+    if [ "$disk_space_gb" -lt 50 ]; then
+        log "ERROR" "Insufficient disk space: ${disk_space_gb}GB < 50GB required"
+        requirements_met=false
+    fi
+    
+    if [ "$(uname -m)" != "x86_64" ]; then
+        log "WARN" "Architecture $(uname -m) may not be fully supported"
+    fi
+    
+    if [ "$requirements_met" = false ]; then
+        log "ERROR" "System requirements not met - aborting installation"
+        return 1
+    fi
+    
+    log "INFO" "âœ… All system requirements satisfied"
+    return 0
+}
+
+# Main execution starts here
+main() {
+    # Initialize logging
+    mkdir -p "$(dirname "$LOG_FILE")"
+    log "INFO" "========================================="
+    log "INFO" "ğŸ¯ ä¾å¤§å°† (Taisho) - æ—¥æœ¬èªR1 ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"
+    log "INFO" "========================================="
+    log "INFO" "Model: cyberagent/DeepSeek-R1-Distill-Qwen-14B-Japanese"
+    log "INFO" "Target: CT 101 (192.168.1.11)"
+    log "INFO" "OpenVINO: INT8 quantization for optimal performance"
+    log "INFO" "Enhanced with comprehensive error handling"
+    log "INFO" "========================================="
+
+    # Create backup if needed
+    create_backup
+
+    # Check if running in correct container
+    if [ "$(hostname)" != "taisho-r1-japanese" ]; then
+        log "WARN" "Expected hostname 'taisho-r1-japanese', got '$(hostname)'"
+        read -p "Continue anyway? [y/N]: " -n 1 -r
+        echo
+        if [[ ! $REPLY =~ ^[Yy]$ ]]; then
+            log "INFO" "Installation cancelled by user"
+            exit 0
+        fi
+    fi
+
+    # Enhanced system requirements check
+    log "INFO" "[1/9] Enhanced system requirements check"
+    check_system_requirements
+
+    # Update system
+    log "INFO" "[2/9] System update"
+    log "DEBUG" "Updating package lists..."
+    apt update || {
+        log "ERROR" "Failed to update package lists"
+        return 1
+    }
+    
+    log "DEBUG" "Installing required packages..."
+    apt install -y python3 python3-pip python3-venv git curl wget htop build-essential || {
+        log "ERROR" "Failed to install required packages"
+        return 1
+    }
+    log "INFO" "âœ… System packages installed"
+
+    # Create virtual environment
+    log "INFO" "[3/9] Python environment setup"
+    cd /opt
+    
+    if [ -d "openvino-env" ]; then
+        log "WARN" "Existing virtual environment found - removing"
+        rm -rf openvino-env
+    fi
+    
+    log "DEBUG" "Creating Python virtual environment..."
+    python3 -m venv openvino-env || {
+        log "ERROR" "Failed to create virtual environment"
+        return 1
+    }
+    
+    log "DEBUG" "Activating virtual environment..."
+    source openvino-env/bin/activate || {
+        log "ERROR" "Failed to activate virtual environment"
+        return 1
+    }
+    log "INFO" "âœ… Python virtual environment ready"
+
+    # Install OpenVINO and dependencies
+    log "INFO" "[4/9] OpenVINO installation"
+    log "DEBUG" "Upgrading pip..."
+    pip install --upgrade pip || {
+        log "ERROR" "Failed to upgrade pip"
+        return 1
+    }
+    
+    log "DEBUG" "Installing OpenVINO and ML dependencies..."
+    pip install openvino==2024.0.0 openvino-dev optimum[openvino] transformers torch || {
+        log "ERROR" "Failed to install OpenVINO dependencies"
+        return 1
+    }
+    
+    log "DEBUG" "Installing web server and utility dependencies..."
+    pip install flask requests psutil huggingface_hub || {
+        log "ERROR" "Failed to install server dependencies"
+        return 1
+    }
+
+    log "INFO" "âœ… OpenVINO installation complete"
+
+    # Create model directory
+    log "INFO" "[5/9] Model download and setup"
+    log "DEBUG" "Creating model directory..."
+    mkdir -p /opt/models || {
+        log "ERROR" "Failed to create model directory"
+        return 1
+    }
+    cd /opt/models
+
+    # Download Japanese R1 model with progress tracking
+    log "INFO" "ğŸ—ï¸ Downloading cyberagent/DeepSeek-R1-Distill-Qwen-14B-Japanese"
+    log "INFO" "Model size: ~2.8GB - This may take 10-30 minutes depending on network speed..."
+    
+    # Check if model already exists and is complete
+    if [ -f "deepseek-r1-japanese/.download_complete" ]; then
+        log "INFO" "Model already downloaded - skipping"
+    else
+        # Remove any partial download
+        [ -d "deepseek-r1-japanese" ] && rm -rf deepseek-r1-japanese
+        
+        log "DEBUG" "Starting model download..."
+        huggingface-cli download \
+          cyberagent/DeepSeek-R1-Distill-Qwen-14B-Japanese \
+          --local-dir deepseek-r1-japanese \
+          --local-dir-use-symlinks False || {
+            log "ERROR" "Model download failed"
+            return 1
+        }
+        
+        # Mark download as complete
+        touch deepseek-r1-japanese/.download_complete
+        log "INFO" "âœ… Model download complete"
+    fi
+
+# Convert to OpenVINO format with INT8 quantization
+echo "[6/8] OpenVINO INT8 quantization"
+echo "ğŸ”§ Converting to optimized INT8 format (30-60 minutes)..."
+echo "This process uses significant CPU and memory resources."
+
+optimum-cli export openvino \
+  --model deepseek-r1-japanese \
+  --task text-generation \
+  --weight-format int8 \
+  --group-size 128 \
+  --ratio 0.8 \
+  --output deepseek-r1-japanese-int8
+
+echo "âœ… INT8 quantization complete"
+
+# Create inference server
+echo "[7/8] Inference server setup"
+cat > /opt/taisho_server.py << 'PYTHON'
+#!/usr/bin/env python3
+"""Taisho (ä¾å¤§å°†) Inference Server - Japanese R1
+
+High-performance inference server for DeepSeek R1 Japanese model.
+Optimized for deep thinking and Japanese language processing.
+"""
+
+import os
+import time
+import logging
+from flask import Flask, request, jsonify
+from optimum.intel import OVModelForCausalLM
+from transformers import AutoTokenizer
+import psutil
+import threading
+
+# Configure logging
+logging.basicConfig(
+    level=logging.INFO,
+    format='[%(asctime)s] %(levelname)s - %(message)s',
+    datefmt='%H:%M:%S'
+)
+logger = logging.getLogger(__name__)
+
+app = Flask(__name__)
+
+# Global model variables
+model = None
+tokenizer = None
+model_stats = {
+    'requests_processed': 0,
+    'total_tokens_generated': 0,
+    'average_response_time': 0,
+    'start_time': time.time(),
+    'model_loaded': False
+}
+
+def load_model():
+    """Load the Japanese R1 model with OpenVINO optimization."""
+    global model, tokenizer
+    
+    logger.info("ğŸ¯ ä¾å¤§å°†ã‚·ã‚¹ãƒ†ãƒ èµ·å‹•ä¸­...")
+    logger.info("Loading DeepSeek-R1-Distill-Qwen-14B-Japanese (INT8)")
+    
+    model_path = "/opt/models/deepseek-r1-japanese-int8"
+    tokenizer_path = "/opt/models/deepseek-r1-japanese"
+    
+    try:
+        # Load tokenizer
+        logger.info("Loading tokenizer...")
+        tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)
+        
+        # Load optimized model
+        logger.info("Loading INT8 optimized model...")
+        model = OVModelForCausalLM.from_pretrained(
+            model_path,
+            device="CPU",
+            ov_config={
+                "PERFORMANCE_HINT": "LATENCY",
+                "NUM_STREAMS": "1", 
+                "INFERENCE_PRECISION_HINT": "u8",
+                "KV_CACHE_PRECISION": "u8"
+            }
+        )
+        
+        model_stats['model_loaded'] = True
+        logger.info("âœ… Japanese R1 model loaded successfully")
+        logger.info("ğŸ¯ Ready for deep thinking and Japanese inference")
+        
+    except Exception as e:
+        logger.error(f"âŒ Model loading failed: {e}")
+        raise
+
+@app.route('/api/generate', methods=['POST'])
+def generate():
+    """Generate response using Japanese R1."""
+    global model, tokenizer, model_stats
+    
+    if not model or not tokenizer:
+        return jsonify({'error': 'ä¾å¤§å°†ãƒ¢ãƒ‡ãƒ«æœªåˆæœŸåŒ–'}), 500
+    
+    data = request.json
+    prompt = data.get('prompt', '')
+    system = data.get('system', '')
+    max_tokens = data.get('max_tokens', 1000)
+    temperature = data.get('temperature', 0.6)
+    
+    if not prompt:
+        return jsonify({'error': 'ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãŒå¿…è¦ã§ã™'}), 400
+    
+    start_time = time.time()
+    
+    try:
+        # Format prompt for Japanese R1
+        if system:
+            full_prompt = f"<|system|>\n{system}\n<|user|>\n{prompt}\n<|assistant|>\n"
+        else:
+            full_prompt = f"<|user|>\n{prompt}\n<|assistant|>\n"
+        
+        logger.info(f"ğŸ¤” [æ€è€ƒé–‹å§‹] {len(prompt)}æ–‡å­—ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå‡¦ç†")
+        
+        # Tokenize
+        inputs = tokenizer(full_prompt, return_tensors="pt")
+        input_length = inputs.input_ids.shape[1]
+        
+        # Generate with <think> support
+        import torch
+        with torch.no_grad():
+            outputs = model.generate(
+                **inputs,
+                max_new_tokens=max_tokens,
+                temperature=temperature,
+                do_sample=True,
+                pad_token_id=tokenizer.eos_token_id,
+                repetition_penalty=1.1,
+                no_repeat_ngram_size=3
+            )
+        
+        # Decode response
+        response = tokenizer.decode(outputs[0][input_length:], skip_special_tokens=True)
+        
+        # Calculate statistics
+        end_time = time.time()
+        response_time = end_time - start_time
+        tokens_generated = len(outputs[0]) - input_length
+        
+        # Update stats
+        model_stats['requests_processed'] += 1
+        model_stats['total_tokens_generated'] += tokens_generated
+        model_stats['average_response_time'] = (
+            (model_stats['average_response_time'] * (model_stats['requests_processed'] - 1) + response_time) /
+            model_stats['requests_processed']
+        )
+        
+        logger.info(
+            f"ğŸ’¡ [æ€è€ƒå®Œäº†] {response_time:.1f}ç§’, {tokens_generated}ãƒˆãƒ¼ã‚¯ãƒ³, "
+            f"{tokens_generated/response_time:.1f} tok/s"
+        )
+        
+        return jsonify({
+            'response': response,
+            'stats': {
+                'response_time_seconds': response_time,
+                'tokens_generated': tokens_generated,
+                'tokens_per_second': tokens_generated / response_time if response_time > 0 else 0,
+                'thinking_indicators': response.count('<think>'),
+                'japanese_detected': any(ord(char) > 0x3040 for char in response)
+            }
+        })
+        
+    except Exception as e:
+        logger.error(f"âŒ Generation error: {e}")
+        return jsonify({'error': f'æ¨è«–ã‚¨ãƒ©ãƒ¼: {str(e)}'}), 500
+
+@app.route('/api/health', methods=['GET'])
+def health():
+    """Health check endpoint."""
+    system_info = {
+        'cpu_percent': psutil.cpu_percent(interval=1),
+        'memory_percent': psutil.virtual_memory().percent,
+        'uptime_hours': (time.time() - model_stats['start_time']) / 3600
+    }
+    
+    return jsonify({
+        'status': 'healthy' if model_stats['model_loaded'] else 'loading',
+        'model': 'DeepSeek-R1-Japanese',
+        'version': 'v7.0',
+        'agent': 'ä¾å¤§å°†',
+        'system': system_info,
+        'stats': model_stats
+    })
+
+@app.route('/api/stats', methods=['GET'])
+def stats():
+    """Detailed statistics."""
+    return jsonify({
+        'model_stats': model_stats,
+        'system_stats': {
+            'cpu_count': psutil.cpu_count(),
+            'memory_total_gb': psutil.virtual_memory().total // (1024**3),
+            'memory_available_gb': psutil.virtual_memory().available // (1024**3),
+            'load_average': os.getloadavg()
+        },
+        'performance': {
+            'requests_per_hour': model_stats['requests_processed'] / ((time.time() - model_stats['start_time']) / 3600) if model_stats['requests_processed'] > 0 else 0,
+            'average_tokens_per_request': model_stats['total_tokens_generated'] / model_stats['requests_processed'] if model_stats['requests_processed'] > 0 else 0
+        }
+    })
+
+if __name__ == '__main__':
+    # Load model in separate thread to avoid blocking
+    model_thread = threading.Thread(target=load_model)
+    model_thread.start()
+    
+    logger.info("ğŸŒ¸ ä¾å¤§å°†ã‚µãƒ¼ãƒãƒ¼èµ·å‹• (ãƒãƒ¼ãƒˆ: 11434)")
+    logger.info("Japanese R1 inference server starting...")
+    
+    # Start Flask server
+    app.run(
+        host='0.0.0.0',
+        port=11434,
+        debug=False,
+        threaded=True
+    )
+PYTHON
+
+chmod +x /opt/taisho_server.py
+
+# Create systemd service
+echo "[8/8] System service setup"
+cat > /etc/systemd/system/taisho-japanese-r1.service << 'EOF'
+[Unit]
+Description=Taisho (ä¾å¤§å°†) Japanese R1 Inference Server
+After=network.target
+
+[Service]
+Type=simple
+User=root
+WorkingDirectory=/opt
+Environment=PYTHONPATH=/opt/openvino-env/lib/python3.10/site-packages
+ExecStartPre=/bin/bash -c 'source /opt/openvino-env/bin/activate'
+ExecStart=/opt/openvino-env/bin/python /opt/taisho_server.py
+Restart=on-failure
+RestartSec=10
+TimeoutStartSec=300
+
+# Optimization settings
+Environment=OMP_NUM_THREADS=6
+Environment=KMP_AFFINITY=granularity=fine,compact,1,0
+Environment=OPENVINO_LOG_LEVEL=WARNING
+
+# Memory settings
+LimitNOFILE=65536
+LimitMEMLOCK=infinity
+
+[Install]
+WantedBy=multi-user.target
+EOF
+
+# Enable and start service
+systemctl daemon-reload
+systemctl enable taisho-japanese-r1
+systemctl start taisho-japanese-r1
+
+# Wait for service to start
+echo "â³ Waiting for service to start (may take 2-3 minutes for model loading)..."
+sleep 10
+
+# Test the service
+echo "ğŸ§ª Testing Japanese R1 inference..."
+for i in {1..30}; do
+    if curl -s http://localhost:11434/api/health > /dev/null; then
+        echo "âœ… Service is responding"
+        break
+    fi
+    echo "Waiting... ($i/30)"
+    sleep 5
+done
+
+# Final test with Japanese input
+echo "ğŸ—¾ Testing Japanese processing..."
+response=$(curl -s -X POST http://localhost:11434/api/generate \
+  -H "Content-Type: application/json" \
+  -d '{
+    "prompt": "ã“ã‚“ã«ã¡ã¯ã€ç§ã¯ä¾å¤§å°†ã§ã™ã€‚",
+    "system": "æ—¥æœ¬èªã§ç°¡æ½”ã«å›ç­”ã—ã¦ãã ã•ã„ã€‚",
+    "max_tokens": 50
+  }' | python3 -c "import sys, json; data=json.load(sys.stdin); print(data.get('response', 'Error: ' + str(data)))" 2>/dev/null || echo "Error during test")
+
+echo "Response: $response"
+
+echo ""
+echo "========================================"
+echo "ğŸ‰ ä¾å¤§å°† (Japanese R1) ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—å®Œäº†!"
+echo "========================================"
+echo "Model: cyberagent/DeepSeek-R1-Distill-Qwen-14B-Japanese"
+echo "Quantization: INT8 (optimized for performance)"
+echo "Server: http://192.168.1.11:11434"
+echo "Service: taisho-japanese-r1.service"
+echo ""
+echo "Management commands:"
+echo "  systemctl status taisho-japanese-r1"
+echo "  systemctl restart taisho-japanese-r1"
+echo "  journalctl -u taisho-japanese-r1 -f"
+echo ""
+echo "Health check:"
+echo "  curl http://192.168.1.11:11434/api/health"
+echo ""
+echo "Test generation:"
+echo '  curl -X POST http://192.168.1.11:11434/api/generate \'
+echo '    -H "Content-Type: application/json" \'
+echo '    -d "{\"prompt\": \"ESP32ã®I2Sè¨­å®šã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„\", \"max_tokens\": 200}"'
+echo ""
+    log "INFO" "ğŸ¯ ä¾å¤§å°†ã¯æ·±ã„æ€è€ƒ(<think>)ã§çš†æ§˜ã‚’ãŠæ”¯ãˆã—ã¾ã™!"
+    log "INFO" "========================================"
+    
+    # Final success message
+    log "INFO" "âœ… Setup completed successfully!"
+    log "INFO" "All logs saved to: $LOG_FILE"
+    
+    return 0
+}
+
+# Execute main function
+if [[ "${BASH_SOURCE[0]}" == "${0}" ]]; then
+    main "$@"
+    exit $?
+fi
\ No newline at end of file
diff --git a/templates/context_template.md b/templates/context_template.md
new file mode 100644
index 0000000..7b19646
--- /dev/null
+++ b/templates/context_template.md
@@ -0,0 +1,23 @@
+# {project_id}
+æœ€çµ‚æ›´æ–°: YYYY-MM-DD
+
+## Whatï¼ˆã“ã‚Œã¯ä½•ã‹ï¼‰
+<!-- ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®æ¦‚è¦ -->
+
+## Whyï¼ˆãªãœã‚„ã‚‹ã®ã‹ï¼‰
+<!-- ç›®çš„ãƒ»å‹•æ©Ÿ -->
+
+## Whoï¼ˆèª°ãŒé–¢ä¿‚ã™ã‚‹ã‹ï¼‰
+<!-- ã‚¹ãƒ†ãƒ¼ã‚¯ãƒ›ãƒ«ãƒ€ãƒ¼ -->
+
+## Constraintsï¼ˆåˆ¶ç´„ã¯ä½•ã‹ï¼‰
+<!-- æŠ€è¡“çš„åˆ¶ç´„ã€äºˆç®—ã€æœŸé™ -->
+
+## Current Stateï¼ˆä»Šã©ã“ã«ã„ã‚‹ã‹ï¼‰
+<!-- ç¾åœ¨ã®é€²æ—çŠ¶æ³ -->
+
+## Decisionsï¼ˆæ±ºã¾ã£ãŸã“ã¨ï¼‰
+<!-- ç¢ºå®šã—ãŸè¨­è¨ˆåˆ¤æ–­ -->
+
+## Notesï¼ˆãƒ¡ãƒ¢ãƒ»æ°—ã¥ãï¼‰
+<!-- ãã®ä»–ãƒ¡ãƒ¢ -->
diff --git a/tests/test_error_handling.py b/tests/test_error_handling.py
new file mode 100644
index 0000000..5cb2223
--- /dev/null
+++ b/tests/test_error_handling.py
@@ -0,0 +1,340 @@
+"""Test Suite for Enhanced Error Handling System
+
+Comprehensive tests for the ShogunAI v7.0 error handling system,
+circuit breakers, retry policies, and fallback strategies.
+"""
+
+import pytest
+import asyncio
+import time
+from unittest.mock import Mock, patch, AsyncMock
+from typing import Dict, Any
+
+from shogun.core.error_handling import (
+    ShogunErrorHandler,
+    FallbackStrategy,
+    ErrorSeverity,
+    ShogunError,
+    OperationFailedError,
+    CircuitBreakerOpenError,
+    ErrorContext,
+    CircuitBreakerState,
+)
+
+
+class TestShogunErrorHandler:
+    """Test cases for the main error handler."""
+
+    @pytest.fixture
+    def config(self) -> Dict[str, Any]:
+        """Test configuration."""
+        return {
+            "error_handling": {
+                "retry_policy": {
+                    "max_attempts": 3,
+                    "backoff_strategy": "exponential",
+                    "base_delay": 0.1,
+                    "max_delay": 1.0,
+                    "jitter": True,
+                },
+                "timeout_settings": {
+                    "api_request": 10,
+                    "local_r1": 30,
+                    "groq_request": 5,
+                },
+                "fallback_behavior": {
+                    "api_failure": "local_r1",
+                    "r1_failure": "simplified_response",
+                },
+                "circuit_breaker": {
+                    "enabled": True,
+                    "failure_threshold": 3,
+                    "recovery_timeout": 60,
+                    "half_open_requests": 2,
+                },
+            }
+        }
+
+    @pytest.fixture
+    def error_handler(self, config):
+        """Create error handler instance for testing."""
+        return ShogunErrorHandler(config)
+
+    def test_initialization(self, error_handler):
+        """Test proper initialization of error handler."""
+        assert len(error_handler.circuit_breakers) > 0
+        assert "taisho_r1" in error_handler.circuit_breakers
+        assert "groq_api" in error_handler.circuit_breakers
+        assert error_handler.error_stats["total_errors"] == 0
+
+    @pytest.mark.asyncio
+    async def test_successful_operation(self, error_handler):
+        """Test handling of successful operations."""
+        async with error_handler.handle_operation(
+            operation="test_operation",
+            component="test_component"
+        ) as context:
+            assert context.operation == "test_operation"
+            assert context.component == "test_component"
+            assert context.attempt == 1
+
+    @pytest.mark.asyncio
+    async def test_timeout_handling(self, error_handler):
+        """Test timeout handling."""
+        with pytest.raises(asyncio.TimeoutError):
+            async with error_handler.handle_operation(
+                operation="timeout_test",
+                component="test_component",
+                timeout=0.1
+            ):
+                await asyncio.sleep(0.2)
+
+    @pytest.mark.asyncio
+    async def test_retry_with_exponential_backoff(self, error_handler):
+        """Test retry mechanism with exponential backoff."""
+        attempt_count = 0
+        start_time = time.time()
+
+        try:
+            async with error_handler.handle_operation(
+                operation="retry_test",
+                component="test_component"
+            ):
+                nonlocal attempt_count
+                attempt_count += 1
+                if attempt_count < 3:
+                    raise ValueError(f"Attempt {attempt_count} failed")
+        except OperationFailedError:
+            pass
+
+        elapsed = time.time() - start_time
+        assert attempt_count == 3
+        # Should have some delay due to backoff
+        assert elapsed > 0.2  # At least base delays
+
+    def test_circuit_breaker_initialization(self, error_handler):
+        """Test circuit breaker initialization."""
+        breaker = error_handler.circuit_breakers["test_component"]
+        assert breaker.state == "CLOSED"
+        assert breaker.failures == 0
+
+    def test_circuit_breaker_open_on_failures(self, error_handler):
+        """Test circuit breaker opens after threshold failures."""
+        component = "test_component"
+        threshold = error_handler.circuit_config.get("failure_threshold", 5)
+
+        # Simulate failures to reach threshold
+        for _ in range(threshold):
+            error_handler._record_failure(component)
+
+        breaker = error_handler.circuit_breakers[component]
+        assert breaker.state == "OPEN"
+        assert not error_handler._check_circuit_breaker(component)
+
+    def test_circuit_breaker_recovery(self, error_handler):
+        """Test circuit breaker recovery after timeout."""
+        component = "test_component"
+        breaker = error_handler.circuit_breakers[component]
+
+        # Force open state
+        breaker.state = "OPEN"
+        breaker.last_failure = time.time() - 100  # Old failure
+
+        # Should transition to half-open
+        assert error_handler._check_circuit_breaker(component)
+        assert breaker.state == "HALF_OPEN"
+
+    def test_error_severity_determination(self, error_handler):
+        """Test error severity classification."""
+        # Critical error
+        context = ErrorContext(
+            operation="test", component="test", attempt=1, max_attempts=3,
+            error_type="AuthenticationError", error_message="Auth failed"
+        )
+        severity = error_handler._determine_error_severity(context, Exception())
+        assert severity == ErrorSeverity.CRITICAL
+
+        # Low severity error
+        context.error_type = "UnknownError"
+        severity = error_handler._determine_error_severity(context, Exception())
+        assert severity == ErrorSeverity.LOW
+
+    def test_retry_delay_calculation(self, error_handler):
+        """Test retry delay calculations."""
+        # Exponential backoff
+        delay1 = error_handler._calculate_retry_delay(1)
+        delay2 = error_handler._calculate_retry_delay(2)
+        delay3 = error_handler._calculate_retry_delay(3)
+
+        assert delay2 > delay1
+        assert delay3 > delay2
+        assert delay3 <= error_handler.retry_config.get("max_delay", 30)
+
+    @pytest.mark.asyncio
+    async def test_fallback_strategies(self, error_handler):
+        """Test different fallback strategies."""
+        context = ErrorContext(
+            operation="test", component="api", attempt=3, max_attempts=3,
+            error_type="ConnectionError", error_message="API unreachable"
+        )
+
+        # Test simplified response fallback
+        result = await error_handler._apply_fallback_strategy(
+            FallbackStrategy.SIMPLIFIED_RESPONSE, context
+        )
+        assert isinstance(result, str)
+        assert "æŠ€è¡“çš„ãªå•é¡Œ" in result
+
+        # Test skip recording fallback
+        result = await error_handler._apply_fallback_strategy(
+            FallbackStrategy.SKIP_RECORDING, context
+        )
+        assert result is None
+
+    def test_usage_statistics_tracking(self, error_handler):
+        """Test error statistics tracking."""
+        initial_errors = error_handler.error_stats["total_errors"]
+
+        # Simulate error
+        context = ErrorContext(
+            operation="test", component="test", attempt=1, max_attempts=3,
+            error_type="TestError", error_message="Test error"
+        )
+        
+        # The _handle_error method updates stats
+        asyncio.run(error_handler._handle_error(context, Exception()))
+
+        assert error_handler.error_stats["total_errors"] == initial_errors + 1
+        assert "test" in error_handler.error_stats["errors_by_component"]
+        assert "TestError" in error_handler.error_stats["errors_by_type"]
+
+    def test_health_status_reporting(self, error_handler):
+        """Test health status reporting."""
+        status = error_handler.get_health_status()
+
+        assert "overall_health" in status
+        assert "circuit_breakers" in status
+        assert "error_stats" in status
+        assert "timestamp" in status
+        assert status["overall_health"] in ["healthy", "degraded", "critical"]
+
+    def test_error_report_export(self, error_handler, tmp_path):
+        """Test error report export functionality."""
+        report_file = tmp_path / "error_report.json"
+
+        error_handler.export_error_report(str(report_file))
+
+        assert report_file.exists()
+        
+        import json
+        with open(report_file) as f:
+            report = json.load(f)
+
+        assert "timestamp" in report
+        assert "error_stats" in report
+        assert "circuit_breakers" in report
+        assert "configuration" in report
+
+    def test_error_stats_reset(self, error_handler):
+        """Test error statistics reset."""
+        # Generate some stats
+        error_handler.error_stats["total_errors"] = 10
+        error_handler.error_stats["errors_by_component"]["test"] = 5
+
+        # Reset
+        error_handler.reset_error_stats()
+
+        assert error_handler.error_stats["total_errors"] == 0
+        assert len(error_handler.error_stats["errors_by_component"]) == 0
+
+
+class TestIntegrationWithShogun:
+    """Integration tests with main Shogun system."""
+
+    @pytest.fixture
+    def mock_shogun_config(self):
+        """Mock configuration for integration tests."""
+        return {
+            "error_handling": {
+                "retry_policy": {"max_attempts": 2},
+                "circuit_breaker": {"enabled": True, "failure_threshold": 2},
+                "fallback_behavior": {
+                    "api_failure": "local_r1",
+                    "r1_failure": "simplified_response",
+                },
+            }
+        }
+
+    @pytest.mark.asyncio
+    async def test_integration_with_api_provider(self, mock_shogun_config):
+        """Test error handling integration with API providers."""
+        from shogun.providers.anthropic_api import AnthropicAPIProvider
+        
+        # Mock API provider that fails
+        with patch('shogun.providers.anthropic_api.anthropic') as mock_anthropic:
+            mock_client = Mock()
+            mock_anthropic.AsyncAnthropic.return_value = mock_client
+            mock_client.messages.create = AsyncMock(side_effect=Exception("API Error"))
+
+            error_handler = ShogunErrorHandler(mock_shogun_config)
+            provider = AnthropicAPIProvider(api_key="test")
+
+            # This should fail and trigger error handling
+            with pytest.raises(Exception):
+                await provider.generate("test prompt")
+
+            # Error should be recorded
+            assert error_handler.error_stats["total_errors"] >= 0
+
+    @pytest.mark.asyncio
+    async def test_cascade_failure_handling(self, mock_shogun_config):
+        """Test handling of cascading failures."""
+        error_handler = ShogunErrorHandler(mock_shogun_config)
+
+        failure_count = 0
+
+        async def failing_operation():
+            nonlocal failure_count
+            failure_count += 1
+            raise ConnectionError(f"Failure {failure_count}")
+
+        # Multiple operations that should trigger circuit breaker
+        for i in range(3):
+            try:
+                async with error_handler.handle_operation(
+                    operation="cascade_test",
+                    component="cascade_component",
+                    fallback_strategy=FallbackStrategy.SIMPLIFIED_RESPONSE
+                ):
+                    await failing_operation()
+            except (OperationFailedError, CircuitBreakerOpenError):
+                pass
+
+        # Circuit breaker should be open now
+        assert not error_handler._check_circuit_breaker("cascade_component")
+
+    @pytest.mark.asyncio
+    async def test_recovery_after_circuit_breaker_timeout(self, mock_shogun_config):
+        """Test recovery after circuit breaker timeout."""
+        # Reduce recovery timeout for faster testing
+        mock_shogun_config["error_handling"]["circuit_breaker"]["recovery_timeout"] = 0.1
+        
+        error_handler = ShogunErrorHandler(mock_shogun_config)
+        component = "recovery_test"
+
+        # Force circuit breaker open
+        breaker = error_handler.circuit_breakers[component]
+        breaker.state = "OPEN"
+        breaker.last_failure = time.time() - 0.2
+
+        # Should allow operation after timeout
+        assert error_handler._check_circuit_breaker(component)
+        assert breaker.state == "HALF_OPEN"
+
+        # Successful operation should close circuit
+        error_handler._record_success(component)
+        assert breaker.state == "CLOSED"
+
+
+if __name__ == "__main__":
+    pytest.main([__file__, "-v"])
\ No newline at end of file
-- 
2.43.0

