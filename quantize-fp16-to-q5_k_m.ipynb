{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FP16 GGUF → Q5_K_M 量子化ガイド\n",
    "\n",
    "## 概要\n",
    "\n",
    "[grapevine-AI/Qwen2.5-Coder-32B-Instruct-GGUF](https://huggingface.co/grapevine-AI/Qwen2.5-Coder-32B-Instruct-GGUF) の FP16 モデルから Q5_K_M を作成する手順です。\n",
    "\n",
    "### 前提条件\n",
    "\n",
    "| 項目 | 要件 |\n",
    "|------|------|\n",
    "| **Colab プラン** | Pay As You Go |\n",
    "| **ランタイム** | A100 GPU（ディスク容量・RAM確保のため） |\n",
    "| **Google One** | 2TB 契約中（Google Drive に出力を保存） |\n",
    "| **HuggingFace** | アカウント（トークン推奨、なくてもダウンロード可能な場合あり） |\n",
    "\n",
    "### ファイルサイズ見積もり\n",
    "\n",
    "| ファイル | サイズ |\n",
    "|---------|--------|\n",
    "| FP16 GGUF（入力） | 約 65 GB（分割ファイル） |\n",
    "| Q5_K_M GGUF（出力） | 約 23 GB |\n",
    "| llama.cpp ビルド | 約 1 GB |\n",
    "| **合計必要ディスク** | **約 90 GB** |\n",
    "\n",
    "A100 ランタイムのローカルディスクは約 200GB あるため十分です。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 0: ランタイム設定\n",
    "\n",
    "**実行前に必ず以下を確認してください：**\n",
    "\n",
    "1. メニュー → `ランタイム` → `ランタイムのタイプを変更`\n",
    "2. **ハードウェアアクセラレータ**: `A100 GPU` を選択\n",
    "3. 接続して、このノートブックを上から順に実行\n",
    "\n",
    "> **なぜA100か？**  \n",
    "> GPUを使うためではなく、A100ランタイムはディスク容量（約200GB）とシステムRAM（約83GB）が大きいため。  \n",
    "> 32Bモデルの FP16 は約65GB あるので、標準ランタイム（約78GBディスク）では容量不足になります。  \n",
    "> L4 ランタイムでも動作する可能性がありますが、ディスク容量が足りるか確認が必要です。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1: 環境確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ディスク容量とRAMを確認\n",
    "!df -h /\n",
    "print(\"---\")\n",
    "!free -h\n",
    "print(\"---\")\n",
    "# GPU確認（量子化自体にGPUは不要だが、ランタイム種別の確認用）\n",
    "!nvidia-smi --query-gpu=name,memory.total --format=csv,noheader 2>/dev/null || echo \"GPU なし（CPUランタイム）\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2: Google Drive マウント\n",
    "\n",
    "完成した Q5_K_M ファイル（約23GB）を Google Drive に保存するためにマウントします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# 出力先ディレクトリを作成\n",
    "!mkdir -p /content/drive/MyDrive/gguf-models\n",
    "print(\"Google Drive マウント完了。出力先: /content/drive/MyDrive/gguf-models/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3: llama.cpp のビルド\n",
    "\n",
    "`llama-quantize` コマンドを使うために llama.cpp をソースからビルドします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ビルド用の依存パッケージをインストール\n",
    "!apt-get update -qq && apt-get install -y -qq build-essential cmake git > /dev/null 2>&1\n",
    "print(\"依存パッケージインストール完了\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llama.cpp をクローンしてビルド\n",
    "%cd /content\n",
    "!git clone --depth 1 https://github.com/ggerganov/llama.cpp.git\n",
    "%cd /content/llama.cpp\n",
    "\n",
    "# CMake でビルド（CPU のみで十分。量子化に GPU は不要）\n",
    "!cmake -B build -DCMAKE_BUILD_TYPE=Release\n",
    "!cmake --build build --config Release -j$(nproc)\n",
    "\n",
    "print(\"\\n=== ビルド完了 ===\")\n",
    "# llama-quantize の存在を確認\n",
    "!ls -lh /content/llama.cpp/build/bin/llama-quantize\n",
    "# llama-gguf-split の存在を確認（分割ファイルのマージ用）\n",
    "!ls -lh /content/llama.cpp/build/bin/llama-gguf-split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4: FP16 モデルのダウンロード\n",
    "\n",
    "grapevine-AI リポジトリから FP16 GGUF ファイルをダウンロードします。\n",
    "\n",
    "32Bモデルの FP16 は約 65GB あり、HuggingFace の制限により**複数ファイルに分割**されている可能性があります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# huggingface_hub をインストール\n",
    "!pip install -q huggingface_hub[hf_transfer]\n",
    "\n",
    "# hf_transfer で高速ダウンロードを有効化\n",
    "import os\n",
    "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# （任意）HuggingFace にログイン\n",
    "# プライベートリポジトリの場合やレート制限回避のために必要\n",
    "# トークンは https://huggingface.co/settings/tokens で作成できます\n",
    "\n",
    "# from huggingface_hub import login\n",
    "# login(token=\"hf_xxxxx\")  # ← 自分のトークンに置き換え"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FP16 ファイルをダウンロード\n",
    "# --include で fp16 関連ファイルのみを指定\n",
    "!huggingface-cli download \\\n",
    "    grapevine-AI/Qwen2.5-Coder-32B-Instruct-GGUF \\\n",
    "    --include \"*fp16*\" \\\n",
    "    --local-dir /content/fp16-model \\\n",
    "    --local-dir-use-symlinks False\n",
    "\n",
    "print(\"\\n=== ダウンロード完了 ===\")\n",
    "!ls -lh /content/fp16-model/*.gguf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ダウンロードされたファイルの確認\n",
    "import glob\n",
    "\n",
    "fp16_files = sorted(glob.glob(\"/content/fp16-model/*fp16*.gguf\"))\n",
    "print(f\"FP16 ファイル数: {len(fp16_files)}\")\n",
    "for f in fp16_files:\n",
    "    size_gb = os.path.getsize(f) / (1024**3)\n",
    "    print(f\"  {os.path.basename(f)} ({size_gb:.2f} GB)\")\n",
    "\n",
    "total_gb = sum(os.path.getsize(f) for f in fp16_files) / (1024**3)\n",
    "print(f\"\\n合計サイズ: {total_gb:.2f} GB\")\n",
    "\n",
    "# 分割ファイルか単一ファイルかを判定\n",
    "is_split = len(fp16_files) > 1\n",
    "print(f\"分割ファイル: {'はい' if is_split else 'いいえ'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 5: 分割ファイルのマージ（必要な場合のみ）\n",
    "\n",
    "FP16 が複数ファイルに分割されている場合、`llama-gguf-split --merge` で結合します。  \n",
    "単一ファイルの場合はこのセルはスキップされます。\n",
    "\n",
    "> **注意**: マージ中は FP16 の分割ファイル + マージ後のファイルで一時的にディスク使用量が倍増します（約130GB）。  \n",
    "> A100 ランタイム（約200GBディスク）なら問題ありません。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "QUANTIZE_BIN = \"/content/llama.cpp/build/bin/llama-quantize\"\n",
    "GGUF_SPLIT_BIN = \"/content/llama.cpp/build/bin/llama-gguf-split\"\n",
    "\n",
    "fp16_files = sorted(glob.glob(\"/content/fp16-model/*fp16*.gguf\"))\n",
    "is_split = len(fp16_files) > 1\n",
    "\n",
    "if is_split:\n",
    "    # 分割ファイルの場合：最初のシャードを指定してマージ\n",
    "    first_shard = fp16_files[0]\n",
    "    merged_path = \"/content/fp16-model/merged-fp16.gguf\"\n",
    "    print(f\"分割ファイルをマージします...\")\n",
    "    print(f\"  入力: {first_shard}\")\n",
    "    print(f\"  出力: {merged_path}\")\n",
    "\n",
    "    result = subprocess.run(\n",
    "        [GGUF_SPLIT_BIN, \"--merge\", first_shard, merged_path],\n",
    "        capture_output=True, text=True\n",
    "    )\n",
    "    print(result.stdout)\n",
    "    if result.returncode != 0:\n",
    "        print(f\"エラー: {result.stderr}\")\n",
    "        # 最近の llama-quantize は分割ファイルを直接扱えるため、\n",
    "        # マージ失敗時は分割ファイルの最初のシャードを直接使用\n",
    "        print(\"\\n→ マージに失敗しました。分割ファイルを直接 quantize に渡します。\")\n",
    "        INPUT_MODEL = first_shard\n",
    "    else:\n",
    "        size_gb = os.path.getsize(merged_path) / (1024**3)\n",
    "        print(f\"\\nマージ完了: {size_gb:.2f} GB\")\n",
    "        INPUT_MODEL = merged_path\n",
    "\n",
    "        # マージ後、分割ファイルを削除してディスク容量を確保\n",
    "        print(\"分割ファイルを削除してディスク容量を確保...\")\n",
    "        for f in fp16_files:\n",
    "            os.remove(f)\n",
    "        print(\"削除完了\")\n",
    "else:\n",
    "    INPUT_MODEL = fp16_files[0]\n",
    "    print(f\"単一ファイルです: {INPUT_MODEL}\")\n",
    "\n",
    "print(f\"\\n量子化に使用するファイル: {INPUT_MODEL}\")\n",
    "!df -h /"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 6: Q5_K_M に量子化\n",
    "\n",
    "llama.cpp の `llama-quantize` を使って FP16 → Q5_K_M に変換します。\n",
    "\n",
    "### 量子化タイプの解説\n",
    "\n",
    "| タイプ | サイズ (32B) | 品質 | 説明 |\n",
    "|--------|-------------|------|------|\n",
    "| Q4_K_M | ~19 GB | 良い | バランス型。VRAM が限られる場合に最適 |\n",
    "| **Q5_K_M** | **~23 GB** | **とても良い** | **FP16 に近い品質。推奨** |\n",
    "| Q6_K | ~27 GB | 優秀 | ほぼ FP16 品質だがサイズ大 |\n",
    "| Q8_0 | ~34 GB | 最高 | 量子化の中では最高品質 |\n",
    "\n",
    "> `K_M` は K-quant の Medium バリアント。重要なレイヤーにはより高い精度を割り当てます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "OUTPUT_MODEL = \"/content/Qwen2.5-Coder-32B-Instruct-Q5_K_M.gguf\"\n",
    "QUANT_TYPE = \"Q5_K_M\"\n",
    "\n",
    "print(f\"=== 量子化開始 ===\")\n",
    "print(f\"  入力: {INPUT_MODEL}\")\n",
    "print(f\"  出力: {OUTPUT_MODEL}\")\n",
    "print(f\"  量子化タイプ: {QUANT_TYPE}\")\n",
    "print(f\"  ※ 32Bモデルの場合、20〜40分程度かかります\")\n",
    "print()\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "!{QUANTIZE_BIN} \\\n",
    "    {INPUT_MODEL} \\\n",
    "    {OUTPUT_MODEL} \\\n",
    "    {QUANT_TYPE}\n",
    "\n",
    "elapsed = time.time() - start\n",
    "print(f\"\\n=== 量子化完了 ===\")\n",
    "print(f\"所要時間: {elapsed/60:.1f} 分\")\n",
    "\n",
    "if os.path.exists(OUTPUT_MODEL):\n",
    "    size_gb = os.path.getsize(OUTPUT_MODEL) / (1024**3)\n",
    "    print(f\"出力ファイルサイズ: {size_gb:.2f} GB\")\n",
    "else:\n",
    "    print(\"エラー: 出力ファイルが生成されませんでした\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 7: Google Drive に保存\n",
    "\n",
    "生成した Q5_K_M ファイルを Google Drive にコピーします。  \n",
    "Colab のランタイムは時間制限があるため、成果物を必ず保存してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "DRIVE_OUTPUT = \"/content/drive/MyDrive/gguf-models/Qwen2.5-Coder-32B-Instruct-Q5_K_M.gguf\"\n",
    "\n",
    "print(f\"Google Drive にコピー中...\")\n",
    "print(f\"  元: {OUTPUT_MODEL}\")\n",
    "print(f\"  先: {DRIVE_OUTPUT}\")\n",
    "print(f\"  ※ 約23GBのコピーには数分かかります\")\n",
    "\n",
    "start = time.time()\n",
    "shutil.copy2(OUTPUT_MODEL, DRIVE_OUTPUT)\n",
    "elapsed = time.time() - start\n",
    "\n",
    "drive_size = os.path.getsize(DRIVE_OUTPUT) / (1024**3)\n",
    "print(f\"\\nコピー完了！ ({elapsed/60:.1f} 分)\")\n",
    "print(f\"保存先: {DRIVE_OUTPUT} ({drive_size:.2f} GB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 8: クリーンアップ（任意）\n",
    "\n",
    "ローカルディスクの一時ファイルを削除します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FP16 モデルの削除（ローカルディスクの容量確保）\n",
    "!rm -rf /content/fp16-model\n",
    "!rm -f /content/Qwen2.5-Coder-32B-Instruct-Q5_K_M.gguf\n",
    "print(\"ローカルの一時ファイルを削除しました\")\n",
    "print(\"Q5_K_M ファイルは Google Drive に保存済みです\")\n",
    "!df -h /"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 補足: HuggingFace にアップロードする場合（任意）\n",
    "\n",
    "作成した Q5_K_M を自分の HuggingFace リポジトリにアップロードできます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # HuggingFace にアップロード（必要な場合のみコメント解除）\n",
    "# from huggingface_hub import HfApi, login\n",
    "#\n",
    "# # ログイン（Write権限のあるトークンが必要）\n",
    "# login(token=\"hf_xxxxx\")\n",
    "#\n",
    "# api = HfApi()\n",
    "#\n",
    "# # リポジトリ作成（存在しない場合）\n",
    "# repo_id = \"your-username/Qwen2.5-Coder-32B-Instruct-Q5_K_M-GGUF\"\n",
    "# api.create_repo(repo_id, exist_ok=True, repo_type=\"model\")\n",
    "#\n",
    "# # アップロード\n",
    "# api.upload_file(\n",
    "#     path_or_fileobj=DRIVE_OUTPUT,\n",
    "#     path_in_repo=\"Qwen2.5-Coder-32B-Instruct-Q5_K_M.gguf\",\n",
    "#     repo_id=repo_id,\n",
    "# )\n",
    "# print(f\"アップロード完了: https://huggingface.co/{repo_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## トラブルシューティング\n",
    "\n",
    "### ディスク容量不足\n",
    "\n",
    "```\n",
    "Error: failed to open file\n",
    "```\n",
    "\n",
    "→ `df -h /` でディスク残量を確認。FP16ダウンロード前にローカルディスクに70GB以上の空きが必要です。  \n",
    "→ A100ランタイムに切り替えるか、不要なファイルを削除してください。\n",
    "\n",
    "### メモリ不足（OOM）\n",
    "\n",
    "```\n",
    "std::bad_alloc / Killed\n",
    "```\n",
    "\n",
    "→ llama-quantize はレイヤー単位で処理するため通常は問題ありませんが、発生した場合は「ハイメモリ」ランタイムを選択してください。\n",
    "\n",
    "### ダウンロードが途中で止まる\n",
    "\n",
    "→ `huggingface-cli download` はレジューム対応です。同じコマンドを再実行すれば途中から再開されます。\n",
    "\n",
    "### llama-quantize が見つからない\n",
    "\n",
    "→ llama.cpp のビルドに失敗している可能性があります。Step 3 を再実行してください。  \n",
    "→ パスが変わっている場合: `!find /content/llama.cpp/build -name 'llama-quantize'` で場所を確認。\n",
    "\n",
    "### 分割ファイルの直接量子化\n",
    "\n",
    "最近の llama.cpp では、分割 GGUF を直接 `llama-quantize` に渡せます（最初のシャードを指定）。  \n",
    "マージが失敗する場合はこの方法を試してください。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}